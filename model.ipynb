{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HybridTwoWay Model (Colab Ready)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\n",
        "필요한 PyTorch 모듈과 타입 힌트를 불러옵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bef1b69",
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install roboflow torch torchvision torchaudio opencv-python numpy tqdm pillow matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import List, Tuple\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77da3f4e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 1: 설치 및 기본 Import, Roboflow 다운로드\n",
        "# ============================================\n",
        "!pip install roboflow torch torchvision torchaudio opencv-python numpy tqdm pillow matplotlib\n",
        "\n",
        "import math\n",
        "from typing import List, Tuple\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from roboflow import Roboflow\n",
        "\n",
        "rf = Roboflow(api_key=\"HG9M6YJZpcCUgAQaKO9v\")\n",
        "project = rf.workspace(\"arakon\").project(\"detection-base-hqaeg\")\n",
        "version = project.version(6)\n",
        "dataset = version.download(\"yolov8\")\n",
        "\n",
        "print(f'Roboflow dataset downloaded to: {dataset.location}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Utility Functions\n",
        "## 1. Anomaly-Aware CNN Stem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 2: 기본 Conv 블록 + Stem\n",
        "# ============================================\n",
        "\n",
        "def conv_bn_act(in_ch, out_ch, k=3, s=1, p=1, act=True):\n",
        "    m = [nn.Conv2d(in_ch, out_ch, k, s, p, bias=False),\n",
        "         nn.BatchNorm2d(out_ch)]\n",
        "    if act:\n",
        "        m.append(nn.SiLU(inplace=True))\n",
        "    return nn.Sequential(*m)\n",
        "\n",
        "class FixedGaussianBlur(nn.Module):\n",
        "    def __init__(self, channels, k=5, sigma=1.0):\n",
        "        super().__init__()\n",
        "        grid = torch.arange(k).float() - (k - 1) / 2\n",
        "        gauss = torch.exp(-(grid ** 2) / (2 * sigma ** 2))\n",
        "        kernel1d = gauss / gauss.sum()\n",
        "        kernel2d = torch.outer(kernel1d, kernel1d)\n",
        "        weight = kernel2d[None, None, :, :].repeat(channels, 1, 1, 1)\n",
        "        self.register_buffer('weight', weight)\n",
        "        self.groups = channels\n",
        "        self.k = k\n",
        "\n",
        "    def forward(self, x):\n",
        "        pad = (self.k // 2,) * 4\n",
        "        return F.conv2d(F.pad(x, pad, mode='reflect'),\n",
        "                        self.weight, groups=self.groups)\n",
        "\n",
        "class AnomalyAwareStem(nn.Module):\n",
        "    def __init__(self, in_ch=3, base_ch=48):\n",
        "        super().__init__()\n",
        "        C1, C2, C3 = base_ch, base_ch * 2, base_ch * 4\n",
        "        self.stem = nn.Sequential(\n",
        "            conv_bn_act(in_ch, C1, 3, 2, 1),\n",
        "            conv_bn_act(C1, C2, 3, 2, 1),\n",
        "            conv_bn_act(C2, C3, 3, 2, 1),\n",
        "        )\n",
        "        self.blur = FixedGaussianBlur(in_ch, k=5, sigma=1.0)\n",
        "        self.anom = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, in_ch, 3, 1, 1, groups=in_ch, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_ch, C3 // 4, 1, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(C3 // 4),\n",
        "            nn.SiLU(inplace=True),\n",
        "        )\n",
        "        self.fuse = nn.Conv2d(C3 + C3 // 4, C3, 1, 1, 0, bias=False)\n",
        "        self.fuse_bn = nn.BatchNorm2d(C3)\n",
        "        self.vis_head = nn.Conv2d(C3, 1, 1, 1, 0)\n",
        "\n",
        "    @property\n",
        "    def out_channels(self):\n",
        "        return 4 * self.base_ch\n",
        "\n",
        "    def forward(self, x):\n",
        "        f_main = self.stem(x)  # (B,C3,H/8,W/8)\n",
        "        blurred = self.blur(x)\n",
        "        high = x - blurred\n",
        "        high_ds = F.interpolate(high, size=f_main.shape[-2:],\n",
        "                                mode='bilinear', align_corners=False)\n",
        "        f_anom = self.anom(high_ds)\n",
        "        f = torch.cat([f_main, f_anom], dim=1)\n",
        "        f = self.fuse_bn(self.fuse(f))\n",
        "        f = F.silu(f, inplace=True)\n",
        "        v = torch.sigmoid(self.vis_head(f_main))\n",
        "        return f, v\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Vision Transformer Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 3: ViT Encoder + Feedback Adapter\n",
        "# ============================================\n",
        "\n",
        "class PatchEmbed1x1(nn.Module):\n",
        "    \"\"\"CNN 출력 → ViT 임베딩 (해상도 유지)\"\"\"\n",
        "    def __init__(self, in_ch, embed_dim):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(in_ch, embed_dim, 1, 1, 0, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        x = self.bn(x)\n",
        "        x = F.silu(x, inplace=True)\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, dim, mlp_ratio=4.0, drop=0.0):\n",
        "        super().__init__()\n",
        "        hidden = int(dim * mlp_ratio)\n",
        "        self.fc1 = nn.Linear(dim, hidden)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden, dim)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "class MultiheadSelfAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, attn_drop=0.0, proj_drop=0.0):\n",
        "        super().__init__()\n",
        "        assert dim % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=True)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = (self.qkv(x)\n",
        "               .reshape(B, N, 3, self.num_heads, self.head_dim)\n",
        "               .permute(2, 0, 3, 1, 4))\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        out = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        out = self.proj(out)\n",
        "        out = self.proj_drop(out)\n",
        "        return out\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, mlp_ratio=4.0, drop=0.0):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = MultiheadSelfAttention(dim, num_heads, drop, drop)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.mlp = MLP(dim, mlp_ratio, drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class ViTEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim=512, depth=8, num_heads=8):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, mlp_ratio=4.0, drop=0.0)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        for blk in self.blocks:\n",
        "            tokens = blk(tokens)\n",
        "        return tokens\n",
        "\n",
        "class FeedbackAdapter(nn.Module):\n",
        "    \"\"\"\n",
        "    ViT tokens → (gamma, beta) 생성해서 stem feature 재조정\n",
        "    \"\"\"\n",
        "    def __init__(self, d_token: int, c_stem: int, use_bn: bool = True):\n",
        "        super().__init__()\n",
        "        layers = [nn.Conv2d(d_token, c_stem * 2, 1, 1, 0, bias=not use_bn)]\n",
        "        if use_bn:\n",
        "            layers.append(nn.BatchNorm2d(c_stem * 2))\n",
        "        layers.append(nn.SiLU(inplace=True))\n",
        "        self.adapter = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, tokens: torch.Tensor, Ht: int, Wt: int,\n",
        "                f_stem: torch.Tensor):\n",
        "        B, N, D = tokens.shape\n",
        "        t2d = tokens.transpose(1, 2).reshape(B, D, Ht, Wt)\n",
        "        ab = self.adapter(t2d)\n",
        "        Cs = f_stem.shape[1]\n",
        "        gamma, beta = torch.split(ab, Cs, dim=1)\n",
        "        return f_stem * (1 + torch.tanh(gamma)) + beta\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. PAN-Lite Neck"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 4: PAN-Lite Neck (P3, P4, P5) + YOLO Head (3-Scale)\n",
        "# ============================================\n",
        "\n",
        "class PANLite(nn.Module):\n",
        "    \"\"\"\n",
        "    입력: ViT 출력 feature (P3 수준, stride=8)\n",
        "    출력: P3, P4, P5 (stride 8,16,32)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch=512, mid=256):\n",
        "        super().__init__()\n",
        "        self.lateral = conv_bn_act(in_ch, mid, 1, 1, 0)\n",
        "\n",
        "        # P4, P5 생성\n",
        "        self.down4 = conv_bn_act(mid, mid, 3, 2, 1)  # P3 -> P4\n",
        "        self.down5 = conv_bn_act(mid, mid, 3, 2, 1)  # P4 -> P5\n",
        "\n",
        "        # top-down\n",
        "        self.up4 = conv_bn_act(mid + mid, mid, 3, 1, 1)\n",
        "        self.up3 = conv_bn_act(mid + mid, mid, 3, 1, 1)\n",
        "\n",
        "        # bottom-up refine\n",
        "        self.down_f4 = conv_bn_act(mid, mid, 3, 2, 1)\n",
        "        self.fuse4 = conv_bn_act(mid + mid, mid, 3, 1, 1)\n",
        "        self.down_f5 = conv_bn_act(mid, mid, 3, 2, 1)\n",
        "        self.fuse5 = conv_bn_act(mid + mid, mid, 3, 1, 1)\n",
        "\n",
        "    def forward(self, p3):\n",
        "        # channel align\n",
        "        p3 = self.lateral(p3)       # (B,256,H/8,W/8)\n",
        "        p4 = self.down4(p3)         # (B,256,H/16,W/16)\n",
        "        p5 = self.down5(p4)         # (B,256,H/32,W/32)\n",
        "\n",
        "        # top-down\n",
        "        p4u = F.interpolate(p5, size=p4.shape[-2:], mode='nearest')\n",
        "        p4 = self.up4(torch.cat([p4, p4u], dim=1))\n",
        "        p3u = F.interpolate(p4, size=p3.shape[-2:], mode='nearest')\n",
        "        p3 = self.up3(torch.cat([p3, p3u], dim=1))\n",
        "\n",
        "        # bottom-up\n",
        "        p4b = self.down_f4(p3)\n",
        "        p4 = self.fuse4(torch.cat([p4, p4b], dim=1))\n",
        "        p5b = self.down_f5(p4)\n",
        "        p5 = self.fuse5(torch.cat([p5, p5b], dim=1))\n",
        "\n",
        "        return p3, p4, p5\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. YOLO-style Detection Head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ac62674",
      "metadata": {},
      "outputs": [],
      "source": [
        "class YOLOHeadLite(nn.Module):\n",
        "    \"\"\"\n",
        "    P3, P4, P5 각각에서 cls/obj/box 예측 (anchor-free, 단순 버전)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch=256, num_classes=1):\n",
        "        super().__init__()\n",
        "        c = in_ch\n",
        "        # stems\n",
        "        self.stem3 = conv_bn_act(c, c, 3, 1, 1)\n",
        "        self.stem4 = conv_bn_act(c, c, 3, 1, 1)\n",
        "        self.stem5 = conv_bn_act(c, c, 3, 1, 1)\n",
        "\n",
        "        # heads\n",
        "        self.cls3 = nn.Conv2d(c, num_classes, 1, 1, 0)\n",
        "        self.obj3 = nn.Conv2d(c, 1,           1, 1, 0)\n",
        "        self.box3 = nn.Conv2d(c, 4,           1, 1, 0)\n",
        "        nn.init.constant_(self.obj3.bias, -4.59)\n",
        "\n",
        "        self.cls4 = nn.Conv2d(c, num_classes, 1, 1, 0)\n",
        "        self.obj4 = nn.Conv2d(c, 1,           1, 1, 0)\n",
        "        self.box4 = nn.Conv2d(c, 4,           1, 1, 0)\n",
        "        nn.init.constant_(self.obj4.bias, -4.59)\n",
        "\n",
        "        self.cls5 = nn.Conv2d(c, num_classes, 1, 1, 0)\n",
        "        self.obj5 = nn.Conv2d(c, 1,           1, 1, 0)\n",
        "        self.box5 = nn.Conv2d(c, 4,           1, 1, 0)\n",
        "        nn.init.constant_(self.obj5.bias, -4.59)\n",
        "\n",
        "    def forward_single(self, x, stem, cls, obj, box):\n",
        "        f = stem(x)\n",
        "        return cls(f), obj(f), box(f)\n",
        "\n",
        "    def forward(self, p3, p4, p5):\n",
        "        c3, o3, b3 = self.forward_single(p3, self.stem3,\n",
        "                                         self.cls3, self.obj3, self.box3)\n",
        "        c4, o4, b4 = self.forward_single(p4, self.stem4,\n",
        "                                         self.cls4, self.obj4, self.box4)\n",
        "        c5, o5, b5 = self.forward_single(p5, self.stem5,\n",
        "                                         self.cls5, self.obj5, self.box5)\n",
        "        return [(c3, o3, b3), (c4, o4, b4), (c5, o5, b5)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. HybridTwoWay Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HybridTwoWay(nn.Module):\n",
        "    \"\"\"\n",
        "    Stem → ViT (with Pos Embed) → (반복) Feedback → PANLite → YOLOHeadLite\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_ch=3,\n",
        "        stem_base=32,\n",
        "        embed_dim=256,\n",
        "        vit_depth=4,\n",
        "        vit_heads=4,\n",
        "        num_classes=3,\n",
        "        iters=1,\n",
        "        detach_feedback=True,\n",
        "        img_size=640  # [중요] 이미지 사이즈 명시 필요\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert iters >= 1\n",
        "        self.iters = iters\n",
        "        self.detach_feedback = detach_feedback\n",
        "\n",
        "        # 1) CNN Stem\n",
        "        self.stem = AnomalyAwareStem(in_ch=in_ch, base_ch=stem_base)\n",
        "        c_stem = stem_base * 4 \n",
        "\n",
        "        # 2) Stem feature → ViT token 변환\n",
        "        self.patch = PatchEmbed1x1(c_stem, embed_dim)\n",
        "\n",
        "        # [New] Positional Embedding\n",
        "        # Stem output stride is 8. So patches = (img_size // 8)^2\n",
        "        self.num_patches = (img_size // 8) ** 2\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches, embed_dim))\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "\n",
        "        # 3) ViT Encoder\n",
        "        self.vit = ViTEncoder(\n",
        "            embed_dim=embed_dim,\n",
        "            depth=vit_depth,\n",
        "            num_heads=vit_heads\n",
        "        )\n",
        "\n",
        "        # 4) Feedback Adapter\n",
        "        self.feedback = FeedbackAdapter(\n",
        "            d_token=embed_dim,\n",
        "            c_stem=c_stem,\n",
        "            use_bn=True\n",
        "        )\n",
        "\n",
        "        # 5) Neck + Head\n",
        "        self.neck = PANLite(in_ch=embed_dim, mid=256)\n",
        "        self.head = YOLOHeadLite(in_ch=256, num_classes=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        \n",
        "        # 1) Stem\n",
        "        f_stem, vis = self.stem(x)  # (B, C_s, H/8, W/8)\n",
        "\n",
        "        # 2) 초기 tokens 생성\n",
        "        p0 = self.patch(f_stem)     # (B, D, H/8, W/8)\n",
        "        Ht, Wt = p0.shape[-2:]\n",
        "        tokens = p0.flatten(2).transpose(1, 2)  # (B, N, D)\n",
        "\n",
        "        # [New] Add Positional Embedding\n",
        "        # 학습/추론 이미지 크기가 init과 다를 경우 interpolate 처리 (안전장치)\n",
        "        if tokens.shape[1] != self.pos_embed.shape[1]:\n",
        "            pos_embed = F.interpolate(\n",
        "                self.pos_embed.reshape(1, int(self.num_patches**0.5), int(self.num_patches**0.5), -1).permute(0, 3, 1, 2),\n",
        "                size=(Ht, Wt), mode='bicubic', align_corners=False\n",
        "            ).flatten(2).transpose(1, 2)\n",
        "            tokens = tokens + pos_embed\n",
        "        else:\n",
        "            tokens = tokens + self.pos_embed\n",
        "\n",
        "        f_fb = f_stem\n",
        "        preds = None\n",
        "        aux = None\n",
        "\n",
        "        # 3) iters 만큼 반복\n",
        "        for i in range(self.iters):\n",
        "            # 3-1) ViT\n",
        "            tokens = self.vit(tokens)\n",
        "\n",
        "            # 3-2) Feedback\n",
        "            toks_for_fb = tokens.detach() if self.detach_feedback else tokens\n",
        "            f_fb = self.feedback(toks_for_fb, Ht, Wt, f_fb)\n",
        "\n",
        "            # 3-3) Neck/Head\n",
        "            p3_in = self.patch(f_fb)\n",
        "            p3, p4, p5 = self.neck(p3_in)\n",
        "\n",
        "            preds = self.head(p3, p4, p5)\n",
        "            aux = {\"P3\": p3, \"P4\": p4, \"P5\": p5, \"V\": vis}\n",
        "\n",
        "            # 3-4) 다음 루프 토큰 준비 (여기서도 위치정보 다시 더해줌)\n",
        "            if i != self.iters - 1:\n",
        "                tokens = p3_in.flatten(2).transpose(1, 2)\n",
        "                # 동일하게 위치 정보 더하기\n",
        "                if tokens.shape[1] != self.pos_embed.shape[1]:\n",
        "                     tokens = tokens + pos_embed # 위에서 계산한 resized 사용\n",
        "                else:\n",
        "                     tokens = tokens + self.pos_embed\n",
        "\n",
        "        return preds, aux"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b16ccb66",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 6: Dataset / Dataloader\n",
        "# ============================================\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "\n",
        "IMG_SIZE = 640\n",
        "\n",
        "def yolo_collate_fn(batch):\n",
        "    imgs = []\n",
        "    targets = []\n",
        "    for img, tgt in batch:\n",
        "        imgs.append(img)\n",
        "        targets.append(tgt)\n",
        "    imgs = torch.stack(imgs, 0)\n",
        "    return imgs, targets\n",
        "\n",
        "class YoloDataset(Dataset):\n",
        "    def __init__(self, root):\n",
        "        self.img_dir = os.path.join(root, \"images\")\n",
        "        self.label_dir = os.path.join(root, \"labels\")\n",
        "        self.images = sorted(os.listdir(self.img_dir))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.images[idx]\n",
        "\n",
        "        img_path = os.path.join(self.img_dir, name)\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "        img = torch.tensor(img).permute(2,0,1).float() / 255.0\n",
        "\n",
        "        label_path = os.path.join(self.label_dir,\n",
        "                                  name.replace(\".jpg\",\".txt\").replace(\".png\",\".txt\"))\n",
        "        boxes = []\n",
        "        if os.path.exists(label_path):\n",
        "            with open(label_path, \"r\") as f:\n",
        "                for line in f.readlines():\n",
        "                    cls, x, y, w, h = map(float, line.split())\n",
        "                    boxes.append([cls, x, y, w, h])\n",
        "\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "        return img, boxes\n",
        "\n",
        "DATA_PATH = dataset.location\n",
        "\n",
        "train_dataset = YoloDataset(os.path.join(DATA_PATH, \"train\"))\n",
        "val_dataset   = YoloDataset(os.path.join(DATA_PATH, \"valid\"))\n",
        "test_dataset  = YoloDataset(os.path.join(DATA_PATH, \"test\"))\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=True,\n",
        "    collate_fn=yolo_collate_fn\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=False,          # 검증은 보통 shuffle=False\n",
        "    collate_fn=yolo_collate_fn\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    collate_fn=yolo_collate_fn\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98c51e46",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 7: YOLO-style Loss (Focal + GIoU)\n",
        "# ============================================\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Binary Focal Loss for logits (BCE with logits + focal modulation)\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha=0.25, gamma=2.0, reduction=\"mean\"):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        \"\"\"\n",
        "        logits: any shape\n",
        "        targets: same shape, in {0,1}\n",
        "        \"\"\"\n",
        "        # p = sigmoid(logit)\n",
        "        prob = torch.sigmoid(logits)\n",
        "        ce = F.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\")\n",
        "\n",
        "        # p_t\n",
        "        p_t = prob * targets + (1 - prob) * (1 - targets)\n",
        "        # focal term\n",
        "        focal_term = (1 - p_t) ** self.gamma\n",
        "\n",
        "        loss = ce * focal_term\n",
        "\n",
        "        if self.alpha >= 0:\n",
        "            alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
        "            loss = alpha_t * loss\n",
        "\n",
        "        if self.reduction == \"mean\":\n",
        "            return loss.mean()\n",
        "        elif self.reduction == \"sum\":\n",
        "            return loss.sum()\n",
        "        else:\n",
        "            return loss\n",
        "\n",
        "\n",
        "def xywh_to_xyxy(box_xywh):\n",
        "    \"\"\"\n",
        "    box_xywh: [..., 4] where box = (x_c, y_c, w, h), normalized [0,1]\n",
        "    return: [..., 4] (x1,y1,x2,y2), normalized [0,1]\n",
        "    \"\"\"\n",
        "    x_c, y_c, w, h = box_xywh.unbind(-1)\n",
        "    x1 = x_c - w / 2\n",
        "    y1 = y_c - h / 2\n",
        "    x2 = x_c + w / 2\n",
        "    y2 = y_c + h / 2\n",
        "    return torch.stack([x1, y1, x2, y2], dim=-1)\n",
        "\n",
        "\n",
        "def giou_loss(pred_box_xyxy, tgt_box_xyxy):\n",
        "    \"\"\"\n",
        "    pred_box_xyxy: [N, 4], normalized [0,1]\n",
        "    tgt_box_xyxy:  [N, 4], normalized [0,1]\n",
        "    return: scalar loss (1 - GIoU 평균)\n",
        "    \"\"\"\n",
        "    # IoU\n",
        "    x1 = torch.max(pred_box_xyxy[:, 0], tgt_box_xyxy[:, 0])\n",
        "    y1 = torch.max(pred_box_xyxy[:, 1], tgt_box_xyxy[:, 1])\n",
        "    x2 = torch.min(pred_box_xyxy[:, 2], tgt_box_xyxy[:, 2])\n",
        "    y2 = torch.min(pred_box_xyxy[:, 3], tgt_box_xyxy[:, 3])\n",
        "\n",
        "    inter_w = (x2 - x1).clamp(min=0)\n",
        "    inter_h = (y2 - y1).clamp(min=0)\n",
        "    inter = inter_w * inter_h\n",
        "\n",
        "    area_p = (pred_box_xyxy[:, 2] - pred_box_xyxy[:, 0]).clamp(min=0) * \\\n",
        "             (pred_box_xyxy[:, 3] - pred_box_xyxy[:, 1]).clamp(min=0)\n",
        "    area_t = (tgt_box_xyxy[:, 2] - tgt_box_xyxy[:, 0]).clamp(min=0) * \\\n",
        "             (tgt_box_xyxy[:, 3] - tgt_box_xyxy[:, 1]).clamp(min=0)\n",
        "\n",
        "    union = area_p + area_t - inter + 1e-6\n",
        "    iou = inter / union  # [N]\n",
        "\n",
        "    # enclosing box\n",
        "    c_x1 = torch.min(pred_box_xyxy[:, 0], tgt_box_xyxy[:, 0])\n",
        "    c_y1 = torch.min(pred_box_xyxy[:, 1], tgt_box_xyxy[:, 1])\n",
        "    c_x2 = torch.max(pred_box_xyxy[:, 2], tgt_box_xyxy[:, 2])\n",
        "    c_y2 = torch.max(pred_box_xyxy[:, 3], tgt_box_xyxy[:, 3])\n",
        "\n",
        "    c_w = (c_x2 - c_x1).clamp(min=0)\n",
        "    c_h = (c_y2 - c_y1).clamp(min=0)\n",
        "    c_area = c_w * c_h + 1e-6\n",
        "\n",
        "    giou = iou - (c_area - union) / c_area  # [N]\n",
        "    loss = 1.0 - giou\n",
        "    return loss.mean()\n",
        "\n",
        "\n",
        "# focal loss 인스턴스 (object + class 둘 다 사용)\n",
        "_focal_loss = FocalLoss(alpha=0.25, gamma=2.0, reduction=\"mean\")\n",
        "\n",
        "\n",
        "def yolo_loss(preds, targets, img_size=512,\n",
        "              lambda_obj=1.0, lambda_cls=1.0, lambda_box=5.0):\n",
        "    \"\"\"\n",
        "    preds: list of 3 scales: [(cls3,obj3,box3), (cls4,obj4,box4), (cls5,obj5,box5)]\n",
        "    targets: list of len B, each [num_gt, 5] (cls, x, y, w, h), all normalized [0,1]\n",
        "    - GT를 scale별로 나누진 않고, 이전처럼 각 scale에 동일하게 할당\n",
        "    - 하지만:\n",
        "        * object/cls -> Focal Loss\n",
        "        * box -> GIoU Loss\n",
        "    \"\"\"\n",
        "    total_obj_loss = 0.0\n",
        "    total_cls_loss = 0.0\n",
        "    total_box_loss = 0.0\n",
        "\n",
        "    for scale_id, (cls_pred, obj_pred, box_pred) in enumerate(preds):\n",
        "        B, C, H, W = cls_pred.shape\n",
        "        device = cls_pred.device\n",
        "\n",
        "        # [B, H*W, C], [B, H*W, 1], [B, H*W, 4]\n",
        "        cls_p = cls_pred.permute(0, 2, 3, 1).reshape(B, H * W, C)\n",
        "        obj_p = obj_pred.permute(0, 2, 3, 1).reshape(B, H * W, 1)\n",
        "        box_p = box_pred.permute(0, 2, 3, 1).reshape(B, H * W, 4)\n",
        "\n",
        "        stride = img_size // H  # 이 scale의 stride\n",
        "\n",
        "        for b in range(B):\n",
        "            gt = targets[b]  # [num_gt, 5]\n",
        "            if gt.numel() == 0:\n",
        "                # GT 없으면 objectness만 전부 0으로 학습 (negative sample)\n",
        "                obj_tgt = torch.zeros((H * W, 1), device=device)\n",
        "                total_obj_loss += _focal_loss(obj_p[b], obj_tgt)\n",
        "                continue\n",
        "\n",
        "            # -------------------------\n",
        "            # GT 분리\n",
        "            # -------------------------\n",
        "            gcls = gt[:, 0].long()    # [num_gt]\n",
        "            gxy_norm = gt[:, 1:3]     # [num_gt, 2], normalized [0,1]\n",
        "            gwh_norm = gt[:, 3:5]     # [num_gt, 2], normalized [0,1]\n",
        "\n",
        "            # grid index 계산을 위한 pixel 단위 좌표\n",
        "            gxy_pix = gxy_norm * img_size\n",
        "\n",
        "            gx = (gxy_pix[:, 0] / stride).long().clamp(0, W - 1)\n",
        "            gy = (gxy_pix[:, 1] / stride).long().clamp(0, H - 1)\n",
        "            gi = gy * W + gx          # [num_gt]\n",
        "\n",
        "            # -------------------------\n",
        "            # objectness target\n",
        "            # -------------------------\n",
        "            obj_tgt = torch.zeros((H * W, 1), device=device)\n",
        "            obj_tgt[gi] = 1.0\n",
        "            total_obj_loss += _focal_loss(obj_p[b], obj_tgt)\n",
        "\n",
        "            # -------------------------\n",
        "            # class target (one-hot)\n",
        "            # -------------------------\n",
        "            cls_tgt = torch.zeros((H * W, C), device=device)\n",
        "            cls_tgt[gi, gcls] = 1.0\n",
        "            total_cls_loss += _focal_loss(cls_p[b], cls_tgt)\n",
        "\n",
        "            # -------------------------\n",
        "            # box GIoU Loss\n",
        "            # -------------------------\n",
        "            # 예측 box: [H*W, 4] -> 선택된 cell만 [num_gt, 4]\n",
        "            pred_raw = box_p[b][gi]          # [num_gt,4], logits\n",
        "            pred_box_norm_xywh = pred_raw.sigmoid()  # [0,1]\n",
        "\n",
        "            # 타깃도 normalized xywh\n",
        "            tgt_box_norm_xywh = torch.cat([gxy_norm, gwh_norm], dim=1)\n",
        "\n",
        "            # xyxy로 변환 후 GIoU\n",
        "            pred_xyxy = xywh_to_xyxy(pred_box_norm_xywh)  # [num_gt, 4]\n",
        "            tgt_xyxy = xywh_to_xyxy(tgt_box_norm_xywh)    # [num_gt, 4]\n",
        "\n",
        "            total_box_loss += giou_loss(pred_xyxy, tgt_xyxy)\n",
        "\n",
        "    total = (lambda_obj * total_obj_loss +\n",
        "             lambda_cls * total_cls_loss +\n",
        "             lambda_box * total_box_loss)\n",
        "    return total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ce9fc16",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 8: Decode Predictions + mAP Evaluation\n",
        "# ============================================\n",
        "\n",
        "import math\n",
        "\n",
        "def decode_predictions(preds, img_size=512, conf_thres=0.25,\n",
        "                       nms_iou_thres=0.5):\n",
        "    \"\"\"\n",
        "    preds: list of 3 scales\n",
        "    return: per-image list of [x1,y1,x2,y2,score,cls]\n",
        "    \"\"\"\n",
        "    all_outputs = []\n",
        "    B = preds[0][0].shape[0]\n",
        "\n",
        "    for b in range(B):\n",
        "        dets_all = []\n",
        "\n",
        "        for (cls_pred, obj_pred, box_pred) in preds:\n",
        "            B_s, C, H, W = cls_pred.shape\n",
        "\n",
        "            cls_logits = cls_pred[b].permute(1,2,0).reshape(H*W, C)   # [HW,C]\n",
        "            obj_logits = obj_pred[b].permute(1,2,0).reshape(H*W, 1)   # [HW,1]\n",
        "            box_logits = box_pred[b].permute(1,2,0).reshape(H*W, 4)   # [HW,4]\n",
        "\n",
        "            obj_scores = obj_logits.sigmoid().squeeze(-1)             # [HW]\n",
        "            cls_scores = cls_logits.sigmoid()                         # [HW,C]\n",
        "            box_norm   = box_logits.sigmoid()                         # [HW,4]\n",
        "\n",
        "            cls_max_scores, cls_ids = cls_scores.max(dim=-1)          # [HW]\n",
        "            scores = obj_scores * cls_max_scores                      # [HW]\n",
        "\n",
        "            # confidence threshold\n",
        "            mask = scores > conf_thres\n",
        "            if mask.sum() == 0:\n",
        "                continue\n",
        "\n",
        "            scores_ = scores[mask]            # [N]\n",
        "            cls_ids_ = cls_ids[mask]          # [N]\n",
        "            boxes = box_norm[mask]            # [N,4]\n",
        "\n",
        "            # cx,cy,w,h -> x1,y1,x2,y2 (image scale)\n",
        "            x_c = boxes[:, 0] * img_size\n",
        "            y_c = boxes[:, 1] * img_size\n",
        "            w   = boxes[:, 2] * img_size\n",
        "            h   = boxes[:, 3] * img_size\n",
        "\n",
        "            x1 = (x_c - w/2).clamp(0, img_size)\n",
        "            y1 = (y_c - h/2).clamp(0, img_size)\n",
        "            x2 = (x_c + w/2).clamp(0, img_size)\n",
        "            y2 = (y_c + h/2).clamp(0, img_size)\n",
        "\n",
        "            # NMS 적용\n",
        "            boxes_xyxy = torch.stack([x1, y1, x2, y2], dim=1)  # [N,4]\n",
        "            keep = nms(boxes_xyxy, scores_, iou_thres=nms_iou_thres)\n",
        "            if keep.numel() == 0:\n",
        "                continue\n",
        "\n",
        "            boxes_xyxy = boxes_xyxy[keep]\n",
        "            scores_ = scores_[keep]\n",
        "            cls_ids_ = cls_ids_[keep]\n",
        "\n",
        "            dets = torch.cat(\n",
        "                [boxes_xyxy,\n",
        "                 scores_.unsqueeze(1),\n",
        "                 cls_ids_.float().unsqueeze(1)],\n",
        "                dim=1\n",
        "            )  # [K, 6]\n",
        "            dets_all.append(dets)\n",
        "\n",
        "        if len(dets_all) == 0:\n",
        "            all_outputs.append([])\n",
        "        else:\n",
        "            all_outputs.append(torch.cat(dets_all, dim=0))\n",
        "    return all_outputs\n",
        "\n",
        "\n",
        "\n",
        "def box_iou(box1, box2):\n",
        "    N = box1.size(0)\n",
        "    M = box2.size(0)\n",
        "    if N == 0 or M == 0:\n",
        "        return torch.zeros(N, M)\n",
        "\n",
        "    tl = torch.max(box1[:, None, :2], box2[:, :2])\n",
        "    br = torch.min(box1[:, None, 2:], box2[:, 2:])\n",
        "\n",
        "    wh = (br - tl).clamp(min=0)\n",
        "    inter = wh[..., 0] * wh[..., 1]\n",
        "\n",
        "    area1 = (box1[:, 2]-box1[:, 0]) * (box1[:, 3]-box1[:, 1])\n",
        "    area2 = (box2[:, 2]-box2[:, 0]) * (box2[:, 3]-box2[:, 1])\n",
        "\n",
        "    iou = inter / (area1[:, None] + area2 - inter + 1e-6)\n",
        "    return iou\n",
        "\n",
        "def nms(boxes: torch.Tensor,\n",
        "        scores: torch.Tensor,\n",
        "        iou_thres: float = 0.5) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    boxes: [N, 4]  (x1,y1,x2,y2)\n",
        "    scores: [N]\n",
        "    return: keep indices (LongTensor)\n",
        "    \"\"\"\n",
        "    if boxes.numel() == 0:\n",
        "        return torch.zeros(0, dtype=torch.long, device=boxes.device)\n",
        "\n",
        "    # score 내림차순 정렬\n",
        "    idxs = scores.argsort(descending=True)\n",
        "    keep = []\n",
        "\n",
        "    while idxs.numel() > 0:\n",
        "        i = idxs[0]\n",
        "        keep.append(i.item())\n",
        "\n",
        "        if idxs.numel() == 1:\n",
        "            break\n",
        "\n",
        "        # 현재 선택된 박스 vs 나머지 박스 IoU 계산\n",
        "        ious = box_iou(\n",
        "            boxes[i].unsqueeze(0),\n",
        "            boxes[idxs[1:]]\n",
        "        ).squeeze(0)  # [N-1]\n",
        "\n",
        "        # IoU가 threshold 미만인 애들만 남기기\n",
        "        idxs = idxs[1:][ious < iou_thres]\n",
        "\n",
        "    return torch.tensor(keep, dtype=torch.long, device=boxes.device)\n",
        "\n",
        "def compute_ap(recall, precision):\n",
        "    mrec = torch.cat([torch.tensor([0.0]), recall, torch.tensor([1.0])])\n",
        "    mpre = torch.cat([torch.tensor([0.0]), precision, torch.tensor([0.0])])\n",
        "\n",
        "    for i in range(mpre.size(0)-1, 0, -1):\n",
        "        mpre[i-1] = torch.max(mpre[i-1], mpre[i])\n",
        "\n",
        "    idx = (mrec[1:] != mrec[:-1]).nonzero().squeeze()\n",
        "    ap = ((mrec[idx+1] - mrec[idx]) * mpre[idx+1]).sum()\n",
        "    return ap.item()\n",
        "\n",
        "def evaluate_map(model, dataloader, num_classes=3, img_size=512,\n",
        "                 iou_thr=0.5, conf_thres=0.25):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    all_dets = {c: [] for c in range(num_classes)}\n",
        "    all_gts  = {c: [] for c in range(num_classes)}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for img_id, (imgs, targets) in enumerate(dataloader):\n",
        "            imgs = imgs.to(device)\n",
        "            targets = [t.to(device) for t in targets]\n",
        "\n",
        "            preds, aux = model(imgs)\n",
        "            dets = decode_predictions(preds, img_size=img_size,\n",
        "                                      conf_thres=conf_thres)\n",
        "\n",
        "            dets = dets[0] if len(dets) > 0 else []\n",
        "\n",
        "            gt = targets[0]\n",
        "            if len(gt) > 0:\n",
        "                gcls = gt[:, 0].long()\n",
        "                gxy  = gt[:, 1:3] * img_size\n",
        "                gwh  = gt[:, 3:5] * img_size\n",
        "\n",
        "                gx1 = gxy[:, 0] - gwh[:, 0] / 2\n",
        "                gy1 = gxy[:, 1] - gwh[:, 1] / 2\n",
        "                gx2 = gxy[:, 0] + gwh[:, 0] / 2\n",
        "                gy2 = gxy[:, 1] + gwh[:, 1] / 2\n",
        "                gboxes = torch.stack([gx1, gy1, gx2, gy2], dim=1)\n",
        "\n",
        "                for c in range(num_classes):\n",
        "                    mask = (gcls == c)\n",
        "                    if mask.sum() > 0:\n",
        "                        all_gts[c].append((img_id, gboxes[mask].cpu()))\n",
        "\n",
        "            if dets is not None and len(dets) > 0:\n",
        "                boxes = dets[:, :4]\n",
        "                scores = dets[:, 4]\n",
        "                cls_ids = dets[:, 5].long()\n",
        "                for c in range(num_classes):\n",
        "                    mask = (cls_ids == c)\n",
        "                    if mask.sum() > 0:\n",
        "                        all_dets[c].append(\n",
        "                            (img_id, scores[mask].cpu(),\n",
        "                             boxes[mask].cpu())\n",
        "                        )\n",
        "\n",
        "    aps = []\n",
        "    for c in range(num_classes):\n",
        "        gts_c = all_gts[c]\n",
        "        if len(gts_c) == 0:\n",
        "            continue\n",
        "\n",
        "        n_gt = sum(boxes.size(0) for _, boxes in gts_c)\n",
        "        gt_dict = {}\n",
        "        for img_id, boxes in gts_c:\n",
        "            gt_dict.setdefault(img_id, [])\n",
        "            gt_dict[img_id].append({\n",
        "                \"boxes\": boxes,\n",
        "                \"matched\": torch.zeros(boxes.size(0),\n",
        "                                       dtype=torch.bool)\n",
        "            })\n",
        "\n",
        "        dets_c = all_dets[c]\n",
        "        if len(dets_c) == 0:\n",
        "            aps.append(0.0)\n",
        "            continue\n",
        "\n",
        "        scores_all = []\n",
        "        boxes_all = []\n",
        "        img_ids_all = []\n",
        "        for img_id, scores, boxes in dets_c:\n",
        "            for i in range(boxes.size(0)):\n",
        "                scores_all.append(scores[i].item())\n",
        "                boxes_all.append(boxes[i])\n",
        "                img_ids_all.append(img_id)\n",
        "\n",
        "        scores_all = torch.tensor(scores_all)\n",
        "        boxes_all = torch.stack(boxes_all, dim=0)\n",
        "\n",
        "        order = scores_all.argsort(descending=True)\n",
        "        scores_all = scores_all[order]\n",
        "        boxes_all = boxes_all[order]\n",
        "        img_ids_all = [img_ids_all[i] for i in order]\n",
        "\n",
        "        tps = torch.zeros(len(scores_all))\n",
        "        fps = torch.zeros(len(scores_all))\n",
        "\n",
        "        for i in range(len(scores_all)):\n",
        "            img_id = img_ids_all[i]\n",
        "            pred_box = boxes_all[i].unsqueeze(0)\n",
        "\n",
        "            if img_id not in gt_dict:\n",
        "                fps[i] = 1\n",
        "                continue\n",
        "\n",
        "            gt_entry = gt_dict[img_id][0]\n",
        "            gboxes = gt_entry[\"boxes\"]\n",
        "            matched = gt_entry[\"matched\"]\n",
        "\n",
        "            ious = box_iou(pred_box, gboxes).squeeze(0)\n",
        "            max_iou, max_idx = ious.max(0)\n",
        "\n",
        "            if max_iou >= iou_thr and not matched[max_idx]:\n",
        "                tps[i] = 1\n",
        "                matched[max_idx] = True\n",
        "            else:\n",
        "                fps[i] = 1\n",
        "\n",
        "        tp_cum = torch.cumsum(tps, dim=0)\n",
        "        fp_cum = torch.cumsum(fps, dim=0)\n",
        "        recall = tp_cum / (n_gt + 1e-6)\n",
        "        precision = tp_cum / (tp_cum + fp_cum + 1e-6)\n",
        "\n",
        "        ap_c = compute_ap(recall, precision)\n",
        "        aps.append(ap_c)\n",
        "\n",
        "    mAP = sum(aps) / len(aps) if len(aps) > 0 else 0.0\n",
        "    return mAP, aps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb248c1f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 9: 학습 루프 + Val mAP 기준 best.pt 저장\n",
        "# ============================================\n",
        "\n",
        "if 'IMG_SIZE' not in globals():\n",
        "    IMG_SIZE = 640\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 모델 설정(학습/검증/테스트 공통)\n",
        "cfg = dict(\n",
        "    in_ch=3,\n",
        "    stem_base=32,\n",
        "    embed_dim=256,\n",
        "    vit_depth=4,\n",
        "    vit_heads=4,\n",
        "    num_classes=3,\n",
        "    iters=1,\n",
        "    detach_feedback=True,\n",
        ")\n",
        "\n",
        "model = HybridTwoWay(**cfg).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "EPOCHS = 5\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "best_map = 0.0\n",
        "best_epoch = -1\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # ------------------------------\n",
        "    # 1) Train\n",
        "    # ------------------------------\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "    for imgs, targets in loop:\n",
        "        imgs = imgs.to(device)\n",
        "        targets = [t.to(device) for t in targets]\n",
        "\n",
        "        preds, aux = model(imgs)\n",
        "        loss = yolo_loss(preds, targets, img_size=IMG_SIZE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} | Train Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # ------------------------------\n",
        "    # 2) Validation mAP\n",
        "    # ------------------------------\n",
        "    val_map, val_aps = evaluate_map(\n",
        "        model,\n",
        "        val_loader,\n",
        "        num_classes=cfg[\"num_classes\"],\n",
        "        img_size=IMG_SIZE\n",
        "    )\n",
        "    print(f\"Epoch {epoch+1} | Val mAP@0.5: {val_map:.4f}\")\n",
        "\n",
        "    # ------------------------------\n",
        "    # 3) best 모델 저장\n",
        "    # ------------------------------\n",
        "    if val_map > best_map:\n",
        "        best_map = val_map\n",
        "        best_epoch = epoch + 1\n",
        "        ckpt = {\n",
        "            \"state_dict\": model.state_dict(),\n",
        "            \"cfg\": cfg,\n",
        "            \"epoch\": best_epoch,\n",
        "            \"val_map\": best_map,\n",
        "        }\n",
        "        torch.save(ckpt, \"hybrid_two_way_best.pt\")\n",
        "        print(f\"✅ New best model saved at epoch {best_epoch} | Val mAP: {best_map:.4f}\")\n",
        "\n",
        "print(\"학습 완료!\")\n",
        "print(f\"Best epoch: {best_epoch}, Best Val mAP: {best_map:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dded0d33",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 저장된 Best 모델 불러오기 및 테스트 평가\n",
        "# ============================================\n",
        "\n",
        "# 1. 체크포인트 파일 로드\n",
        "checkpoint_path = \"hybrid_two_way_best.pt\"\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "# 2. 저장된 Config를 이용해 모델 초기화 (학습 때와 동일한 구조 보장)\n",
        "loaded_cfg = checkpoint[\"cfg\"]\n",
        "model = HybridTwoWay(**loaded_cfg).to(device)\n",
        "\n",
        "# 3. 가중치(state_dict) 로드\n",
        "# 저장할 때 \"state_dict\" 키에 가중치를 담았으므로, 이를 꺼내서 로드해야 함\n",
        "model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\n",
        "model.eval()\n",
        "print(f\"✅ 모델 로드 완료 (Epoch: {checkpoint['epoch']}, Val mAP: {checkpoint['val_map']:.4f})\")\n",
        "\n",
        "# 4. 테스트셋 평가\n",
        "test_map, class_aps = evaluate_map(\n",
        "    model, test_loader, num_classes=loaded_cfg[\"num_classes\"], img_size=IMG_SIZE\n",
        ")\n",
        "\n",
        "print(f\"\\nTest mAP@0.5: {test_map:.4f}\")\n",
        "for i, ap in enumerate(class_aps):\n",
        "    print(f\"  Class {i} AP@0.5: {ap:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Quick Sanity Check\n",
        "Colab에서 바로 실행해 모델 입출력 형태를 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 10: Quick Sanity Check (입출력 shape 확인)\n",
        "# ============================================\n",
        "\n",
        "x = torch.randn(2, 3, 640, 640).to(device)\n",
        "preds, aux = model(x)\n",
        "\n",
        "for level, (c, o, b) in zip([\"P3\",\"P4\",\"P5\"], preds):\n",
        "    print(f\"[{level}] cls: {list(c.shape)}, obj: {list(o.shape)}, box: {list(b.shape)}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
