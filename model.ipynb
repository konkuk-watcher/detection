{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HybridTwoWay Model (Colab Ready)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\n",
        "필요한 PyTorch 모듈과 타입 힌트를 불러옵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77da3f4e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# dataset 가져오기\n",
        "import os\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# --- 옵션 1: Kaggle 데이터셋 사용 (kagglehub 사용) --- #\n",
        "# !pip install kagglehub\n",
        "import kagglehub\n",
        "\n",
        "# 1. 사용할 Kaggle 데이터셋의 핸들(handle)을 입력하세요.\n",
        "#    예: 'kaggle/your-user/your-dataset'\n",
        "KAGGLE_DATASET_HANDLE = 'kaggle/your-user/your-dataset' # <--- 여기에 핸들 입력\n",
        "DATASET_PATH = kagglehub.snapshot_download(KAGGLE_DATASET_HANDLE)\n",
        "\n",
        "# 2. 다운로드된 경로에서 이미지 파일을 로드하는 예시\n",
        "image_folder = os.path.join(DATASET_PATH, 'images') # <--- 이미지 폴더 경로 지정 (데이터셋 구조에 따라 변경)\n",
        "if os.path.exists(image_folder):\n",
        "    image_files = [os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]\n",
        "    if image_files:\n",
        "        print(f'Found {len(image_files)} images in {image_folder}. Loading a sample image...')\n",
        "        sample_image_path = image_files[0]\n",
        "        sample_image = Image.open(sample_image_path).convert('RGB')\n",
        "        print(f'Sample image loaded from {sample_image_path}. Size: {sample_image.size}')\n",
        "        # sample_image.show() # Colab에서는 직접 이미지 표시가 어려울 수 있습니다.\n",
        "    else:\n",
        "        print(f'No image files found in {image_folder}.')\n",
        "else:\n",
        "    print(f'Image folder not found at {image_folder}. Please check the path inside the dataset.')\n",
        "    print(f'Available files/folders in {DATASET_PATH}:', os.listdir(DATASET_PATH))\n",
        "\n",
        "\n",
        "# --- 옵션 2: Google Drive 데이터셋 사용 --- #\n",
        "# 1. 아래 주석을 해제하여 Google Drive를 마운트하세요.\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# 2. Google Drive에 있는 이미지 폴더의 전체 경로를 'GDRIVE_IMAGE_FOLDER'에 입력하세요.\n",
        "# GDRIVE_IMAGE_FOLDER = '/content/drive/My Drive/your_image_folder' # <--- 여기에 이미지 폴더 경로 입력\n",
        "# if os.path.exists(GDRIVE_IMAGE_FOLDER):\n",
        "#     gdrive_image_files = [os.path.join(GDRIVE_IMAGE_FOLDER, f) for f in os.listdir(GDRIVE_IMAGE_FOLDER) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]\n",
        "#     if gdrive_image_files:\n",
        "#         print(f'Found {len(gdrive_image_files)} images in {GDRIVE_IMAGE_FOLDER}. Loading a sample image...')\n",
        "#         gdrive_sample_image_path = gdrive_image_files[0]\n",
        "#         gdrive_sample_image = Image.open(gdrive_sample_image_path).convert('RGB')\n",
        "#         print(f'Sample image loaded from {gdrive_sample_image_path}. Size: {gdrive_sample_image.size}')\n",
        "#     else:\n",
        "#         print(f'No image files found in {GDRIVE_IMAGE_FOLDER}.')\n",
        "# else:\n",
        "#     print(f'Image folder not found at {GDRIVE_IMAGE_FOLDER}. Please check the path.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "roboflow-cell",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 옵션 3: Roboflow 데이터셋 사용 --- #\n",
        "# !pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"HG9M6YJZpcCUgAQaKO9v\")\n",
        "project = rf.workspace(\"arakon\").project(\"detection-base-hqaeg\")\n",
        "version = project.version(6)\n",
        "dataset = version.download(\"yolov8\")\n",
        "\n",
        "print(f'Roboflow dataset downloaded to: {dataset.location}')\n",
        "                "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def conv_bn_act(in_ch, out_ch, k=3, s=1, p=1, act=True):\n",
        "    m = [nn.Conv2d(in_ch, out_ch, k, s, p, bias=False),\n",
        "         nn.BatchNorm2d(out_ch)]\n",
        "    if act:\n",
        "        m.append(nn.SiLU(inplace=True))\n",
        "    return nn.Sequential(*m)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Anomaly-Aware CNN Stem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FixedGaussianBlur(nn.Module):\n",
        "    def __init__(self, channels, k=5, sigma=1.0):\n",
        "        super().__init__()\n",
        "        grid = torch.arange(k).float() - (k - 1) / 2\n",
        "        gauss = torch.exp(-(grid ** 2) / (2 * sigma ** 2))\n",
        "        kernel1d = gauss / gauss.sum()\n",
        "        kernel2d = torch.outer(kernel1d, kernel1d)\n",
        "        weight = kernel2d[None, None, :, :].repeat(channels, 1, 1, 1)\n",
        "        self.register_buffer('weight', weight)\n",
        "        self.groups = channels\n",
        "        self.k = k\n",
        "\n",
        "    def forward(self, x):\n",
        "        pad = (self.k // 2,) * 4\n",
        "        return F.conv2d(F.pad(x, pad, mode='reflect'), self.weight, groups=self.groups)\n",
        "\n",
        "\n",
        "class AnomalyAwareStem(nn.Module):\n",
        "    def __init__(self, in_ch=3, base_ch=48):\n",
        "        super().__init__()\n",
        "        C1, C2, C3 = base_ch, base_ch * 2, base_ch * 4\n",
        "        self.stem = nn.Sequential(\n",
        "            conv_bn_act(in_ch, C1, 3, 2, 1),\n",
        "            conv_bn_act(C1, C2, 3, 2, 1),\n",
        "            conv_bn_act(C2, C3, 3, 2, 1),\n",
        "        )\n",
        "        self.blur = FixedGaussianBlur(in_ch, k=5, sigma=1.0)\n",
        "        self.anom = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, in_ch, 3, 1, 1, groups=in_ch, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_ch, C3 // 4, 1, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(C3 // 4),\n",
        "            nn.SiLU(inplace=True),\n",
        "        )\n",
        "        self.fuse = nn.Conv2d(C3 + C3 // 4, C3, 1, 1, 0, bias=False)\n",
        "        self.fuse_bn = nn.BatchNorm2d(C3)\n",
        "        self.vis_head = nn.Conv2d(C3, 1, 1, 1, 0)\n",
        "\n",
        "    @property\n",
        "    def out_channels(self):\n",
        "        return 4 * 48\n",
        "\n",
        "    def forward(self, x):\n",
        "        f_main = self.stem(x)\n",
        "        blurred = self.blur(x)\n",
        "        high = x - blurred\n",
        "        high_ds = F.interpolate(high, size=f_main.shape[-2:], mode='bilinear', align_corners=False)\n",
        "        f_anom = self.anom(high_ds)\n",
        "        f = torch.cat([f_main, f_anom], dim=1)\n",
        "        f = self.fuse_bn(self.fuse(f))\n",
        "        f = F.silu(f, inplace=True)\n",
        "        v = torch.sigmoid(self.vis_head(f_main))\n",
        "        return f, v\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Vision Transformer Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PatchEmbed1x1(nn.Module):\n",
        "    \"\"\"Map CNN features to ViT embeddings while keeping spatial resolution.\"\"\"\n",
        "    def __init__(self, in_ch, embed_dim):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(in_ch, embed_dim, 1, 1, 0, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        x = self.bn(x)\n",
        "        x = F.silu(x, inplace=True)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, dim, mlp_ratio=4.0, drop=0.0):\n",
        "        super().__init__()\n",
        "        hidden = int(dim * mlp_ratio)\n",
        "        self.fc1 = nn.Linear(dim, hidden)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden, dim)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiheadSelfAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, attn_drop=0.0, proj_drop=0.0):\n",
        "        super().__init__()\n",
        "        assert dim % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=True)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        out = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        out = self.proj(out)\n",
        "        out = self.proj_drop(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, mlp_ratio=4.0, drop=0.0):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = MultiheadSelfAttention(dim, num_heads, drop, drop)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.mlp = MLP(dim, mlp_ratio, drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class ViTEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim=512, depth=8, num_heads=8):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, mlp_ratio=4.0, drop=0.0)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        for blk in self.blocks:\n",
        "            tokens = blk(tokens)\n",
        "        return tokens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Feedback Adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeedbackAdapter(nn.Module):\n",
        "    def __init__(self, d_token: int, c_stem: int, use_bn: bool = True):\n",
        "        super().__init__()\n",
        "        layers = [nn.Conv2d(d_token, c_stem * 2, 1, 1, 0, bias=not use_bn)]\n",
        "        if use_bn:\n",
        "            layers.append(nn.BatchNorm2d(c_stem * 2))\n",
        "        layers.append(nn.SiLU(inplace=True))\n",
        "        self.adapter = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, tokens: torch.Tensor, Ht: int, Wt: int, f_stem: torch.Tensor):\n",
        "        B, N, D = tokens.shape\n",
        "        t2d = tokens.transpose(1, 2).reshape(B, D, Ht, Wt)\n",
        "        ab = self.adapter(t2d)\n",
        "        Cs = f_stem.shape[1]\n",
        "        gamma, beta = torch.split(ab, Cs, dim=1)\n",
        "        return f_stem * (1 + torch.tanh(gamma)) + beta\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. PAN-Lite Neck"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PANLite(nn.Module):\n",
        "    def __init__(self, in_ch=512, mid=256):\n",
        "        super().__init__()\n",
        "        self.lateral = conv_bn_act(in_ch, mid, 1, 1, 0)\n",
        "        self.down4 = conv_bn_act(mid, mid, 3, 2, 1)\n",
        "        self.down5 = conv_bn_act(mid, mid, 3, 2, 1)\n",
        "        self.up4 = conv_bn_act(mid + mid, mid, 3, 1, 1)\n",
        "        self.up3 = conv_bn_act(mid + mid, mid, 3, 1, 1)\n",
        "        self.down_f4 = conv_bn_act(mid, mid, 3, 2, 1)\n",
        "        self.fuse4 = conv_bn_act(mid + mid, mid, 3, 1, 1)\n",
        "        self.down_f5 = conv_bn_act(mid, mid, 3, 2, 1)\n",
        "        self.fuse5 = conv_bn_act(mid + mid, mid, 3, 1, 1)\n",
        "\n",
        "    def forward(self, p3):\n",
        "        p3 = self.lateral(p3)\n",
        "        p4 = self.down4(p3)\n",
        "        p5 = self.down5(p4)\n",
        "        p4u = F.interpolate(p5, size=p4.shape[-2:], mode='nearest')\n",
        "        p4 = self.up4(torch.cat([p4, p4u], dim=1))\n",
        "        p3u = F.interpolate(p4, size=p3.shape[-2:], mode='nearest')\n",
        "        p3 = self.up3(torch.cat([p3, p3u], dim=1))\n",
        "        p4b = self.down_f4(p3)\n",
        "        p4 = self.fuse4(torch.cat([p4, p4b], dim=1))\n",
        "        p5b = self.down_f5(p4)\n",
        "        p5 = self.fuse5(torch.cat([p5, p5b], dim=1))\n",
        "        return p3, p4, p5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. YOLO-style Detection Head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class YOLOHeadLite(nn.Module):\n",
        "    def __init__(self, in_ch=256, num_classes=1, reg_max=0):\n",
        "        super().__init__()\n",
        "        c = in_ch\n",
        "        self.stem3 = conv_bn_act(c, c, 3, 1, 1)\n",
        "        self.stem4 = conv_bn_act(c, c, 3, 1, 1)\n",
        "        self.stem5 = conv_bn_act(c, c, 3, 1, 1)\n",
        "        self.cls3 = nn.Conv2d(c, num_classes, 1, 1, 0)\n",
        "        self.obj3 = nn.Conv2d(c, 1, 1, 1, 0)\n",
        "        self.box3 = nn.Conv2d(c, 4, 1, 1, 0)\n",
        "        self.cls4 = nn.Conv2d(c, num_classes, 1, 1, 0)\n",
        "        self.obj4 = nn.Conv2d(c, 1, 1, 1, 0)\n",
        "        self.box4 = nn.Conv2d(c, 4, 1, 1, 0)\n",
        "        self.cls5 = nn.Conv2d(c, num_classes, 1, 1, 0)\n",
        "        self.obj5 = nn.Conv2d(c, 1, 1, 1, 0)\n",
        "        self.box5 = nn.Conv2d(c, 4, 1, 1, 0)\n",
        "\n",
        "    def forward_single(self, x, stem, cls, obj, box):\n",
        "        f = stem(x)\n",
        "        return cls(f), obj(f), box(f)\n",
        "\n",
        "    def forward(self, p3, p4, p5):\n",
        "        c3, o3, b3 = self.forward_single(p3, self.stem3, self.cls3, self.obj3, self.box3)\n",
        "        c4, o4, b4 = self.forward_single(p4, self.stem4, self.cls4, self.obj4, self.box4)\n",
        "        c5, o5, b5 = self.forward_single(p5, self.stem5, self.cls5, self.obj5, self.box5)\n",
        "        return [(c3, o3, b3), (c4, o4, b4), (c5, o5, b5)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. HybridTwoWay Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HybridTwoWay(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_ch=3,\n",
        "        stem_base=48,\n",
        "        embed_dim=512,\n",
        "        vit_depth=8,\n",
        "        vit_heads=8,\n",
        "        num_classes=1,\n",
        "        iters=1,\n",
        "        detach_feedback=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert iters >= 1\n",
        "        self.iters = iters\n",
        "        self.detach_feedback = detach_feedback\n",
        "        self.stem = AnomalyAwareStem(in_ch=in_ch, base_ch=stem_base)\n",
        "        c_stem = stem_base * 4\n",
        "        self.patch = PatchEmbed1x1(c_stem, embed_dim)\n",
        "        self.vit = ViTEncoder(embed_dim=embed_dim, depth=vit_depth, num_heads=vit_heads)\n",
        "        self.feedback = FeedbackAdapter(embed_dim, c_stem, use_bn=True)\n",
        "        self.neck = PANLite(in_ch=embed_dim, mid=256)\n",
        "        self.head = YOLOHeadLite(in_ch=256, num_classes=num_classes)\n",
        "\n",
        "    def forward_once(self, x):\n",
        "        f_stem, vis = self.stem(x)\n",
        "        p = self.patch(f_stem)\n",
        "        Ht, Wt = p.shape[-2:]\n",
        "        tokens = p.flatten(2).transpose(1, 2)\n",
        "        tokens = self.vit(tokens)\n",
        "        toks_for_fb = tokens.detach() if self.detach_feedback else tokens\n",
        "        f_fb = self.feedback(toks_for_fb, Ht, Wt, f_stem)\n",
        "        p3_in = self.patch(f_fb)\n",
        "        p3 = p3_in\n",
        "        p3, p4, p5 = self.neck(p3)\n",
        "        preds = self.head(p3, p4, p5)\n",
        "        aux = {\"P3\": p3, \"P4\": p4, \"P5\": p5, \"V\": vis}\n",
        "        return preds, aux, f_fb\n",
        "\n",
        "    def forward(self, x):\n",
        "        preds, aux, f_fb = self.forward_once(x)\n",
        "        for _ in range(self.iters - 1):\n",
        "            preds, aux, f_fb = self.forward_once(x)\n",
        "        return preds, aux\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Quick Sanity Check\n",
        "Colab에서 바로 실행해 모델 입출력 형태를 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = HybridTwoWay(\n",
        "    in_ch=3,\n",
        "    stem_base=48,\n",
        "    embed_dim=512,\n",
        "    vit_depth=8,\n",
        "    vit_heads=8,\n",
        "    num_classes=1,\n",
        "    iters=1,\n",
        "    detach_feedback=True,\n",
        ")\n",
        "\n",
        "x = torch.randn(2, 3, 640, 640)\n",
        "preds, aux = model(x)\n",
        "for i, (c, o, b) in enumerate(preds, start=3):\n",
        "    print(f\"[TwoWay] P{i} cls:{list(c.shape)} obj:{list(o.shape)} box:{list(b.shape)}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
