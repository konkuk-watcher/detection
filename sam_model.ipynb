{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82a72e65",
      "metadata": {},
      "outputs": [],
      "source": [
        "# cell 1\n",
        "!pip install -q roboflow torch torchvision opencv-python numpy tqdm segment-anything\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from roboflow import Roboflow\n",
        "from tqdm import tqdm\n",
        "from segment_anything import sam_model_registry\n",
        "\n",
        "# ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞\n",
        "IMG_SIZE = 1024\n",
        "BATCH_SIZE = 4\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f\"üî• Device: {DEVICE}, Resolution: {IMG_SIZE}x{IMG_SIZE}\")\n",
        "\n",
        "# SAM Í∞ÄÏ§ëÏπò Îã§Ïö¥Î°úÎìú\n",
        "if not os.path.exists(\"sam_vit_b_01ec64.pth\"):\n",
        "    print(\"‚¨áÔ∏è Downloading SAM weights...\")\n",
        "    !wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n",
        "\n",
        "# dataset\n",
        "rf = Roboflow(api_key=\"HG9M6YJZpcCUgAQaKO9v\")\n",
        "project = rf.workspace(\"arakon\").project(\"detection-base-hqaeg\")\n",
        "version = project.version(6)\n",
        "dataset = version.download(\"yolov8\")\n",
        "\n",
        "# 2. Dataset ÌÅ¥ÎûòÏä§\n",
        "def yolo_collate_fn(batch):\n",
        "    imgs = torch.stack([b[0] for b in batch], 0)\n",
        "    targets = [b[1] for b in batch]\n",
        "    return imgs, targets\n",
        "\n",
        "class YoloDataset(Dataset):\n",
        "    def __init__(self, root, img_size=1024):\n",
        "        self.img_dir = os.path.join(root, \"images\")\n",
        "        self.label_dir = os.path.join(root, \"labels\")\n",
        "        self.files = sorted([x for x in os.listdir(self.img_dir) if x.endswith(('.jpg', '.png', '.jpeg'))])\n",
        "        self.img_size = img_size\n",
        "\n",
        "    def __len__(self): return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.files[idx]\n",
        "        img_path = os.path.join(self.img_dir, name)\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        # Resize & Normalize\n",
        "        img = cv2.resize(img, (self.img_size, self.img_size))\n",
        "        img_tensor = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n",
        "\n",
        "        label_path = os.path.join(self.label_dir, name.rsplit('.', 1)[0] + \".txt\")\n",
        "        boxes = []\n",
        "        if os.path.exists(label_path):\n",
        "            with open(label_path, \"r\") as f:\n",
        "                for line in f.readlines():\n",
        "                    vals = list(map(float, line.strip().split()))\n",
        "                    if len(vals) == 5:\n",
        "                        boxes.append(vals) # cls, cx, cy, w, h\n",
        "        \n",
        "        target = torch.tensor(boxes, dtype=torch.float32) if len(boxes) > 0 else torch.zeros((0, 5))\n",
        "        return img_tensor, target\n",
        "\n",
        "# DataLoader ÏÉùÏÑ±\n",
        "train_loader = DataLoader(YoloDataset(dataset.location + \"/train\", IMG_SIZE), batch_size=BATCH_SIZE, shuffle=True, collate_fn=yolo_collate_fn, num_workers=2)\n",
        "val_loader = DataLoader(YoloDataset(dataset.location + \"/valid\", IMG_SIZE), batch_size=BATCH_SIZE, shuffle=False, collate_fn=yolo_collate_fn, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "143c6985",
      "metadata": {},
      "outputs": [],
      "source": [
        "# cell 2\n",
        "class ConvNormAct(nn.Module):\n",
        "    def __init__(self, in_c, out_c, k=3, s=1, p=1):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_c, out_c, k, s, p, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_c)\n",
        "        self.act = nn.SiLU()\n",
        "    def forward(self, x): return self.act(self.bn(self.conv(x)))\n",
        "\n",
        "class SAMDetector(nn.Module):\n",
        "    def __init__(self, checkpoint_path=\"sam_vit_b_01ec64.pth\", num_classes=3):\n",
        "        super().__init__()\n",
        "        print(\"üîÑ Loading SAM Backbone...\")\n",
        "        self.sam = sam_model_registry[\"vit_b\"](checkpoint=checkpoint_path)\n",
        "        \n",
        "        # Backbone Freeze\n",
        "        for param in self.sam.image_encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "            \n",
        "        self.sam_out_dim = 256\n",
        "        \n",
        "        # Adapter (FPN)\n",
        "        self.p4_conv = ConvNormAct(256, 256)\n",
        "        self.p3_up = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.p3_conv = ConvNormAct(256, 256)\n",
        "        self.p5_down = ConvNormAct(256, 256, k=3, s=2, p=1)\n",
        "        \n",
        "        # Detection Head\n",
        "        self.heads = nn.ModuleList([\n",
        "            nn.Conv2d(256, num_classes + 5, 1) for _ in range(3)\n",
        "        ])\n",
        "        \n",
        "    def forward(self, x):\n",
        "        with torch.no_grad():\n",
        "            features = self.sam.image_encoder(x)\n",
        "            \n",
        "        p4 = self.p4_conv(features)          \n",
        "        p3 = self.p3_conv(self.p3_up(p4))    \n",
        "        p5 = self.p5_down(p4)                \n",
        "        \n",
        "        outputs = []\n",
        "        for i, feat in enumerate([p3, p4, p5]):\n",
        "            out = self.heads[i](feat)\n",
        "            cls_p = out[:, :3, ...]\n",
        "            obj_p = out[:, 3:4, ...]\n",
        "            box_p = out[:, 4:, ...]\n",
        "            outputs.append((cls_p, obj_p, box_p))\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8c83488",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# [Cell 3] Loss Function & Utils (NMS Fixed)\n",
        "# =========================================================\n",
        "\n",
        "# 1. IoU Í≥ÑÏÇ∞ Ìï®Ïàò\n",
        "def box_iou(box1, box2):\n",
        "    lt = torch.max(box1[:, :2], box2[:, :2])\n",
        "    rb = torch.min(box1[:, 2:], box2[:, 2:])\n",
        "    wh = (rb - lt).clamp(min=0)\n",
        "    inter = wh[:, 0] * wh[:, 1]\n",
        "    \n",
        "    area1 = (box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1])\n",
        "    area2 = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1])\n",
        "    union = area1 + area2 - inter + 1e-6\n",
        "    \n",
        "    return inter / union\n",
        "\n",
        "# 2. Loss Function\n",
        "class ComputeLoss(nn.Module):\n",
        "    def __init__(self, num_classes=3):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def forward(self, preds, batch_targets, img_size):\n",
        "        loss_total = 0.0\n",
        "        strides = [8, 16, 32]\n",
        "        \n",
        "        for i, (cls_p, obj_p, box_p) in enumerate(preds):\n",
        "            stride = strides[i]\n",
        "            B, _, H, W = cls_p.shape\n",
        "            device = cls_p.device\n",
        "            \n",
        "            grid_y, grid_x = torch.meshgrid(torch.arange(H, device=device), torch.arange(W, device=device), indexing='ij')\n",
        "            grid = torch.stack((grid_x, grid_y), 2).float()\n",
        "            \n",
        "            b_sig = box_p.permute(0, 2, 3, 1).sigmoid() \n",
        "            pred_cx = (b_sig[..., 0] * 2.0 - 0.5 + grid[..., 0]) * stride\n",
        "            pred_cy = (b_sig[..., 1] * 2.0 - 0.5 + grid[..., 1]) * stride\n",
        "            pred_w = (b_sig[..., 2] * 2.0) ** 2 * stride\n",
        "            pred_h = (b_sig[..., 3] * 2.0) ** 2 * stride\n",
        "            \n",
        "            pred_xyxy = torch.stack([pred_cx - pred_w/2, pred_cy - pred_h/2, \n",
        "                                     pred_cx + pred_w/2, pred_cy + pred_h/2], dim=-1)\n",
        "            \n",
        "            anchors_flat = (grid + 0.5) * stride \n",
        "            anchors_flat = anchors_flat.view(-1, 2)\n",
        "            \n",
        "            obj_target = torch.zeros(B, H, W, 1, device=device)\n",
        "            cls_target = torch.zeros(B, H, W, self.num_classes, device=device)\n",
        "            loss_box_layer = 0.0\n",
        "            num_pos_layer = 0\n",
        "            \n",
        "            for b in range(B):\n",
        "                gt = batch_targets[b]\n",
        "                if len(gt) == 0: continue\n",
        "                gt_cls = gt[:, 0].long().to(device)\n",
        "                cx, cy, w, h = gt[:, 1]*img_size, gt[:, 2]*img_size, gt[:, 3]*img_size, gt[:, 4]*img_size\n",
        "                gt_xyxy = torch.stack([cx-w/2, cy-h/2, cx+w/2, cy+h/2], dim=1).to(device)\n",
        "                \n",
        "                lt = gt_xyxy[:, None, :2]\n",
        "                rb = gt_xyxy[:, None, 2:]\n",
        "                deltas = torch.cat((anchors_flat[None] - lt, rb - anchors_flat[None]), dim=2)\n",
        "                is_in_gt = deltas.min(dim=2)[0] > 0 \n",
        "                matched_gt_idx = is_in_gt.max(dim=0)[1] \n",
        "                has_match = is_in_gt.max(dim=0)[0] \n",
        "                \n",
        "                if has_match.sum() > 0:\n",
        "                    pos_mask = has_match.view(H, W)\n",
        "                    obj_target[b, pos_mask] = 1.0\n",
        "                    cls_target[b, pos_mask, gt_cls[matched_gt_idx[has_match]]] = 1.0\n",
        "                    iou = box_iou(pred_xyxy[b, pos_mask], gt_xyxy[matched_gt_idx[has_match]])\n",
        "                    loss_box_layer += (1.0 - iou).sum()\n",
        "                    num_pos_layer += has_match.sum()\n",
        "\n",
        "            loss_obj = F.binary_cross_entropy_with_logits(obj_p.permute(0,2,3,1).reshape(-1,1), obj_target.reshape(-1,1), reduction='sum')\n",
        "            loss_cls = F.binary_cross_entropy_with_logits(cls_p.permute(0,2,3,1).reshape(-1,self.num_classes), cls_target.reshape(-1,self.num_classes), reduction='sum')\n",
        "            loss_total += (loss_obj + loss_cls + loss_box_layer * 5.0) / max(1.0, num_pos_layer) / B\n",
        "\n",
        "        return loss_total\n",
        "\n",
        "# 3. Post Processing (NMS Ìè¨Ìï®) - [IndexError ÏàòÏ†ïÎê®]\n",
        "def post_process(outputs, conf_thres=0.5, iou_thres=0.45):\n",
        "    all_boxes, all_scores, all_classes = [], [], []\n",
        "    strides = [8, 16, 32]\n",
        "    \n",
        "    for i, (cls_p, obj_p, box_p) in enumerate(outputs):\n",
        "        device = cls_p.device\n",
        "        B, _, H, W = cls_p.shape\n",
        "        stride = strides[i]\n",
        "        \n",
        "        grid_y, grid_x = torch.meshgrid(torch.arange(H, device=device), torch.arange(W, device=device), indexing='ij')\n",
        "        grid = torch.stack((grid_x, grid_y), 2).float()\n",
        "        \n",
        "        # [ÏàòÏ†ï] Grid Ï∞®Ïõê ÌôïÏû•: (H, W, 2) -> (B, H, W, 2)\n",
        "        grid = grid.unsqueeze(0).expand(B, -1, -1, -1)\n",
        "        \n",
        "        obj_sig = obj_p.permute(0, 2, 3, 1).sigmoid().squeeze(-1)\n",
        "        cls_sig = cls_p.permute(0, 2, 3, 1).sigmoid()\n",
        "        box_sig = box_p.permute(0, 2, 3, 1).sigmoid()\n",
        "        \n",
        "        class_conf, class_pred = torch.max(cls_sig, dim=-1)\n",
        "        conf = obj_sig * class_conf\n",
        "        mask = conf > conf_thres\n",
        "        \n",
        "        if mask.any():\n",
        "            grid_m = grid[mask]; box_m = box_sig[mask]; conf_m = conf[mask]; cls_m = class_pred[mask]\n",
        "            \n",
        "            pred_cx = (box_m[:, 0] * 2.0 - 0.5 + grid_m[:, 0]) * stride\n",
        "            pred_cy = (box_m[:, 1] * 2.0 - 0.5 + grid_m[:, 1]) * stride\n",
        "            pred_w = (box_m[:, 2] * 2.0) ** 2 * stride\n",
        "            pred_h = (box_m[:, 3] * 2.0) ** 2 * stride\n",
        "            \n",
        "            boxes = torch.stack([pred_cx - pred_w/2, pred_cy - pred_h/2, \n",
        "                                 pred_cx + pred_w/2, pred_cy + pred_h/2], dim=1)\n",
        "            all_boxes.append(boxes)\n",
        "            all_scores.append(conf_m)\n",
        "            all_classes.append(cls_m)\n",
        "            \n",
        "    if not all_boxes: return []\n",
        "    \n",
        "    pred_boxes = torch.cat(all_boxes, dim=0)\n",
        "    pred_scores = torch.cat(all_scores, dim=0)\n",
        "    pred_classes = torch.cat(all_classes, dim=0)\n",
        "    \n",
        "    keep = torchvision.ops.nms(pred_boxes, pred_scores, iou_thres)\n",
        "    return [[*pred_boxes[k].tolist(), pred_scores[k].item(), pred_classes[k].item()] for k in keep]\n",
        "\n",
        "# 4. Visualization\n",
        "def plot_results(img_tensor, detections):\n",
        "    img = img_tensor.cpu().permute(1, 2, 0).numpy() * 255.0\n",
        "    img = img.astype(np.uint8).copy()\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "    \n",
        "    for x1, y1, x2, y2, score, cls_id in detections:\n",
        "        cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
        "        cv2.putText(img, f\"{int(cls_id)}: {score:.2f}\", (int(x1), int(y1)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "        \n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54589c00",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# [Cell 4] Training & Evaluation Loop\n",
        "# =========================================================\n",
        "\n",
        "# Test Loader Ï†ïÏùò (Cell 2ÏóêÏÑú ÎàÑÎùΩÎê®)\n",
        "test_loader = DataLoader(YoloDataset(dataset.location + \"/test\", IMG_SIZE), batch_size=BATCH_SIZE, shuffle=False, collate_fn=yolo_collate_fn, num_workers=2)\n",
        "\n",
        "model = SAMDetector(num_classes=3).to(DEVICE)\n",
        "criterion = ComputeLoss(num_classes=3).to(DEVICE)\n",
        "optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=1e-4)\n",
        "scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "EPOCHS = 10\n",
        "best_val_loss = float('inf')\n",
        "save_path = \"sam_best.pth\"\n",
        "\n",
        "print(f\"üöÄ Start Training...\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # --- Train ---\n",
        "    model.train()\n",
        "    loop = tqdm(train_loader, desc=f\"Ep {epoch+1}/{EPOCHS}\")\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    for imgs, targets in loop:\n",
        "        imgs = imgs.to(DEVICE)\n",
        "        \n",
        "        with torch.amp.autocast('cuda'):\n",
        "            preds = model(imgs)\n",
        "            loss = criterion(preds, targets, IMG_SIZE)\n",
        "            \n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "        loop.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "    \n",
        "    avg_train_loss = train_loss/len(train_loader)\n",
        "    \n",
        "    # --- Validation (Every Epoch) ---\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, (imgs, targets) in enumerate(val_loader):\n",
        "            imgs = imgs.to(DEVICE)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                preds = model(imgs)\n",
        "                val_loss += criterion(preds, targets, IMG_SIZE).item()\n",
        "            \n",
        "            # ÏãúÍ∞ÅÌôî (Í∞Å ÏóêÌè≠ Ï≤´ Î∞∞ÏπòÎßå)\n",
        "            if i == 0:\n",
        "                single_pred = [(p[0][0:1], p[1][0:1], p[2][0:1]) for p in preds]\n",
        "                dets = post_process(single_pred, conf_thres=0.4)\n",
        "                print(f\"üñºÔ∏è Valid Sample Detections: {len(dets)}\")\n",
        "                plot_results(imgs[0], dets) \n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f\"üìâ Train: {avg_train_loss:.4f} | üìä Val: {avg_val_loss:.4f}\")\n",
        "    \n",
        "    # Save Best Model\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"üíæ Best Model Saved: {avg_val_loss:.4f}\")\n",
        "    \n",
        "    print(\"-\" * 30)\n",
        "\n",
        "print(\"üèÅ Training Finished.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d1635f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# cell 5 \n",
        "# --- Test Set Evaluation ---\n",
        "print(\"\\nüöÄ Testing with Best Model...\")\n",
        "model.load_state_dict(torch.load(save_path))\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (imgs, targets) in enumerate(test_loader):\n",
        "        imgs = imgs.to(DEVICE)\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            preds = model(imgs)\n",
        "            test_loss += criterion(preds, targets, IMG_SIZE).item()\n",
        "        \n",
        "        # Test ÏÖã Í≤∞Í≥º ÏãúÍ∞ÅÌôî (Ï≤òÏùå 2Í∞ú Î∞∞ÏπòÎßå)\n",
        "        if i < 2:\n",
        "            print(f\"üñºÔ∏è Test Batch {i} Result:\")\n",
        "            single_pred = [(p[0][0:1], p[1][0:1], p[2][0:1]) for p in preds]\n",
        "            dets = post_process(single_pred, conf_thres=0.3)\n",
        "            plot_results(imgs[0], dets)\n",
        "\n",
        "print(f\"üèÜ Final Test Loss: {test_loss/len(test_loader):.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
