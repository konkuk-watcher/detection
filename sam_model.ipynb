{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82a72e65",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# [Cell 1] ÌôòÍ≤Ω ÏÑ§Ï†ï Î∞è Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ (SAM Í∏∞Î∞ò)\n",
        "# =========================================================\n",
        "!pip install roboflow torch torchvision torchaudio opencv-python numpy tqdm pillow matplotlib albumentations timm segment-anything\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from roboflow import Roboflow\n",
        "from tqdm import tqdm\n",
        "from segment_anything import sam_model_registry\n",
        "\n",
        "# SAMÏùÄ 1024x1024 ÏûÖÎ†•\n",
        "IMG_SIZE = 1024 \n",
        "BATCH_SIZE = 4\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f\"üî• Device: {device}, Resolution: {IMG_SIZE}x{IMG_SIZE}\")\n",
        "\n",
        "# 1. SAM Í∞ÄÏ§ëÏπò Îã§Ïö¥Î°úÎìú (ViT-Base)\n",
        "if not os.path.exists(\"sam_vit_b_01ec64.pth\"):\n",
        "    print(\"‚¨áÔ∏è Downloading SAM weights...\")\n",
        "    !wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n",
        "\n",
        "# 2. Îç∞Ïù¥ÌÑ∞ÏÖã Îã§Ïö¥Î°úÎìú\n",
        "rf = Roboflow(api_key=\"HG9M6YJZpcCUgAQaKO9v\")\n",
        "project = rf.workspace(\"arakon\").project(\"detection-base-hqaeg\")\n",
        "version = project.version(6)\n",
        "dataset = version.download(\"yolov8\")\n",
        "\n",
        "# 3. Îç∞Ïù¥ÌÑ∞ÏÖã ÌÅ¥ÎûòÏä§ (1024px Î¶¨ÏÇ¨Ïù¥Ïßï)\n",
        "def yolo_collate_fn(batch):\n",
        "    imgs = torch.stack([b[0] for b in batch], 0)\n",
        "    targets = [b[1] for b in batch]\n",
        "    return imgs, targets\n",
        "\n",
        "class YoloDataset(Dataset):\n",
        "    def __init__(self, root, is_train=True):\n",
        "        self.img_dir = os.path.join(root, \"images\")\n",
        "        self.label_dir = os.path.join(root, \"labels\")\n",
        "        self.files = sorted([x for x in os.listdir(self.img_dir) if x.endswith(('.jpg', '.png', '.jpeg'))])\n",
        "        self.img_size = IMG_SIZE\n",
        "\n",
        "    def __len__(self): return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.files[idx]\n",
        "        img_path = os.path.join(self.img_dir, name)\n",
        "        img = cv2.imread(img_path)\n",
        "        \n",
        "        # SAMÏùÄ RGB ÏûÖÎ†•ÏùÑ Î∞õÏäµÎãàÎã§.\n",
        "        img = cv2.resize(img, (self.img_size, self.img_size))\n",
        "        img_tensor = torch.from_numpy(img[:, :, ::-1].copy()).permute(2, 0, 1).float() \n",
        "        # SAM Preprocessing: (pixel - mean) / std (Ïó¨Í∏∞ÏÑ† Îã®Ïàú 0-1 Ï†ïÍ∑úÌôî ÌõÑ Î™®Îç∏ ÎÇ¥Î∂Ä Ï≤òÎ¶¨ ÎØøÏùå)\n",
        "        # ÌïòÏßÄÎßå SAMÏùÄ ÏõêÎûò 0-255 ÏûÖÎ†•ÏùÑ Í∏∞ÎåÄÌïòÎäî Í≤ΩÏö∞Í∞Ä ÎßéÏúºÎÇò, Ïó¨Í∏∞ÏÑ† ÌëúÏ§ÄÏ†ÅÏù∏ 0-1 Ï†ïÍ∑úÌôî ÏÇ¨Ïö©\n",
        "        # (Segment Anything ÎùºÏù¥Î∏åÎü¨Î¶¨ ÎÇ¥Î∂Ä preprocessÍ∞Ä Ï≤òÎ¶¨Ìï¥Ï£ºÏßÄÎßå, Ïó¨Í∏∞ÏÑ† Custom HeadÎùº ÏßÅÏ†ë Ìï®)\n",
        "        img_tensor = img_tensor / 255.0 \n",
        "\n",
        "        label_path = os.path.join(self.label_dir, name.rsplit('.', 1)[0] + \".txt\")\n",
        "        boxes = []\n",
        "        if os.path.exists(label_path):\n",
        "            with open(label_path, \"r\") as f:\n",
        "                for line in f.readlines():\n",
        "                    vals = list(map(float, line.strip().split()))\n",
        "                    if len(vals) == 5:\n",
        "                        boxes.append(vals) # cls, cx, cy, w, h\n",
        "        \n",
        "        target = torch.tensor(boxes, dtype=torch.float32) if len(boxes) > 0 else torch.zeros((0, 5))\n",
        "        return img_tensor, target\n",
        "\n",
        "train_loader = DataLoader(YoloDataset(dataset.location + \"/train\"), batch_size=BATCH_SIZE, shuffle=True, collate_fn=yolo_collate_fn, num_workers=2)\n",
        "val_loader = DataLoader(YoloDataset(dataset.location + \"/valid\"), batch_size=BATCH_SIZE, shuffle=False, collate_fn=yolo_collate_fn, num_workers=2)\n",
        "test_loader = DataLoader(YoloDataset(dataset.location + \"/test\"), batch_size=BATCH_SIZE, shuffle=False, collate_fn=yolo_collate_fn, num_workers=2)\n",
        "\n",
        "print(\"‚úÖ Data Ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "143c6985",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# [Cell 2] SAM Backbone + Adapter + Detection Head\n",
        "# =========================================================\n",
        "\n",
        "class ConvNormAct(nn.Module):\n",
        "    def __init__(self, in_c, out_c, k=3, s=1, p=1):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_c, out_c, k, s, p, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_c)\n",
        "        self.act = nn.SiLU()\n",
        "    def forward(self, x): return self.act(self.bn(self.conv(x)))\n",
        "\n",
        "class SAMDetector(nn.Module):\n",
        "    def __init__(self, checkpoint_path=\"sam_vit_b_01ec64.pth\", num_classes=3):\n",
        "        super().__init__()\n",
        "        \n",
        "        # 1. Load SAM (ViT-Base)\n",
        "        print(\"üîÑ Loading SAM Backbone...\")\n",
        "        self.sam = sam_model_registry[\"vit_b\"](checkpoint=checkpoint_path)\n",
        "        \n",
        "        # [ÌïµÏã¨] Backbone Freeze (ÌïôÏäµ Ïïà Ìï® -> Î©îÎ™®Î¶¨ Ï†àÏïΩ, ÌäπÏßï Î≥¥Ï°¥)\n",
        "        for param in self.sam.image_encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "            \n",
        "        # SAM ViT-B Output channel = 256\n",
        "        self.sam_out_dim = 256\n",
        "        \n",
        "        # 2. Adapter (Neck / FPN)\n",
        "        # SAM Ï∂úÎ†•ÏùÄ (B, 256, 64, 64) ÌïòÎÇòÎøêÏûÑ (1024px Í∏∞Ï§Ä, stride 16)\n",
        "        # Ïù¥Î•º ÌÜµÌï¥ P3(stride 8), P4(stride 16), P5(stride 32)Î•º ÎßåÎì§Ïñ¥Ïïº Ìï®.\n",
        "        \n",
        "        # P4: Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©ÌïòÍ±∞ÎÇò Ï†ïÏ†ú\n",
        "        self.p4_conv = ConvNormAct(256, 256)\n",
        "        \n",
        "        # P3: 2Î∞∞ Upsample\n",
        "        self.p3_up = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.p3_conv = ConvNormAct(256, 256)\n",
        "        \n",
        "        # P5: 2Î∞∞ Downsample\n",
        "        self.p5_down = ConvNormAct(256, 256, k=3, s=2, p=1)\n",
        "        \n",
        "        # 3. Detection Head (YOLO Style)\n",
        "        # Í∞Å Î†àÎ≤®(P3, P4, P5)ÎßàÎã§ ÏòàÏ∏° ÏàòÌñâ\n",
        "        self.heads = nn.ModuleList([\n",
        "            nn.Conv2d(256, num_classes + 5, 1) for _ in range(3) # 5 = 1(obj) + 4(box)\n",
        "        ])\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # 1. Backbone (Frozen)\n",
        "        # SAMÏùÄ ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄÎ•º (1024, 1024)Î°ú Í∏∞ÎåÄÌïòÎ©∞ ÎÇ¥Î∂ÄÏ†ÅÏúºÎ°ú Ï†ïÍ∑úÌôîÌï®.\n",
        "        # Ïó¨Í∏∞ÏÑ† Í∞ÑÎã®Ìûà 0-1 ÌÖêÏÑúÎ•º SAM preprocessÏóê ÎßûÏ∂∞ Ïä§ÏºÄÏùºÎßÅÌïòÎäî Í≤å Ï¢ãÏßÄÎßå,\n",
        "        # ÌïôÏäµ Í∞ÄÎä•Ìïú AdapterÍ∞Ä Î≥¥Ï†ïÌï¥Ï£ºÎØÄÎ°ú Í∑∏ÎåÄÎ°ú ÎÑ£Ïñ¥ÎèÑ Î¨¥Î∞©.\n",
        "        with torch.no_grad():\n",
        "            # (B, 3, 1024, 1024) -> (B, 256, 64, 64)\n",
        "            features = self.sam.image_encoder(x)\n",
        "            \n",
        "        # 2. Neck (Generate Multi-scale Features)\n",
        "        p4 = self.p4_conv(features)          # (B, 256, 64, 64) - Stride 16\n",
        "        p3 = self.p3_conv(self.p3_up(p4))    # (B, 256, 128, 128) - Stride 8\n",
        "        p5 = self.p5_down(p4)                # (B, 256, 32, 32) - Stride 32\n",
        "        \n",
        "        # 3. Head\n",
        "        outputs = []\n",
        "        for i, feat in enumerate([p3, p4, p5]):\n",
        "            out = self.heads[i](feat) # (B, C+5, H, W)\n",
        "            \n",
        "            # Split output\n",
        "            # cls: 0~C, obj: C, box: C+1~C+5\n",
        "            cls_p = out[:, :3, ...]\n",
        "            obj_p = out[:, 3:4, ...]\n",
        "            box_p = out[:, 4:, ...]\n",
        "            outputs.append((cls_p, obj_p, box_p))\n",
        "            \n",
        "        return outputs\n",
        "\n",
        "print(\"‚úÖ SAM Detector Defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8c83488",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# [Cell 3] Loss Function & Utils\n",
        "# =========================================================\n",
        "\n",
        "def box_iou(box1, box2):\n",
        "    lt = torch.max(box1[:, None, :2], box2[:, :2])\n",
        "    rb = torch.min(box1[:, None, 2:], box2[:, 2:])\n",
        "    inter = (rb - lt).clamp(min=0).prod(2)\n",
        "    area1 = (box1[:, 2]-box1[:, 0]) * (box1[:, 3]-box1[:, 1])\n",
        "    area2 = (box2[:, 2]-box2[:, 0]) * (box2[:, 3]-box2[:, 1])\n",
        "    return inter / (area1[:, None] + area2 - inter + 1e-6)\n",
        "\n",
        "def select_candidates(xy_centers, gt_bboxes):\n",
        "    bs, n_boxes, _ = gt_bboxes.shape\n",
        "    lt, rb = gt_bboxes.view(-1, 1, 4).chunk(2, 2)\n",
        "    bbox_deltas = torch.cat((xy_centers[None] - lt, rb - xy_centers[None]), dim=2).view(bs, n_boxes, -1, 4)\n",
        "    return bbox_deltas.min(3)[0] > 1e-9\n",
        "\n",
        "class ComputeLoss(nn.Module):\n",
        "    def __init__(self, num_classes=3):\n",
        "        super().__init__()\n",
        "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def forward(self, preds, batch_targets, img_size):\n",
        "        # preds: list of (cls, obj, box)\n",
        "        loss_total = 0.0\n",
        "        strides = [8, 16, 32]\n",
        "        \n",
        "        batch_size = preds[0][0].shape[0]\n",
        "        \n",
        "        # ÏïµÏª§ ÏÉùÏÑ± Î∞è ÏòàÏ∏°Í∞í ÎîîÏΩîÎî©\n",
        "        for i, (cls_p, obj_p, box_p) in enumerate(preds):\n",
        "            stride = strides[i]\n",
        "            B, _, H, W = cls_p.shape\n",
        "            \n",
        "            grid_y, grid_x = torch.meshgrid(torch.arange(H), torch.arange(W), indexing='ij')\n",
        "            grid = torch.stack((grid_x, grid_y), 2).to(cls_p.device).float()\n",
        "            \n",
        "            # Box Decoding\n",
        "            b_sig = box_p.permute(0, 2, 3, 1).sigmoid() # (B, H, W, 4)\n",
        "            pred_cx = (b_sig[..., 0] * 2.0 - 0.5 + grid[..., 0]) * stride\n",
        "            pred_cy = (b_sig[..., 1] * 2.0 - 0.5 + grid[..., 1]) * stride\n",
        "            pred_w = (b_sig[..., 2] * 2.0) ** 2 * stride\n",
        "            pred_h = (b_sig[..., 3] * 2.0) ** 2 * stride\n",
        "            \n",
        "            pred_xyxy = torch.stack([pred_cx - pred_w/2, pred_cy - pred_h/2, \n",
        "                                     pred_cx + pred_w/2, pred_cy + pred_h/2], dim=-1)\n",
        "            \n",
        "            # --- Target Matching (Simplified Anchor-free) ---\n",
        "            # Í∞Å GT Î∞ïÏä§ ÏïàÏóê Ï§ëÏã¨Ïù¥ Îì§Ïñ¥Ïò§Îäî ÏïµÏª§Î•º PositiveÎ°ú Í∞ÑÏ£º\n",
        "            anchors_xy = (grid + 0.5) * stride # (H, W, 2)\n",
        "            anchors_flat = anchors_xy.view(-1, 2)\n",
        "            \n",
        "            obj_target = torch.zeros(B, H, W, 1, device=cls_p.device)\n",
        "            cls_target = torch.zeros(B, H, W, self.num_classes, device=cls_p.device)\n",
        "            \n",
        "            loss_box_layer = 0.0\n",
        "            num_pos_layer = 0\n",
        "            \n",
        "            for b in range(B):\n",
        "                gt = batch_targets[b]\n",
        "                if len(gt) == 0: continue\n",
        "                \n",
        "                # GT: [cls, cx, cy, w, h] (norm) -> xyxy (pixel)\n",
        "                gt_cls = gt[:, 0].long()\n",
        "                cx, cy, w, h = gt[:, 1]*img_size, gt[:, 2]*img_size, gt[:, 3]*img_size, gt[:, 4]*img_size\n",
        "                x1, y1, x2, y2 = cx - w/2, cy - h/2, cx + w/2, cy + h/2\n",
        "                gt_xyxy = torch.stack([x1, y1, x2, y2], dim=1).to(cls_p.device)\n",
        "                \n",
        "                # Check anchors in GT\n",
        "                # anchors_flat: (N_anc, 2)\n",
        "                # gt_xyxy: (N_gt, 4)\n",
        "                # mask: (N_gt, N_anc)\n",
        "                lt = gt_xyxy[:, None, :2]\n",
        "                rb = gt_xyxy[:, None, 2:]\n",
        "                # (N_gt, N_anc, 2)\n",
        "                deltas = torch.cat((anchors_flat[None] - lt, rb - anchors_flat[None]), dim=2)\n",
        "                is_in_gt = deltas.min(dim=2)[0] > 0 # (N_gt, N_anc)\n",
        "                \n",
        "                # Í∞Å ÏïµÏª§Î≥ÑÎ°ú Îß§Ïπ≠Îêú GT Ï∞æÍ∏∞ (ÌïòÎÇòÎùºÎèÑ Îì§Ïñ¥ÏûàÏúºÎ©¥ OK)\n",
        "                # Ïó¨Í∏∞ÏÑ† Í∞ÑÎã®Ìûà 'ÎßàÏßÄÎßâ GT'Ïóê Îß§Ïπ≠ (Î≥µÏû°Ìïú Ìï†Îãπ Î°úÏßÅ ÎåÄÏã†)\n",
        "                matched_gt_idx = is_in_gt.max(dim=0)[1] # (N_anc,)\n",
        "                has_match = is_in_gt.max(dim=0)[0] # (N_anc,)\n",
        "                \n",
        "                if has_match.sum() > 0:\n",
        "                    pos_mask = has_match.view(H, W)\n",
        "                    \n",
        "                    # Obj Target\n",
        "                    obj_target[b, pos_mask] = 1.0\n",
        "                    \n",
        "                    # Cls Target\n",
        "                    matched_labels = gt_cls[matched_gt_idx[has_match]]\n",
        "                    cls_target[b, pos_mask, matched_labels] = 1.0\n",
        "                    \n",
        "                    # Box Loss (CIoU)\n",
        "                    pred_box_pos = pred_xyxy[b, pos_mask]\n",
        "                    gt_box_pos = gt_xyxy[matched_gt_idx[has_match]]\n",
        "                    iou = box_iou(pred_box_pos, gt_box_pos).diag()\n",
        "                    loss_box_layer += (1.0 - iou).sum()\n",
        "                    num_pos_layer += has_match.sum()\n",
        "\n",
        "            # Loss Accumulation\n",
        "            # Objectness (Focal style)\n",
        "            obj_pred_flat = obj_p.permute(0, 2, 3, 1).reshape(-1, 1)\n",
        "            obj_tgt_flat = obj_target.reshape(-1, 1)\n",
        "            loss_obj = F.binary_cross_entropy_with_logits(obj_pred_flat, obj_tgt_flat, reduction='sum')\n",
        "            \n",
        "            # Cls\n",
        "            cls_pred_flat = cls_p.permute(0, 2, 3, 1).reshape(-1, self.num_classes)\n",
        "            cls_tgt_flat = cls_target.reshape(-1, self.num_classes)\n",
        "            loss_cls = F.binary_cross_entropy_with_logits(cls_pred_flat, cls_tgt_flat, reduction='sum')\n",
        "            \n",
        "            # Normalize\n",
        "            num_pos = max(1.0, num_pos_layer)\n",
        "            loss_total += (loss_obj + loss_cls + loss_box_layer * 5.0) / num_pos / B\n",
        "\n",
        "        return loss_total\n",
        "\n",
        "criterion = ComputeLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54589c00",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# [Cell 4] SAM-Adapter Training Loop\n",
        "# =========================================================\n",
        "\n",
        "# Î™®Îç∏ Ï¥àÍ∏∞Ìôî\n",
        "model = SAMDetector(num_classes=3).to(device)\n",
        "\n",
        "# ÌïôÏäµ Í∞ÄÎä•Ìïú ÌååÎùºÎØ∏ÌÑ∞Îßå OptimizerÏóê Ï†ÑÎã¨ (Ï§ëÏöî!)\n",
        "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.AdamW(trainable_params, lr=1e-4, weight_decay=0.01)\n",
        "scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "EPOCHS = 15\n",
        "print(f\"üöÄ Start Training SAM-Adapter (1024x1024)...\")\n",
        "print(f\"üìå Trainable Params: {len(trainable_params)}\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
        "    epoch_loss = 0.0\n",
        "    \n",
        "    for i, (imgs, targets) in enumerate(loop):\n",
        "        imgs = imgs.to(device)\n",
        "        \n",
        "        with torch.amp.autocast('cuda'):\n",
        "            preds = model(imgs)\n",
        "            loss = criterion(preds, targets, IMG_SIZE)\n",
        "            \n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        loop.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "        \n",
        "    print(f\"Epoch {epoch+1} Avg Loss: {epoch_loss / len(train_loader):.4f}\")\n",
        "    \n",
        "    # Checkpoint\n",
        "    torch.save(model.state_dict(), \"sam_adapter_best.pt\")\n",
        "\n",
        "print(\"üèÅ Training Finished!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
