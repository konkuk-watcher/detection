{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c59b24b7",
      "metadata": {},
      "source": [
        "# HybridTwoWay Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e70754e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install roboflow torch torchvision torchaudio opencv-python numpy tqdm pillow matplotlib albumentations timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75977566",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 0: ÏÑ§Ïπò Î∞è Í∏∞Î≥∏ Import, Roboflow Îã§Ïö¥Î°úÎìú\n",
        "# ============================================\n",
        "\n",
        "import math\n",
        "import os\n",
        "from typing import List, Tuple\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from roboflow import Roboflow\n",
        "from tqdm import tqdm\n",
        "import timm # [NEW] PretrainingÏö© ÎùºÏù¥Î∏åÎü¨Î¶¨\n",
        "import albumentations as A # [NEW] Ï¶ùÍ∞ï ÎùºÏù¥Î∏åÎü¨Î¶¨\n",
        "import random\n",
        "\n",
        "# [ÏòµÏÖò] matmul precision (ÏßÄÏõêÎêòÎäî ÌôòÍ≤ΩÏóêÏÑúÎßå)\n",
        "try:\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "rf = Roboflow(api_key=\"HG9M6YJZpcCUgAQaKO9v\")\n",
        "project = rf.workspace(\"arakon\").project(\"detection-base-hqaeg\")\n",
        "version = project.version(6)\n",
        "dataset = version.download(\"yolov8\")\n",
        "\n",
        "print(f'Roboflow dataset downloaded to: {dataset.location}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0896b602",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 1: Í≥µÏö© Ïú†Ìã∏ IoU, NMS Îì±\n",
        "# ============================================\n",
        "\n",
        "def box_iou_matrix(box1, box2):\n",
        "    \"\"\"\n",
        "    box1: (N, 4), box2: (M, 4)  -> (N, M)\n",
        "    \"\"\"\n",
        "    N = box1.size(0)\n",
        "    M = box2.size(0)\n",
        "    if N == 0 or M == 0:\n",
        "        return torch.zeros(N, M, device=box1.device)\n",
        "    tl = torch.max(box1[:, None, :2], box2[:, :2])  # (N, M, 2)\n",
        "    br = torch.min(box1[:, None, 2:], box2[:, 2:])  # (N, M, 2)\n",
        "    wh = (br - tl).clamp(min=0)\n",
        "    inter = wh[..., 0] * wh[..., 1]\n",
        "    area1 = (box1[:, 2]-box1[:, 0]) * (box1[:, 3]-box1[:, 1])\n",
        "    area2 = (box2[:, 2]-box2[:, 0]) * (box2[:, 3]-box2[:, 1])\n",
        "    return inter / (area1[:, None] + area2 - inter + 1e-6)\n",
        "\n",
        "def box_iou_pair(box1, box2):\n",
        "    \"\"\"\n",
        "    Í∞Å anchorÏóê ÎåÄÌï¥ Îß§Ïπ≠Îêú GT ÌïòÎÇòÏî© ÏûàÏùÑ Îïå:\n",
        "    box1, box2: (N, 4) -> (N,) IoU\n",
        "    \"\"\"\n",
        "    if box1.numel() == 0:\n",
        "        return torch.zeros(0, device=box1.device)\n",
        "    tl = torch.max(box1[:, :2], box2[:, :2])\n",
        "    br = torch.min(box1[:, 2:], box2[:, 2:])\n",
        "    wh = (br - tl).clamp(min=0)\n",
        "    inter = wh[:, 0] * wh[:, 1]\n",
        "    area1 = (box1[:, 2]-box1[:, 0]) * (box1[:, 3]-box1[:, 1])\n",
        "    area2 = (box2[:, 2]-box2[:, 0]) * (box2[:, 3]-box2[:, 1])\n",
        "    return inter / (area1 + area2 - inter + 1e-6)\n",
        "\n",
        "def nms(boxes, scores, iou_thres=0.5):\n",
        "    if boxes.numel() == 0:\n",
        "        return torch.zeros(0, dtype=torch.long, device=boxes.device)\n",
        "    idxs = scores.argsort(descending=True)\n",
        "    keep = []\n",
        "    while idxs.numel() > 0:\n",
        "        i = idxs[0]\n",
        "        keep.append(i.item())\n",
        "        if idxs.numel() == 1:\n",
        "            break\n",
        "        ious = box_iou_matrix(boxes[i].unsqueeze(0), boxes[idxs[1:]]).squeeze(0)\n",
        "        idxs = idxs[1:][ious < iou_thres]\n",
        "    return torch.tensor(keep, dtype=torch.long, device=boxes.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ec82c85",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 2: Conv Block & Stem\n",
        "# ============================================\n",
        "\n",
        "def conv_bn_act(in_ch, out_ch, k=3, s=1, p=1, act=True):\n",
        "    m = [nn.Conv2d(in_ch, out_ch, k, s, p, bias=False),\n",
        "         nn.BatchNorm2d(out_ch)]\n",
        "    if act:\n",
        "        m.append(nn.SiLU(inplace=True))\n",
        "    return nn.Sequential(*m)\n",
        "\n",
        "class FixedGaussianBlur(nn.Module):\n",
        "    def __init__(self, channels, k=5, sigma=1.0):\n",
        "        super().__init__()\n",
        "        grid = torch.arange(k).float() - (k - 1) / 2\n",
        "        gauss = torch.exp(-(grid ** 2) / (2 * sigma ** 2))\n",
        "        kernel1d = gauss / gauss.sum()\n",
        "        kernel2d = torch.outer(kernel1d, kernel1d)\n",
        "        weight = kernel2d[None, None, :, :].repeat(channels, 1, 1, 1)\n",
        "        self.register_buffer('weight', weight)\n",
        "        self.groups = channels\n",
        "        self.k = k\n",
        "\n",
        "    def forward(self, x):\n",
        "        pad = (self.k // 2,) * 4\n",
        "        return F.conv2d(F.pad(x, pad, mode='reflect'),\n",
        "                        self.weight, groups=self.groups)\n",
        "\n",
        "class AnomalyAwareStem(nn.Module):\n",
        "    def __init__(self, in_ch=3, base_ch=48):\n",
        "        super().__init__()\n",
        "        C1, C2, C3 = base_ch, base_ch * 2, base_ch * 4\n",
        "        self.stem = nn.Sequential(\n",
        "            conv_bn_act(in_ch, C1, 3, 2, 1),\n",
        "            conv_bn_act(C1, C2, 3, 2, 1),\n",
        "            conv_bn_act(C2, C3, 3, 2, 1),\n",
        "        )\n",
        "        self.blur = FixedGaussianBlur(in_ch, k=5, sigma=1.0)\n",
        "        self.anom = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, in_ch, 3, 1, 1, groups=in_ch, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_ch, C3 // 4, 1, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(C3 // 4),\n",
        "            nn.SiLU(inplace=True),\n",
        "        )\n",
        "        self.fuse = nn.Conv2d(C3 + C3 // 4, C3, 1, 1, 0, bias=False)\n",
        "        self.fuse_bn = nn.BatchNorm2d(C3)\n",
        "        self.vis_head = nn.Conv2d(C3, 1, 1, 1, 0)\n",
        "        self.base_ch = base_ch\n",
        "\n",
        "    @property\n",
        "    def out_channels(self):\n",
        "        return 4 * self.base_ch\n",
        "\n",
        "    def forward(self, x):\n",
        "        f_main = self.stem(x)\n",
        "        blurred = self.blur(x)\n",
        "        high = x - blurred\n",
        "        high_ds = F.interpolate(high, size=f_main.shape[-2:], mode='bilinear', align_corners=False)\n",
        "        f_anom = self.anom(high_ds)\n",
        "        f = torch.cat([f_main, f_anom], dim=1)\n",
        "        f = self.fuse_bn(self.fuse(f))\n",
        "        f = F.silu(f, inplace=True)\n",
        "        v = torch.sigmoid(self.vis_head(f_main))\n",
        "        return f, v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78d5ade6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 3 : Feedback Adapter\n",
        "# ============================================\n",
        "\n",
        "class FeedbackAdapter(nn.Module):\n",
        "    def __init__(self, d_token: int, c_stem: int, use_bn: bool = True):\n",
        "        super().__init__()\n",
        "        layers = [nn.Conv2d(d_token, c_stem * 2, 1, 1, 0, bias=not use_bn)]\n",
        "        if use_bn:\n",
        "            layers.append(nn.BatchNorm2d(c_stem * 2))\n",
        "        layers.append(nn.SiLU(inplace=True))\n",
        "        self.adapter = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, tokens, Ht, Wt, f_stem):\n",
        "        B, N, D = tokens.shape\n",
        "        t2d = tokens.transpose(1, 2).reshape(B, D, Ht, Wt)\n",
        "        ab = self.adapter(t2d)\n",
        "        Cs = f_stem.shape[1]\n",
        "        gamma, beta = torch.split(ab, Cs, dim=1)\n",
        "        return f_stem * (1 + torch.tanh(gamma)) + beta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83a70ab2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 4 : PANLite Neck\n",
        "# ============================================\n",
        "\n",
        "class PANLite(nn.Module):\n",
        "    def __init__(self, in_ch=512, mid=256):\n",
        "        super().__init__()\n",
        "        self.lateral = conv_bn_act(in_ch, mid, 1, 1, 0)\n",
        "        self.down4 = conv_bn_act(mid, mid, 3, 2, 1)\n",
        "        self.down5 = conv_bn_act(mid, mid, 3, 2, 1)\n",
        "        self.up4 = conv_bn_act(mid + mid, mid, 3, 1, 1)\n",
        "        self.up3 = conv_bn_act(mid + mid, mid, 3, 1, 1)\n",
        "        self.down_f4 = conv_bn_act(mid, mid, 3, 2, 1)\n",
        "        self.fuse4 = conv_bn_act(mid + mid, mid, 3, 1, 1)\n",
        "        self.down_f5 = conv_bn_act(mid, mid, 3, 2, 1)\n",
        "        self.fuse5 = conv_bn_act(mid + mid, mid, 3, 1, 1)\n",
        "\n",
        "    def forward(self, p3):\n",
        "        p3 = self.lateral(p3)\n",
        "        p4 = self.down4(p3)\n",
        "        p5 = self.down5(p4)\n",
        "\n",
        "        p4u = F.interpolate(p5, size=p4.shape[-2:], mode='nearest')\n",
        "        p4 = self.up4(torch.cat([p4, p4u], dim=1))\n",
        "\n",
        "        p3u = F.interpolate(p4, size=p3.shape[-2:], mode='nearest')\n",
        "        p3 = self.up3(torch.cat([p3, p3u], dim=1))\n",
        "\n",
        "        p4b = self.down_f4(p3)\n",
        "        p4 = self.fuse4(torch.cat([p4, p4b], dim=1))\n",
        "\n",
        "        p5b = self.down_f5(p4)\n",
        "        p5 = self.fuse5(torch.cat([p5, p5b], dim=1))\n",
        "\n",
        "        return p3, p4, p5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c34a335",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 5 : YOLOLite Head\n",
        "# ============================================\n",
        "\n",
        "class YOLOHeadLite(nn.Module):\n",
        "    def __init__(self, in_ch=256, num_classes=1):\n",
        "        super().__init__()\n",
        "        c = in_ch\n",
        "        self.stem3 = conv_bn_act(c, c, 3, 1, 1)\n",
        "        self.stem4 = conv_bn_act(c, c, 3, 1, 1)\n",
        "        self.stem5 = conv_bn_act(c, c, 3, 1, 1)\n",
        "\n",
        "        self.cls3 = nn.Conv2d(c, num_classes, 1, 1, 0)\n",
        "        self.obj3 = nn.Conv2d(c, 1, 1, 1, 0)\n",
        "        self.box3 = nn.Conv2d(c, 4, 1, 1, 0)\n",
        "        nn.init.constant_(self.obj3.bias, -4.59)\n",
        "\n",
        "        self.cls4 = nn.Conv2d(c, num_classes, 1, 1, 0)\n",
        "        self.obj4 = nn.Conv2d(c, 1, 1, 1, 0)\n",
        "        self.box4 = nn.Conv2d(c, 4, 1, 1, 0)\n",
        "        nn.init.constant_(self.obj4.bias, -4.59)\n",
        "\n",
        "        self.cls5 = nn.Conv2d(c, num_classes, 1, 1, 0)\n",
        "        self.obj5 = nn.Conv2d(c, 1, 1, 1, 0)\n",
        "        self.box5 = nn.Conv2d(c, 4, 1, 1, 0)\n",
        "        nn.init.constant_(self.obj5.bias, -4.59)\n",
        "\n",
        "    def forward_single(self, x, stem, cls, obj, box):\n",
        "        f = stem(x)\n",
        "        return cls(f), obj(f), box(f)\n",
        "\n",
        "    def forward(self, p3, p4, p5):\n",
        "        c3, o3, b3 = self.forward_single(p3, self.stem3, self.cls3, self.obj3, self.box3)\n",
        "        c4, o4, b4 = self.forward_single(p4, self.stem4, self.cls4, self.obj4, self.box4)\n",
        "        c5, o5, b5 = self.forward_single(p5, self.stem5, self.cls5, self.obj5, self.box5)\n",
        "        return [(c3, o3, b3), (c4, o4, b4), (c5, o5, b5)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8873b4ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 6 : HybridTwoWay Model (timm ViT + Feedback)\n",
        "# ============================================\n",
        "\n",
        "class HybridTwoWay(nn.Module):\n",
        "    def __init__(self, \n",
        "                 in_ch=3, \n",
        "                 stem_base=32, \n",
        "                 embed_dim=768,\n",
        "                 vit_model_name='vit_base_patch16_224.augreg_in21k_ft_in1k',\n",
        "                 num_classes=3, \n",
        "                 iters=1, \n",
        "                 detach_feedback=True, \n",
        "                 img_size=512):\n",
        "        super().__init__()\n",
        "        self.iters = iters\n",
        "        self.detach_feedback = detach_feedback\n",
        "        self.img_size = img_size\n",
        "\n",
        "        # Stem\n",
        "        self.stem = AnomalyAwareStem(in_ch=in_ch, base_ch=stem_base)\n",
        "        c_stem = stem_base * 4  # ex) 64*4=256\n",
        "\n",
        "        # Pretrained ViT\n",
        "        print(f\"üîÑ Loading Pretrained Weights: {vit_model_name}...\")\n",
        "        self.vit = timm.create_model(vit_model_name, pretrained=True, num_classes=0)\n",
        "        self.vit_dim = self.vit.num_features\n",
        "\n",
        "        # Stem -> ViT\n",
        "        self.to_vit = nn.Sequential(\n",
        "            nn.Conv2d(c_stem, self.vit_dim, 1),\n",
        "            nn.BatchNorm2d(self.vit_dim),\n",
        "            nn.SiLU()\n",
        "        )\n",
        "\n",
        "        # ViT -> Neck(256)\n",
        "        self.from_vit = nn.Sequential(\n",
        "            nn.Conv2d(self.vit_dim, 256, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.SiLU()\n",
        "        )\n",
        "\n",
        "        # Feedback Adapter\n",
        "        self.feedback = FeedbackAdapter(d_token=self.vit_dim, c_stem=c_stem, use_bn=True)\n",
        "\n",
        "        # Neck & Head\n",
        "        self.neck = PANLite(in_ch=256, mid=256)\n",
        "        self.head = YOLOHeadLite(in_ch=256, num_classes=num_classes)\n",
        "\n",
        "        # pos_embed ÏïàÏ†Ñ Ï≤òÎ¶¨Ïö© flag\n",
        "        self.has_pos_embed = hasattr(self.vit, \"pos_embed\") and self.vit.pos_embed is not None\n",
        "\n",
        "    def get_interpolated_pos_embed(self, Ht, Wt):\n",
        "        \"\"\"\n",
        "        timm ViTÏùò pos_embedÎ•º ÌòÑÏû¨ ÌÜ†ÌÅ∞ grid(Ht,Wt)Ïóê ÎßûÍ≤å interpolation\n",
        "        \"\"\"\n",
        "        if not self.has_pos_embed:\n",
        "            return 0\n",
        "\n",
        "        pos_embed = self.vit.pos_embed  # (1, 1+N, D) or (1, N, D)\n",
        "        num_prefix = getattr(self.vit, \"num_prefix_tokens\", 1)\n",
        "        if pos_embed.shape[1] == Ht * Wt + num_prefix:\n",
        "            # ÌÜ†ÌÅ∞ ÏàòÍ∞Ä Ïù¥ÎØ∏ Í∞ôÏúºÎ©¥ Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©\n",
        "            src_pos = pos_embed[:, num_prefix:]\n",
        "            return src_pos\n",
        "\n",
        "        src_pos = pos_embed[:, num_prefix:]  # (1, N, D)\n",
        "        N = src_pos.shape[1]\n",
        "        src_h = int(N ** 0.5)\n",
        "        src_w = src_h\n",
        "\n",
        "        src_pos = src_pos.reshape(1, src_h, src_w, -1).permute(0, 3, 1, 2)\n",
        "        dst = F.interpolate(src_pos, size=(Ht, Wt), mode='bicubic', align_corners=False)\n",
        "        dst = dst.flatten(2).transpose(1, 2)  # (1, Ht*Wt, D)\n",
        "        return dst\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Stem\n",
        "        f_stem, vis = self.stem(x)  # (B, Cs, Hs, Ws)\n",
        "        f_vit_in = self.to_vit(f_stem)  # (B, D, Ht, Wt)\n",
        "        B, D, Ht, Wt = f_vit_in.shape\n",
        "\n",
        "        tokens = f_vit_in.flatten(2).transpose(1, 2)  # (B, N, D), N=Ht*Wt\n",
        "\n",
        "        # Positional Embedding interpolate\n",
        "        pos_embed = self.get_interpolated_pos_embed(Ht, Wt)  # (1, N, D) or 0\n",
        "        if isinstance(pos_embed, torch.Tensor):\n",
        "            pos_embed = pos_embed.to(tokens.device)\n",
        "            tokens = tokens + pos_embed  # broadcast (1,N,D) + (B,N,D)\n",
        "\n",
        "        f_fb = f_stem\n",
        "        preds, aux = None, None\n",
        "\n",
        "        for i in range(self.iters):\n",
        "            # ViT blocks (ÏïàÏ†ÑÌïòÍ≤å loop)\n",
        "            t = tokens\n",
        "            for blk in self.vit.blocks:\n",
        "                t = blk(t)\n",
        "            if hasattr(self.vit, \"norm\") and self.vit.norm is not None:\n",
        "                t = self.vit.norm(t)\n",
        "            tokens_out = t  # (B,N,D)\n",
        "\n",
        "            # Feedback\n",
        "            toks_for_fb = tokens_out.detach() if self.detach_feedback else tokens_out\n",
        "            f_fb = self.feedback(toks_for_fb, Ht, Wt, f_fb)\n",
        "\n",
        "            # NeckÏóê ÎÑ£ÏùÑ feature (Stem domain -> ViT dim -> Neck dim)\n",
        "            p3_in = self.from_vit(self.to_vit(f_fb))  # (B,256,Ht,Wt)\n",
        "            p3, p4, p5 = self.neck(p3_in)\n",
        "            preds = self.head(p3, p4, p5)\n",
        "            aux = {\"P3\": p3, \"P4\": p4, \"P5\": p5, \"V\": vis}\n",
        "\n",
        "            # Îã§Ïùå iterationÏù¥Î©¥ tokens Í∞±Ïã†\n",
        "            if i != self.iters - 1:\n",
        "                f_vit_in = self.to_vit(f_fb)\n",
        "                B2, D2, Ht2, Wt2 = f_vit_in.shape\n",
        "                tokens = f_vit_in.flatten(2).transpose(1, 2)\n",
        "                pos_embed = self.get_interpolated_pos_embed(Ht2, Wt2)\n",
        "                if isinstance(pos_embed, torch.Tensor):\n",
        "                    pos_embed = pos_embed.to(tokens.device)\n",
        "                    tokens = tokens + pos_embed\n",
        "\n",
        "        return preds, aux"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "723c00b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 7 : Dataset & Dataloader\n",
        "# ============================================\n",
        "\n",
        "IMG_SIZE = 512\n",
        "\n",
        "def yolo_collate_fn(batch):\n",
        "    imgs = []\n",
        "    targets = []\n",
        "    for img, tgt in batch:\n",
        "        imgs.append(img)\n",
        "        targets.append(tgt)\n",
        "    imgs = torch.stack(imgs, 0)\n",
        "    return imgs, targets\n",
        "\n",
        "class YoloDataset(Dataset):\n",
        "    def __init__(self, root, is_train=True, mosaic_prob=0.5):\n",
        "        self.img_dir = os.path.join(root, \"images\")\n",
        "        self.label_dir = os.path.join(root, \"labels\")\n",
        "        self.images = sorted(os.listdir(self.img_dir))\n",
        "        self.is_train = is_train\n",
        "        self.mosaic_prob = mosaic_prob if is_train else 0.0\n",
        "        self.img_size = IMG_SIZE\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def load_image_and_boxes(self, index):\n",
        "        \"\"\"ÏõêÎ≥∏ ÌÅ¨Í∏∞ Í∑∏ÎåÄÎ°ú Î°úÎìú, RGB + xyxy(abs) Î¶¨ÌÑ¥\"\"\"\n",
        "        name = self.images[index]\n",
        "        img_path = os.path.join(self.img_dir, name)\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        h, w = img.shape[:2]\n",
        "\n",
        "        label_path = os.path.join(self.label_dir, name.rsplit(\".\", 1)[0] + \".txt\")\n",
        "        boxes = []\n",
        "        if os.path.exists(label_path):\n",
        "            with open(label_path, \"r\") as f:\n",
        "                for line in f.readlines():\n",
        "                    cls, cx, cy, bw, bh = map(float, line.split())\n",
        "                    x1 = (cx - bw/2) * w\n",
        "                    y1 = (cy - bh/2) * h\n",
        "                    x2 = (cx + bw/2) * w\n",
        "                    y2 = (cy + bh/2) * h\n",
        "                    boxes.append([cls, x1, y1, x2, y2])\n",
        "        boxes = np.array(boxes) if len(boxes) > 0 else np.zeros((0, 5))\n",
        "        return img, boxes, (h, w)\n",
        "\n",
        "    def load_mosaic(self, index):\n",
        "        \"\"\"\n",
        "        [ÏïàÏ†Ñ Î≤ÑÏ†Ñ]\n",
        "        ÏµúÏ¢Ö Ïù¥ÎØ∏ÏßÄ: (img_size, img_size)\n",
        "        Í∞Å ÌÉÄÏùº: (tile, tile) = (img_size/2, img_size/2)\n",
        "        4Ïû•ÏùÑ Î¨¥Ï°∞Í±¥ 2x2Î°ú Î∞∞Ïπò ‚Üí Ïä¨ÎùºÏù¥Ïã± Î∂àÏùºÏπò ÏóÜÏùå\n",
        "        \"\"\"\n",
        "        tile = self.img_size // 2\n",
        "        mosaic_img = np.full((self.img_size, self.img_size, 3), 114, dtype=np.uint8)\n",
        "        mosaic_boxes = []\n",
        "\n",
        "        indices = [index] + [random.randint(0, len(self.images) - 1) for _ in range(3)]\n",
        "\n",
        "        for i, idx in enumerate(indices):\n",
        "            img, boxes, (h0, w0) = self.load_image_and_boxes(idx)\n",
        "\n",
        "            # ÏõêÎ≥∏ ‚Üí ÌÉÄÏùº ÌÅ¨Í∏∞ (256x256)\n",
        "            img_resized = cv2.resize(img, (tile, tile))\n",
        "            scale_x = tile / w0\n",
        "            scale_y = tile / h0\n",
        "\n",
        "            # ÌÉÄÏùº Î∞∞Ïπò ÏúÑÏπò\n",
        "            row = i // 2  # 0 or 1\n",
        "            col = i % 2   # 0 or 1\n",
        "            x_off = col * tile\n",
        "            y_off = row * tile\n",
        "\n",
        "            # Î™®ÏûêÏù¥ÌÅ¨ Ïù¥ÎØ∏ÏßÄÏóê Î∂ôÏù¥Í∏∞\n",
        "            mosaic_img[y_off:y_off+tile, x_off:x_off+tile] = img_resized\n",
        "\n",
        "            # Î∞ïÏä§ Ïä§ÏºÄÏùº + Ïò§ÌîÑÏÖã\n",
        "            if len(boxes) > 0:\n",
        "                b = boxes.copy()\n",
        "                b[:, 1] = b[:, 1] * scale_x + x_off\n",
        "                b[:, 3] = b[:, 3] * scale_x + x_off\n",
        "                b[:, 2] = b[:, 2] * scale_y + y_off\n",
        "                b[:, 4] = b[:, 4] * scale_y + y_off\n",
        "                mosaic_boxes.append(b)\n",
        "\n",
        "        if len(mosaic_boxes) > 0:\n",
        "            mosaic_boxes = np.concatenate(mosaic_boxes, 0)\n",
        "            np.clip(mosaic_boxes[:, 1:], 0, self.img_size, out=mosaic_boxes[:, 1:])\n",
        "        else:\n",
        "            mosaic_boxes = np.zeros((0, 5))\n",
        "\n",
        "        return mosaic_img, mosaic_boxes\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 1) Ïù¥ÎØ∏ÏßÄ/Î∞ïÏä§ Î°úÎìú (mosaic or not)\n",
        "        if self.is_train and random.random() < self.mosaic_prob:\n",
        "            img, boxes_xyxy = self.load_mosaic(idx)\n",
        "        else:\n",
        "            img, boxes_xyxy, (h0, w0) = self.load_image_and_boxes(idx)\n",
        "            # ÏõêÎ≥∏ ‚Üí (img_size, img_size)\n",
        "            img = cv2.resize(img, (self.img_size, self.img_size))\n",
        "            if len(boxes_xyxy) > 0:\n",
        "                boxes_xyxy = boxes_xyxy.copy()\n",
        "                scale_x = self.img_size / w0\n",
        "                scale_y = self.img_size / h0\n",
        "                boxes_xyxy[:, 1] *= scale_x\n",
        "                boxes_xyxy[:, 3] *= scale_x\n",
        "                boxes_xyxy[:, 2] *= scale_y\n",
        "                boxes_xyxy[:, 4] *= scale_y\n",
        "                np.clip(boxes_xyxy[:, 1:], 0, self.img_size, out=boxes_xyxy[:, 1:])\n",
        "            else:\n",
        "                boxes_xyxy = np.zeros((0, 5))\n",
        "\n",
        "        # 2) Ïù¥ÎØ∏ÏßÄ Ï†ïÍ∑úÌôî ‚Üí Tensor\n",
        "        img = img.astype(np.float32) / 255.0\n",
        "        img = torch.from_numpy(img).permute(2, 0, 1)  # (3,H,W)\n",
        "\n",
        "        # 3) xyxy ‚Üí cxcywh (normalized)\n",
        "        targets = []\n",
        "        if len(boxes_xyxy) > 0:\n",
        "            boxes_xyxy = boxes_xyxy[\n",
        "                np.logical_and(\n",
        "                    boxes_xyxy[:, 3] > boxes_xyxy[:, 1],\n",
        "                    boxes_xyxy[:, 4] > boxes_xyxy[:, 2]\n",
        "                )\n",
        "            ]\n",
        "            for box in boxes_xyxy:\n",
        "                cls = box[0]\n",
        "                x1, y1, x2, y2 = box[1:]\n",
        "                cx = (x1 + x2) / 2 / self.img_size\n",
        "                cy = (y1 + y2) / 2 / self.img_size\n",
        "                w  = (x2 - x1) / self.img_size\n",
        "                h  = (y2 - y1) / self.img_size\n",
        "                targets.append([cls, cx, cy, w, h])\n",
        "\n",
        "        targets = torch.tensor(targets, dtype=torch.float32)\n",
        "        return img, targets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98c51e46",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 8 : Task Aligned Assigner & Loss\n",
        "# ============================================\n",
        "\n",
        "def select_candidates_in_gts(xy_centers, gt_bboxes, eps=1e-9):\n",
        "    n_anchors = xy_centers.shape[0]\n",
        "    bs, n_boxes, _ = gt_bboxes.shape\n",
        "    lt, rb = gt_bboxes.view(-1, 1, 4).chunk(2, 2)  # (B*M,1,2) each\n",
        "    bbox_deltas = torch.cat((xy_centers[None] - lt, rb - xy_centers[None]), dim=2)\n",
        "    bbox_deltas = bbox_deltas.view(bs, n_boxes, n_anchors, -1)\n",
        "    return bbox_deltas.min(3)[0] > eps\n",
        "\n",
        "class TaskAlignedAssigner(nn.Module):\n",
        "    def __init__(self, topk=13, alpha=1.0, beta=6.0, eps=1e-9):\n",
        "        super().__init__()\n",
        "        self.topk = topk\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.eps = eps\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt):\n",
        "        \"\"\"\n",
        "        pd_scores: (B, A, C) - sigmoidÎêú cls score\n",
        "        pd_bboxes: (B, A, 4) - xyxy\n",
        "        anc_points: (A, 2)\n",
        "        gt_labels: (B, M, 1)\n",
        "        gt_bboxes: (B, M, 4) - xyxy\n",
        "        mask_gt: (B, M)\n",
        "        \"\"\"\n",
        "        bs, n_max_boxes = gt_bboxes.shape[:2]\n",
        "\n",
        "        mask_pos, align_metric, overlaps = self.get_pos_mask(\n",
        "            pd_scores, pd_bboxes, gt_labels, gt_bboxes, anc_points, mask_gt\n",
        "        )\n",
        "\n",
        "        # top-k\n",
        "        target_gt_idx, fg_mask, mask_pos = self.select_topk_candidates(\n",
        "            align_metric * mask_pos,\n",
        "            topk_mask=mask_gt[..., None].expand(-1, -1, self.topk).bool()\n",
        "        )\n",
        "\n",
        "        # target labels & bboxes\n",
        "        target_labels = gt_labels.long().squeeze(-1)  # (B,M)\n",
        "        # (B,A) ‚Üí (B,A) label\n",
        "        target_labels = torch.gather(target_labels, 1, target_gt_idx.clamp(min=0))\n",
        "        target_bboxes = []\n",
        "        for b in range(bs):\n",
        "            tb = gt_bboxes[b, target_gt_idx[b].clamp(min=0)]  # (A,4)\n",
        "            target_bboxes.append(tb)\n",
        "        target_bboxes = torch.stack(target_bboxes, dim=0)  # (B,A,4)\n",
        "\n",
        "        # alignment metric as score target (Ïó¨Í∏∞ÏÑ† Îî∞Î°ú Ïì∞ÏßÑ ÏïäÏßÄÎßå ÎÇ®Í≤®Îë†)\n",
        "        return target_labels, target_bboxes, None, fg_mask, target_gt_idx\n",
        "\n",
        "    def get_pos_mask(self, pd_scores, pd_bboxes, gt_labels, gt_bboxes, anc_points, mask_gt):\n",
        "        bs, n_max_boxes = gt_bboxes.shape[:2]\n",
        "        mask_in_gts = select_candidates_in_gts(anc_points, gt_bboxes)  # (B,M,A)\n",
        "\n",
        "        overlaps = []\n",
        "        for b in range(bs):\n",
        "            iou = box_iou_matrix(gt_bboxes[b], pd_bboxes[b])  # (M,A)\n",
        "            overlaps.append(iou)\n",
        "        overlaps = torch.stack(overlaps, dim=0)  # (B,M,A)\n",
        "\n",
        "        bbox_scores = []\n",
        "        B, A, C = pd_scores.shape\n",
        "        for b in range(bs):\n",
        "            labels = gt_labels[b].long().squeeze(-1)  # (M,)\n",
        "            # [FIX] out-of-range Î∞©ÏßÄ\n",
        "            labels = labels.clamp(min=0, max=C-1)\n",
        "            scores = pd_scores[b][:, labels]  # (A,M)\n",
        "            bbox_scores.append(scores.T)      # (M,A)\n",
        "        bbox_scores = torch.stack(bbox_scores, dim=0)  # (B,M,A)\n",
        "\n",
        "        align_metric = bbox_scores.pow(self.alpha) * overlaps.pow(self.beta)\n",
        "\n",
        "        mask_pos = mask_in_gts & mask_gt[..., None].bool()\n",
        "        return mask_pos, align_metric, overlaps\n",
        "\n",
        "    def select_topk_candidates(self, metrics, topk_mask=None):\n",
        "        B, M, A = metrics.shape\n",
        "        topk = min(self.topk, A)\n",
        "        topk_metrics, topk_idxs = torch.topk(metrics, k=topk, dim=-1, largest=True)\n",
        "\n",
        "        topk_mask_full = torch.zeros_like(metrics, dtype=torch.bool)\n",
        "        topk_mask_full.scatter_(-1, topk_idxs, True)\n",
        "\n",
        "        mask_pos = topk_mask_full\n",
        "        fg_mask = mask_pos.sum(-2) > 0  # (B,A)\n",
        "\n",
        "        # Í∞Å anchorÎßàÎã§ Ïñ¥Îñ§ GT ÏÑ†ÌÉùÎêòÏóàÎäîÏßÄ\n",
        "        metrics_pos = metrics.clone()\n",
        "        metrics_pos[~mask_pos] = -1\n",
        "        target_gt_idx = metrics_pos.argmax(dim=1)  # (B,A)\n",
        "\n",
        "        return target_gt_idx, fg_mask, mask_pos\n",
        "\n",
        "class ComputeLoss(nn.Module):\n",
        "    def __init__(self, num_classes=3, img_size=512):\n",
        "        super().__init__()\n",
        "        self.assigner = TaskAlignedAssigner(topk=10)\n",
        "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
        "        self.num_classes = num_classes\n",
        "        self.img_size = img_size\n",
        "\n",
        "    def forward(self, preds, batch_targets, img_size):\n",
        "        # preds: [(cls, obj, box), ...]\n",
        "        # batch_targets: list of (N,5) [cls,cx,cy,w,h]\n",
        "        cls_preds = []\n",
        "        box_preds = []\n",
        "        anchors = []\n",
        "\n",
        "        B = preds[0][0].shape[0]\n",
        "\n",
        "        for i, (cls, obj, box) in enumerate(preds):\n",
        "            bsz, C, H, W = cls.shape\n",
        "            stride = img_size // H\n",
        "\n",
        "            grid_y, grid_x = torch.meshgrid(\n",
        "                torch.arange(H, device=cls.device),\n",
        "                torch.arange(W, device=cls.device),\n",
        "                indexing='ij'\n",
        "            )\n",
        "            grid = torch.stack((grid_x, grid_y), 2).float()\n",
        "            grid = (grid + 0.5) * stride\n",
        "            anchors.append(grid.view(-1, 2))\n",
        "\n",
        "            cls_preds.append(cls.permute(0, 2, 3, 1).reshape(bsz, -1, C))\n",
        "            b_p = box.permute(0, 2, 3, 1).reshape(bsz, -1, 4).sigmoid()\n",
        "\n",
        "            cx = b_p[..., 0] * img_size\n",
        "            cy = b_p[..., 1] * img_size\n",
        "            w  = b_p[..., 2] * img_size\n",
        "            h  = b_p[..., 3] * img_size\n",
        "            x1 = cx - w/2\n",
        "            y1 = cy - h/2\n",
        "            x2 = cx + w/2\n",
        "            y2 = cy + h/2\n",
        "            box_preds.append(torch.stack([x1, y1, x2, y2], -1))\n",
        "\n",
        "        cls_preds = torch.cat(cls_preds, dim=1)  # (B,A_tot,C)\n",
        "        box_preds = torch.cat(box_preds, dim=1)  # (B,A_tot,4)\n",
        "        anchors = torch.cat(anchors, dim=0)      # (A_tot,2)\n",
        "\n",
        "        max_boxes = max([len(t) for t in batch_targets])\n",
        "        if max_boxes == 0:\n",
        "            return cls_preds.sum() * 0.0\n",
        "\n",
        "        batch_gt_labels = torch.zeros(B, max_boxes, 1, device=cls_preds.device)\n",
        "        batch_gt_bboxes = torch.zeros(B, max_boxes, 4, device=cls_preds.device)\n",
        "        batch_mask_gt   = torch.zeros(B, max_boxes, device=cls_preds.device)\n",
        "\n",
        "        for b, t in enumerate(batch_targets):\n",
        "            n = len(t)\n",
        "            if n > 0:\n",
        "                batch_gt_labels[b, :n, 0] = t[:, 0]\n",
        "                cx, cy, w, h = t[:, 1], t[:, 2], t[:, 3], t[:, 4]\n",
        "                x1 = (cx - w/2) * img_size\n",
        "                y1 = (cy - h/2) * img_size\n",
        "                x2 = (cx + w/2) * img_size\n",
        "                y2 = (cy + h/2) * img_size\n",
        "                batch_gt_bboxes[b, :n] = torch.stack([x1, y1, x2, y2], dim=-1)\n",
        "                batch_mask_gt[b, :n] = 1.0\n",
        "\n",
        "        target_labels, target_bboxes, _, fg_mask, target_gt_idx = self.assigner(\n",
        "            cls_preds.sigmoid(), box_preds, anchors,\n",
        "            batch_gt_labels, batch_gt_bboxes, batch_mask_gt\n",
        "        )\n",
        "\n",
        "        num_pos = fg_mask.sum().clamp(min=1.0)\n",
        "\n",
        "        # Classification Target\n",
        "        target_cls = torch.zeros_like(cls_preds)\n",
        "        for b in range(B):\n",
        "            pos_idx = fg_mask[b].nonzero(as_tuple=False).squeeze(-1)\n",
        "            if pos_idx.numel() == 0:\n",
        "                continue\n",
        "            cls_idx = target_labels[b, pos_idx].long().clamp(min=0, max=self.num_classes-1)\n",
        "            target_cls[b, pos_idx, cls_idx] = 1.0\n",
        "\n",
        "        loss_cls = self.bce(cls_preds, target_cls).sum() / num_pos\n",
        "\n",
        "        # Box Loss (element-wise IoU)\n",
        "        loss_box = 0.0\n",
        "        if fg_mask.sum() > 0:\n",
        "            pos_pred_box = box_preds[fg_mask]\n",
        "            pos_tgt_box = target_bboxes[fg_mask]\n",
        "            ious = box_iou_pair(pos_pred_box, pos_tgt_box)\n",
        "            loss_box = (1.0 - ious).sum() / num_pos\n",
        "\n",
        "        return loss_cls + 4.0 * loss_box\n",
        "\n",
        "criterion = ComputeLoss(num_classes=3, img_size=IMG_SIZE)\n",
        "\n",
        "def yolo_loss_tal(preds, targets, img_size):\n",
        "    return criterion(preds, targets, img_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "276bf933",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 9 : Decode Predictions + mAP Evaluation\n",
        "# ============================================\n",
        "\n",
        "def decode_predictions(preds, img_size=512, conf_thres=0.25, nms_iou_thres=0.5):\n",
        "    all_outputs = []\n",
        "    B = preds[0][0].shape[0]\n",
        "\n",
        "    for b in range(B):\n",
        "        dets_all = []\n",
        "        for (cls_pred, obj_pred, box_pred) in preds:\n",
        "            B_s, C, H, W = cls_pred.shape\n",
        "\n",
        "            cls_logits = cls_pred[b].permute(1,2,0).reshape(H*W, C)\n",
        "            box_logits = box_pred[b].permute(1,2,0).reshape(H*W, 4)\n",
        "\n",
        "            cls_scores = cls_logits.sigmoid()\n",
        "            box_norm = box_logits.sigmoid()\n",
        "\n",
        "            cls_max_scores, cls_ids = cls_scores.max(dim=-1)\n",
        "            scores = cls_max_scores\n",
        "\n",
        "            mask = scores > conf_thres\n",
        "            if mask.sum() == 0:\n",
        "                continue\n",
        "\n",
        "            scores_ = scores[mask]\n",
        "            cls_ids_ = cls_ids[mask]\n",
        "            boxes = box_norm[mask]\n",
        "\n",
        "            x_c = boxes[:, 0] * img_size\n",
        "            y_c = boxes[:, 1] * img_size\n",
        "            w   = boxes[:, 2] * img_size\n",
        "            h   = boxes[:, 3] * img_size\n",
        "            x1 = (x_c - w/2).clamp(0, img_size)\n",
        "            y1 = (y_c - h/2).clamp(0, img_size)\n",
        "            x2 = (x_c + w/2).clamp(0, img_size)\n",
        "            y2 = (y_c + h/2).clamp(0, img_size)\n",
        "\n",
        "            boxes_xyxy = torch.stack([x1, y1, x2, y2], dim=1)\n",
        "\n",
        "            keep = nms(boxes_xyxy, scores_, iou_thres=nms_iou_thres)\n",
        "            if keep.numel() == 0:\n",
        "                continue\n",
        "\n",
        "            dets = torch.cat([\n",
        "                boxes_xyxy[keep],\n",
        "                scores_[keep].unsqueeze(1),\n",
        "                cls_ids_[keep].float().unsqueeze(1)\n",
        "            ], dim=1)\n",
        "            dets_all.append(dets)\n",
        "\n",
        "        all_outputs.append(torch.cat(dets_all, dim=0) if len(dets_all) > 0 else [])\n",
        "    return all_outputs\n",
        "\n",
        "def compute_ap(recall, precision):\n",
        "    mrec = torch.cat([torch.tensor([0.0]), recall, torch.tensor([1.0])])\n",
        "    mpre = torch.cat([torch.tensor([0.0]), precision, torch.tensor([0.0])])\n",
        "    for i in range(mpre.size(0)-1, 0, -1):\n",
        "        mpre[i-1] = torch.max(mpre[i-1], mpre[i])\n",
        "    idx = (mrec[1:] != mrec[:-1]).nonzero().squeeze()\n",
        "    return ((mrec[idx+1] - mrec[idx]) * mpre[idx+1]).sum().item()\n",
        "\n",
        "def evaluate_map(model, dataloader, num_classes=3, img_size=512, iou_thr=0.5, conf_thres=0.25):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    all_dets = {c: [] for c in range(num_classes)}\n",
        "    all_gts  = {c: [] for c in range(num_classes)}\n",
        "    global_img_id = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_i, (imgs, targets) in enumerate(dataloader):\n",
        "            imgs = imgs.to(device)\n",
        "            targets = [t.to(device) for t in targets]\n",
        "            preds, _ = model(imgs)\n",
        "            dets_list = decode_predictions(preds, img_size=img_size, conf_thres=conf_thres)\n",
        "\n",
        "            for b in range(len(imgs)):\n",
        "                dets = dets_list[b]\n",
        "                gt = targets[b]\n",
        "                current_img_id = global_img_id\n",
        "                global_img_id += 1\n",
        "\n",
        "                if len(gt) > 0:\n",
        "                    gcls = gt[:, 0].long()\n",
        "                    gxy  = gt[:, 1:3] * img_size\n",
        "                    gwh  = gt[:, 3:5] * img_size\n",
        "                    gx1 = gxy[:, 0] - gwh[:, 0]/2\n",
        "                    gy1 = gxy[:, 1] - gwh[:, 1]/2\n",
        "                    gx2 = gxy[:, 0] + gwh[:, 0]/2\n",
        "                    gy2 = gxy[:, 1] + gwh[:, 1]/2\n",
        "                    gboxes = torch.stack([gx1, gy1, gx2, gy2], dim=1)\n",
        "                    for c in range(num_classes):\n",
        "                        mask = (gcls == c)\n",
        "                        if mask.sum() > 0:\n",
        "                            all_gts[c].append((current_img_id, gboxes[mask].cpu()))\n",
        "\n",
        "                if dets is not None and len(dets) > 0:\n",
        "                    boxes, scores, cls_ids = dets[:, :4], dets[:, 4], dets[:, 5].long()\n",
        "                    for c in range(num_classes):\n",
        "                        mask = (cls_ids == c)\n",
        "                        if mask.sum() > 0:\n",
        "                            all_dets[c].append((current_img_id, scores[mask].cpu(), boxes[mask].cpu()))\n",
        "\n",
        "    aps = []\n",
        "    for c in range(num_classes):\n",
        "        gts_c = all_gts[c]\n",
        "        if len(gts_c) == 0:\n",
        "            continue\n",
        "        # [FIX] ÎèôÏùº Ïù¥ÎØ∏ÏßÄ Ïó¨Îü¨ ÏóîÌä∏Î¶¨ concat\n",
        "        gt_dict = {}\n",
        "        for img_id, boxes in gts_c:\n",
        "            if img_id not in gt_dict:\n",
        "                gt_dict[img_id] = {\n",
        "                    \"boxes\": boxes.clone(),\n",
        "                    \"matched\": torch.zeros(boxes.size(0), dtype=torch.bool)\n",
        "                }\n",
        "            else:\n",
        "                old = gt_dict[img_id]\n",
        "                new_boxes = torch.cat([old[\"boxes\"], boxes], dim=0)\n",
        "                new_matched = torch.cat(\n",
        "                    [old[\"matched\"], torch.zeros(boxes.size(0), dtype=torch.bool)],\n",
        "                    dim=0\n",
        "                )\n",
        "                gt_dict[img_id] = {\"boxes\": new_boxes, \"matched\": new_matched}\n",
        "\n",
        "        n_gt = sum(v[\"boxes\"].size(0) for v in gt_dict.values())\n",
        "\n",
        "        dets_c = all_dets[c]\n",
        "        if len(dets_c) == 0:\n",
        "            aps.append(0.0)\n",
        "            continue\n",
        "\n",
        "        scores_all, boxes_all, img_ids_all = [], [], []\n",
        "        for img_id, scores, boxes in dets_c:\n",
        "            for i in range(boxes.size(0)):\n",
        "                scores_all.append(scores[i].item())\n",
        "                boxes_all.append(boxes[i])\n",
        "                img_ids_all.append(img_id)\n",
        "\n",
        "        scores_all = torch.tensor(scores_all)\n",
        "        boxes_all = torch.stack(boxes_all, dim=0)\n",
        "        order = scores_all.argsort(descending=True)\n",
        "        scores_all = scores_all[order]\n",
        "        boxes_all = boxes_all[order]\n",
        "        img_ids_all = [img_ids_all[i] for i in order]\n",
        "\n",
        "        tps = torch.zeros(len(scores_all))\n",
        "        fps = torch.zeros(len(scores_all))\n",
        "\n",
        "        for i in range(len(scores_all)):\n",
        "            img_id = img_ids_all[i]\n",
        "            pred_box = boxes_all[i].unsqueeze(0)\n",
        "            if img_id not in gt_dict:\n",
        "                fps[i] = 1\n",
        "                continue\n",
        "            gt_entry = gt_dict[img_id]\n",
        "            ious = box_iou_matrix(pred_box, gt_entry[\"boxes\"]).squeeze(0)\n",
        "            if ious.numel() == 0:\n",
        "                fps[i] = 1\n",
        "                continue\n",
        "            max_iou, max_idx = ious.max(0)\n",
        "            if max_iou >= iou_thr and not gt_entry[\"matched\"][max_idx]:\n",
        "                tps[i] = 1\n",
        "                gt_entry[\"matched\"][max_idx] = True\n",
        "            else:\n",
        "                fps[i] = 1\n",
        "\n",
        "        tp_cum = torch.cumsum(tps, dim=0)\n",
        "        fp_cum = torch.cumsum(fps, dim=0)\n",
        "        recall = tp_cum / (n_gt + 1e-6)\n",
        "        precision = tp_cum / (tp_cum + fp_cum + 1e-6)\n",
        "        aps.append(compute_ap(recall, precision))\n",
        "\n",
        "    mAP = sum(aps) / len(aps) if len(aps) > 0 else 0.0\n",
        "    return mAP, aps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "417b2f55",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 10: Îç∞Ïù¥ÌÑ∞ÏÖã Ï†ïÏùò\n",
        "# ============================================\n",
        "DATA_PATH = dataset.location\n",
        "train_dataset = YoloDataset(os.path.join(DATA_PATH, \"train\"), is_train=True)\n",
        "val_dataset   = YoloDataset(os.path.join(DATA_PATH, \"valid\"), is_train=False)\n",
        "test_dataset  = YoloDataset(os.path.join(DATA_PATH, \"test\"), is_train=False)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True,\n",
        "                          collate_fn=yolo_collate_fn, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False,\n",
        "                        collate_fn=yolo_collate_fn, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False,\n",
        "                         collate_fn=yolo_collate_fn, num_workers=2, pin_memory=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using Device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a2f274e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 11 : Train Config & Loop\n",
        "# ============================================\n",
        "\n",
        "cfg = dict(\n",
        "    in_ch=3,\n",
        "    stem_base=64,\n",
        "    embed_dim=768,\n",
        "    vit_model_name='vit_base_patch16_224.augreg_in21k_ft_in1k',\n",
        "    num_classes=3,\n",
        "    iters=1,\n",
        "    detach_feedback=False,\n",
        "    img_size=IMG_SIZE\n",
        ")\n",
        "\n",
        "print(f\"‚öôÔ∏è Configuration: {cfg}\")\n",
        "\n",
        "model = HybridTwoWay(**cfg).to(device)\n",
        "\n",
        "# AMP ÏÇ¨Ïö© Ïó¨Î∂Ä\n",
        "use_amp = (device.type == 'cuda')\n",
        "if use_amp:\n",
        "    scaler = torch.amp.GradScaler('cuda')\n",
        "else:\n",
        "    scaler = None\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.05)\n",
        "EPOCHS = 15\n",
        "ACCUM_STEPS = 4\n",
        "\n",
        "steps_per_epoch = max(1, len(train_loader) // ACCUM_STEPS)  # [FIX] 0 Î∞©ÏßÄ\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=2e-5,\n",
        "    epochs=EPOCHS,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    pct_start=0.3,\n",
        "    div_factor=25.0,\n",
        "    final_div_factor=1000.0\n",
        ")\n",
        "\n",
        "best_map = -1.0\n",
        "best_epoch = -1\n",
        "\n",
        "print(\"‚úÖ Safe Mode Ready: Iters=1, Max LR=2e-5\")\n",
        "\n",
        "print(\"üöÄ Start Training (Safe Mode)...\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for i, (imgs, targets) in enumerate(loop):\n",
        "        imgs = imgs.to(device)\n",
        "        targets = [t.to(device) for t in targets]\n",
        "\n",
        "        if use_amp:\n",
        "            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
        "                preds, aux = model(imgs)\n",
        "                loss = yolo_loss_tal(preds, targets, img_size=IMG_SIZE)\n",
        "                loss = loss / ACCUM_STEPS\n",
        "            scaler.scale(loss).backward()\n",
        "        else:\n",
        "            preds, aux = model(imgs)\n",
        "            loss = yolo_loss_tal(preds, targets, img_size=IMG_SIZE)\n",
        "            loss = loss / ACCUM_STEPS\n",
        "            loss.backward()\n",
        "\n",
        "        if (i + 1) % ACCUM_STEPS == 0:\n",
        "            if use_amp:\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "                optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            scheduler.step()\n",
        "\n",
        "        current_loss = loss.item() * ACCUM_STEPS\n",
        "        total_loss += current_loss\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        loop.set_postfix(loss=f\"{current_loss:.4f}\", lr=f\"{current_lr:.8f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} | Train Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    val_map, val_aps = evaluate_map(\n",
        "        model, val_loader,\n",
        "        num_classes=cfg[\"num_classes\"],\n",
        "        img_size=IMG_SIZE,\n",
        "        iou_thr=0.5,\n",
        "        conf_thres=0.001\n",
        "    )\n",
        "    print(f\"Epoch {epoch+1} | Val mAP@0.5: {val_map:.4f}\")\n",
        "\n",
        "    if val_map > best_map:\n",
        "        best_map = val_map\n",
        "        best_epoch = epoch + 1\n",
        "        state_dict = model._orig_mod.state_dict() if hasattr(model, '_orig_mod') else model.state_dict()\n",
        "        ckpt = {\"state_dict\": state_dict, \"cfg\": cfg, \"epoch\": best_epoch, \"val_map\": best_map}\n",
        "        torch.save(ckpt, \"hybrid_two_way_best.pt\")\n",
        "        print(f\"‚úÖ Best model saved! (Val mAP: {best_map:.4f})\")\n",
        "\n",
        "print(\"üèÅ Training Finished!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dded0d33",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 12 : Load Best & Test Eval\n",
        "# ============================================\n",
        "\n",
        "checkpoint = torch.load(\"hybrid_two_way_best.pt\", map_location=device)\n",
        "loaded_cfg = checkpoint[\"cfg\"]\n",
        "\n",
        "print(f\"üìÑ Loaded Config: {loaded_cfg}\")\n",
        "\n",
        "model = HybridTwoWay(**loaded_cfg).to(device)\n",
        "model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "model.eval()\n",
        "\n",
        "print(f\"‚úÖ Model Loaded from Epoch {checkpoint['epoch']} (Val mAP: {checkpoint['val_map']:.4f})\")\n",
        "\n",
        "test_map, class_aps = evaluate_map(\n",
        "    model, test_loader,\n",
        "    num_classes=loaded_cfg[\"num_classes\"],\n",
        "    img_size=IMG_SIZE,\n",
        "    conf_thres=0.001\n",
        ")\n",
        "\n",
        "print(f\"\\nüèÜ Final Test mAP@0.5: {test_map:.4f}\")\n",
        "for i, ap in enumerate(class_aps):\n",
        "    print(f\"   Class {i} AP@0.5: {ap:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 13 : Sanity Check\n",
        "# ============================================\n",
        "\n",
        "x = torch.randn(2, 3, IMG_SIZE, IMG_SIZE).to(device)\n",
        "preds, aux = model(x)\n",
        "for level, (c, o, b) in zip([\"P3\",\"P4\",\"P5\"], preds):\n",
        "    print(f\"[{level}] cls: {list(c.shape)}, obj: {list(o.shape)}, box: {list(b.shape)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8340317e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ÌÅ¥ÎûòÏä§ Î∂ÑÌè¨ ÌôïÏù∏\n",
        "def count_class_dist(dataset, name):\n",
        "    cnt = Counter()\n",
        "    for i in range(len(dataset)):\n",
        "        _, tgt = dataset[i]\n",
        "        if tgt.numel() == 0:\n",
        "            continue\n",
        "        cls_ids = tgt[:, 0].long().tolist()\n",
        "        cnt.update(cls_ids)\n",
        "    print(f\"\\n[{name}] class distribution:\")\n",
        "    for k in sorted(cnt.keys()):\n",
        "        print(f\"  class {k}: {cnt[k]} boxes\")\n",
        "\n",
        "count_class_dist(train_dataset, \"train\")\n",
        "count_class_dist(val_dataset,   \"val\")\n",
        "count_class_dist(test_dataset,  \"test\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
