{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HybridTwoWay Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bef1b69",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install roboflow torch torchvision torchaudio opencv-python numpy tqdm pillow matplotlib albumentations timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77da3f4e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 1: ÏÑ§Ïπò Î∞è Í∏∞Î≥∏ Import, Roboflow Îã§Ïö¥Î°úÎìú\n",
        "# ============================================\n",
        "\n",
        "import math\n",
        "import os\n",
        "from typing import List, Tuple\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from roboflow import Roboflow\n",
        "from tqdm import tqdm\n",
        "import timm # [NEW] PretrainingÏö© ÎùºÏù¥Î∏åÎü¨Î¶¨\n",
        "import albumentations as A # [NEW] Ï¶ùÍ∞ï ÎùºÏù¥Î∏åÎü¨Î¶¨\n",
        "\n",
        "rf = Roboflow(api_key=\"HG9M6YJZpcCUgAQaKO9v\")\n",
        "project = rf.workspace(\"arakon\").project(\"detection-base-hqaeg\")\n",
        "version = project.version(6)\n",
        "dataset = version.download(\"yolov8\")\n",
        "\n",
        "print(f'Roboflow dataset downloaded to: {dataset.location}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 2: Í∏∞Î≥∏ Conv Î∏îÎ°ù + Stem\n",
        "# ============================================\n",
        "\n",
        "def conv_bn_act(in_ch, out_ch, k=3, s=1, p=1, act=True):\n",
        "    m = [nn.Conv2d(in_ch, out_ch, k, s, p, bias=False),\n",
        "         nn.BatchNorm2d(out_ch)]\n",
        "    if act:\n",
        "        m.append(nn.SiLU(inplace=True))\n",
        "    return nn.Sequential(*m)\n",
        "\n",
        "class FixedGaussianBlur(nn.Module):\n",
        "    def __init__(self, channels, k=5, sigma=1.0):\n",
        "        super().__init__()\n",
        "        grid = torch.arange(k).float() - (k - 1) / 2\n",
        "        gauss = torch.exp(-(grid ** 2) / (2 * sigma ** 2))\n",
        "        kernel1d = gauss / gauss.sum()\n",
        "        kernel2d = torch.outer(kernel1d, kernel1d)\n",
        "        weight = kernel2d[None, None, :, :].repeat(channels, 1, 1, 1)\n",
        "        self.register_buffer('weight', weight)\n",
        "        self.groups = channels\n",
        "        self.k = k\n",
        "\n",
        "    def forward(self, x):\n",
        "        pad = (self.k // 2,) * 4\n",
        "        return F.conv2d(F.pad(x, pad, mode='reflect'),\n",
        "                        self.weight, groups=self.groups)\n",
        "class AnomalyAwareStem(nn.Module):\n",
        "    def __init__(self, in_ch=3, base_ch=48):\n",
        "        super().__init__()\n",
        "        C1, C2, C3 = base_ch, base_ch * 2, base_ch * 4\n",
        "        self.stem = nn.Sequential(\n",
        "            conv_bn_act(in_ch, C1, 3, 2, 1),\n",
        "            conv_bn_act(C1, C2, 3, 2, 1),\n",
        "            conv_bn_act(C2, C3, 3, 2, 1),\n",
        "        )\n",
        "        self.blur = FixedGaussianBlur(in_ch, k=5, sigma=1.0)\n",
        "        self.anom = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, in_ch, 3, 1, 1, groups=in_ch, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_ch, C3 // 4, 1, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(C3 // 4),\n",
        "            nn.SiLU(inplace=True),\n",
        "        )\n",
        "        self.fuse = nn.Conv2d(C3 + C3 // 4, C3, 1, 1, 0, bias=False)\n",
        "        self.fuse_bn = nn.BatchNorm2d(C3)\n",
        "        self.vis_head = nn.Conv2d(C3, 1, 1, 1, 0)\n",
        "        self.base_ch = base_ch # Ï†ÄÏû•\n",
        "\n",
        "    @property\n",
        "    def out_channels(self):\n",
        "        return 4 * self.base_ch\n",
        "\n",
        "    def forward(self, x):\n",
        "        f_main = self.stem(x)\n",
        "        blurred = self.blur(x)\n",
        "        high = x - blurred\n",
        "        high_ds = F.interpolate(high, size=f_main.shape[-2:], mode='bilinear', align_corners=False)\n",
        "        f_anom = self.anom(high_ds)\n",
        "        f = torch.cat([f_main, f_anom], dim=1)\n",
        "        f = self.fuse_bn(self.fuse(f))\n",
        "        f = F.silu(f, inplace=True)\n",
        "        v = torch.sigmoid(self.vis_head(f_main))\n",
        "        return f, v\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 3: ViT Encoder + Feedback Adapter\n",
        "# ============================================\n",
        "\n",
        "class PatchEmbed1x1(nn.Module):\n",
        "    def __init__(self, in_ch, embed_dim):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(in_ch, embed_dim, 1, 1, 0, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        x = self.bn(x)\n",
        "        x = F.silu(x, inplace=True)\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, dim, mlp_ratio=4.0, drop=0.0):\n",
        "        super().__init__()\n",
        "        hidden = int(dim * mlp_ratio)\n",
        "        self.fc1 = nn.Linear(dim, hidden)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden, dim)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "class MultiheadSelfAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, attn_drop=0.0, proj_drop=0.0):\n",
        "        super().__init__()\n",
        "        assert dim % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=True)\n",
        "        self.attn_drop_p = attn_drop \n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = (self.qkv(x)\n",
        "               .reshape(B, N, 3, self.num_heads, self.head_dim)\n",
        "               .permute(2, 0, 3, 1, 4))\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        \n",
        "        # Flash Attention\n",
        "        out = F.scaled_dot_product_attention(\n",
        "            q, k, v,\n",
        "            dropout_p=self.attn_drop_p if self.training else 0.0,\n",
        "            scale=self.scale\n",
        "        )\n",
        "        out = out.transpose(1, 2).reshape(B, N, C)\n",
        "        out = self.proj(out)\n",
        "        out = self.proj_drop(out)\n",
        "        return out\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, mlp_ratio=4.0, drop=0.0):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = MultiheadSelfAttention(dim, num_heads, drop, drop)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.mlp = MLP(dim, mlp_ratio, drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class ViTEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim=512, depth=8, num_heads=8):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, mlp_ratio=4.0, drop=0.0)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        for blk in self.blocks:\n",
        "            tokens = blk(tokens)\n",
        "        return tokens\n",
        "\n",
        "class FeedbackAdapter(nn.Module):\n",
        "    def __init__(self, d_token: int, c_stem: int, use_bn: bool = True):\n",
        "        super().__init__()\n",
        "        layers = [nn.Conv2d(d_token, c_stem * 2, 1, 1, 0, bias=not use_bn)]\n",
        "        if use_bn:\n",
        "            layers.append(nn.BatchNorm2d(c_stem * 2))\n",
        "        layers.append(nn.SiLU(inplace=True))\n",
        "        self.adapter = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, tokens, Ht, Wt, f_stem):\n",
        "        B, N, D = tokens.shape\n",
        "        t2d = tokens.transpose(1, 2).reshape(B, D, Ht, Wt)\n",
        "        ab = self.adapter(t2d)\n",
        "        Cs = f_stem.shape[1]\n",
        "        gamma, beta = torch.split(ab, Cs, dim=1)\n",
        "        return f_stem * (1 + torch.tanh(gamma)) + beta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 4: PANLite Neck\n",
        "# ============================================\n",
        "\n",
        "class PANLite(nn.Module):\n",
        "    def __init__(self, in_ch=512, mid=256):\n",
        "        super().__init__()\n",
        "        self.lateral = conv_bn_act(in_ch, mid, 1, 1, 0)\n",
        "        self.down4 = conv_bn_act(mid, mid, 3, 2, 1)\n",
        "        self.down5 = conv_bn_act(mid, mid, 3, 2, 1)\n",
        "        self.up4 = conv_bn_act(mid + mid, mid, 3, 1, 1)\n",
        "        self.up3 = conv_bn_act(mid + mid, mid, 3, 1, 1)\n",
        "        self.down_f4 = conv_bn_act(mid, mid, 3, 2, 1)\n",
        "        self.fuse4 = conv_bn_act(mid + mid, mid, 3, 1, 1)\n",
        "        self.down_f5 = conv_bn_act(mid, mid, 3, 2, 1)\n",
        "        self.fuse5 = conv_bn_act(mid + mid, mid, 3, 1, 1)\n",
        "\n",
        "    def forward(self, p3):\n",
        "        p3 = self.lateral(p3)\n",
        "        p4 = self.down4(p3)\n",
        "        p5 = self.down5(p4)\n",
        "        p4u = F.interpolate(p5, size=p4.shape[-2:], mode='nearest')\n",
        "        p4 = self.up4(torch.cat([p4, p4u], dim=1))\n",
        "        p3u = F.interpolate(p4, size=p3.shape[-2:], mode='nearest')\n",
        "        p3 = self.up3(torch.cat([p3, p3u], dim=1))\n",
        "        p4b = self.down_f4(p3)\n",
        "        p4 = self.fuse4(torch.cat([p4, p4b], dim=1))\n",
        "        p5b = self.down_f5(p4)\n",
        "        p5 = self.fuse5(torch.cat([p5, p5b], dim=1))\n",
        "        return p3, p4, p5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ac62674",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 5: YOLO Head\n",
        "# ============================================\n",
        "\n",
        "class YOLOHeadLite(nn.Module):\n",
        "    def __init__(self, in_ch=256, num_classes=1):\n",
        "        super().__init__()\n",
        "        c = in_ch\n",
        "        self.stem3 = conv_bn_act(c, c, 3, 1, 1)\n",
        "        self.stem4 = conv_bn_act(c, c, 3, 1, 1)\n",
        "        self.stem5 = conv_bn_act(c, c, 3, 1, 1)\n",
        "        self.cls3 = nn.Conv2d(c, num_classes, 1, 1, 0)\n",
        "        self.obj3 = nn.Conv2d(c, 1, 1, 1, 0)\n",
        "        self.box3 = nn.Conv2d(c, 4, 1, 1, 0)\n",
        "        nn.init.constant_(self.obj3.bias, -4.59)\n",
        "        self.cls4 = nn.Conv2d(c, num_classes, 1, 1, 0)\n",
        "        self.obj4 = nn.Conv2d(c, 1, 1, 1, 0)\n",
        "        self.box4 = nn.Conv2d(c, 4, 1, 1, 0)\n",
        "        nn.init.constant_(self.obj4.bias, -4.59)\n",
        "        self.cls5 = nn.Conv2d(c, num_classes, 1, 1, 0)\n",
        "        self.obj5 = nn.Conv2d(c, 1, 1, 1, 0)\n",
        "        self.box5 = nn.Conv2d(c, 4, 1, 1, 0)\n",
        "        nn.init.constant_(self.obj5.bias, -4.59)\n",
        "\n",
        "    def forward_single(self, x, stem, cls, obj, box):\n",
        "        f = stem(x)\n",
        "        return cls(f), obj(f), box(f)\n",
        "\n",
        "    def forward(self, p3, p4, p5):\n",
        "        c3, o3, b3 = self.forward_single(p3, self.stem3, self.cls3, self.obj3, self.box3)\n",
        "        c4, o4, b4 = self.forward_single(p4, self.stem4, self.cls4, self.obj4, self.box4)\n",
        "        c5, o5, b5 = self.forward_single(p5, self.stem5, self.cls5, self.obj5, self.box5)\n",
        "        return [(c3, o3, b3), (c4, o4, b4), (c5, o5, b5)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 6: HybridTwoWay Model with Timm Pretraining\n",
        "# ============================================\n",
        "\n",
        "class HybridTwoWay(nn.Module):\n",
        "    def __init__(self, \n",
        "                 in_ch=3, \n",
        "                 stem_base=32, \n",
        "                 embed_dim=192, # [Î≥ÄÍ≤Ω] ViT-Tiny (192) ÎòêÎäî Small (384)Ïóê ÎßûÏ∂∞Ïïº Ìï®. Ïó¨Í∏∞ÏÑ† Tiny ÏÇ¨Ïö©\n",
        "                 vit_model_name='vit_tiny_patch16_224.augreg_in21k_ft_in1k', # [NEW] Timm Model Name\n",
        "                 num_classes=3, \n",
        "                 iters=1, \n",
        "                 detach_feedback=True, \n",
        "                 img_size=640):\n",
        "        super().__init__()\n",
        "        self.iters = iters\n",
        "        self.detach_feedback = detach_feedback\n",
        "        \n",
        "        # 1) Stem\n",
        "        self.stem = AnomalyAwareStem(in_ch=in_ch, base_ch=stem_base)\n",
        "        c_stem = stem_base * 4  # e.g., 32*4 = 128\n",
        "        \n",
        "        # 2) ViT from Timm (Pretrained)\n",
        "        # Timm Î™®Îç∏ Î°úÎìú (Features Only)\n",
        "        print(f\"üîÑ Loading Pretrained Weights: {vit_model_name}...\")\n",
        "        self.vit = timm.create_model(vit_model_name, pretrained=True, num_classes=0)\n",
        "        self.vit_dim = self.vit.num_features # e.g., 192 for tiny\n",
        "        \n",
        "        # Stem(128) -> ViT(192) Ïó∞Í≤∞ÏùÑ ÏúÑÌïú Projection\n",
        "        self.to_vit = nn.Sequential(\n",
        "            nn.Conv2d(c_stem, self.vit_dim, 1),\n",
        "            nn.BatchNorm2d(self.vit_dim),\n",
        "            nn.SiLU()\n",
        "        )\n",
        "        \n",
        "        # ViT(192) -> Neck(256) Ïó∞Í≤∞ÏùÑ ÏúÑÌïú Projection\n",
        "        self.from_vit = nn.Sequential(\n",
        "            nn.Conv2d(self.vit_dim, 256, 1), # Neck Input is 256\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.SiLU()\n",
        "        )\n",
        "\n",
        "        # Positional Embedding (Dynamic)\n",
        "        # ViT Patch size = 16. Input to ViT is Stem output (stride 8).\n",
        "        # Effective patch size relative to image is 8? No, stem downsamples.\n",
        "        # We will treat Stem output as feature map.\n",
        "        # We rely on Timm's internal PosEmbed or add our own if needed. \n",
        "        # Timm ViT expects [B, N, D].\n",
        "        \n",
        "        # 3) Feedback Adapter (Dimension ÎßûÏ∂∞ÏÑú ÏàòÏ†ï)\n",
        "        self.feedback = FeedbackAdapter(d_token=self.vit_dim, c_stem=c_stem, use_bn=True)\n",
        "        \n",
        "        # 4) Neck + Head\n",
        "        self.neck = PANLite(in_ch=256, mid=256) # Neck input fixed to 256 via projection\n",
        "        self.head = YOLOHeadLite(in_ch=256, num_classes=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1) Stem\n",
        "        f_stem, vis = self.stem(x) # (B, 128, H/8, W/8)\n",
        "        \n",
        "        # Stem -> ViT Dimension Projection\n",
        "        f_vit_in = self.to_vit(f_stem) # (B, 192, H/8, W/8)\n",
        "        \n",
        "        # Flatten for ViT\n",
        "        B, C, Ht, Wt = f_vit_in.shape\n",
        "        tokens = f_vit_in.flatten(2).transpose(1, 2) # (B, N, 192)\n",
        "        \n",
        "        # Timm ViT Forward\n",
        "        # Timm ViT expects Pos Embed inside. But our sequence length changes with img_size.\n",
        "        # We need to interpolate Timm's pos_embed to match current Ht, Wt.\n",
        "        # This is handled by Timm's `forward_features` if we pass grid size, but standard ViT is rigid.\n",
        "        # Hack: We use the tokens as is, assuming ViT handles arbitrary length if no pos_embed check.\n",
        "        # Better: Manually interpolate Timm's pos embed.\n",
        "        \n",
        "        if self.vit.pos_embed is not None:\n",
        "            # Resize pos_embed\n",
        "            # self.vit.pos_embed: (1, N_pre + 1, D)\n",
        "            # We skip class token handling for simplicity as we use features\n",
        "            src_pos = self.vit.pos_embed[:, self.vit.num_prefix_tokens:]\n",
        "            src_N = src_pos.shape[1]\n",
        "            src_h = int(src_N**0.5)\n",
        "            \n",
        "            pos_embed = F.interpolate(\n",
        "                src_pos.reshape(1, src_h, src_h, -1).permute(0, 3, 1, 2),\n",
        "                size=(Ht, Wt), mode='bicubic', align_corners=False\n",
        "            ).flatten(2).transpose(1, 2)\n",
        "            \n",
        "            # Add to tokens\n",
        "            tokens = tokens + pos_embed\n",
        "\n",
        "        f_fb = f_stem\n",
        "        preds, aux = None, None\n",
        "        \n",
        "        for i in range(self.iters):\n",
        "            # ViT Blocks\n",
        "            tokens = self.vit.blocks(tokens)\n",
        "            tokens = self.vit.norm(tokens)\n",
        "\n",
        "            # Feedback\n",
        "            toks_for_fb = tokens.detach() if self.detach_feedback else tokens\n",
        "            f_fb = self.feedback(toks_for_fb, Ht, Wt, f_fb) # Modifies f_stem features\n",
        "            \n",
        "            # Prepare for Neck\n",
        "            # We need to mix f_fb (Stem domain) and tokens (ViT domain)?\n",
        "            # Or just use f_fb as per original design.\n",
        "            # The original design uses `patch(f_fb)` to go to Neck.\n",
        "            # Here we should project f_fb (128) -> Neck (256)\n",
        "            # But wait, `f_fb` is refreshed stem feature.\n",
        "            \n",
        "            # Re-project f_fb to ViT dim for next loop\n",
        "            if i != self.iters - 1:\n",
        "                 f_vit_in = self.to_vit(f_fb)\n",
        "                 tokens = f_vit_in.flatten(2).transpose(1, 2) + pos_embed\n",
        "\n",
        "            # Project f_fb to Neck\n",
        "            # Original used `self.patch` (Conv 1x1). We use `to_vit` then `from_vit`?\n",
        "            # Let's use a separate projection or reuse.\n",
        "            # Let's project f_fb (128) -> Neck (256) directly\n",
        "            # We need a layer for this. Let's use `self.to_vit` output passed through `self.from_vit` \n",
        "            # effectively 128->192->256.\n",
        "            \n",
        "            p3_in = self.from_vit(self.to_vit(f_fb))\n",
        "            \n",
        "            p3, p4, p5 = self.neck(p3_in)\n",
        "            preds = self.head(p3, p4, p5)\n",
        "            aux = {\"P3\": p3, \"P4\": p4, \"P5\": p5, \"V\": vis}\n",
        "\n",
        "        return preds, aux"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b16ccb66",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 7: Dataset with Mosaic Augmentation\n",
        "# ============================================\n",
        "\n",
        "IMG_SIZE = 640\n",
        "\n",
        "def yolo_collate_fn(batch):\n",
        "    imgs = []\n",
        "    targets = []\n",
        "    for img, tgt in batch:\n",
        "        imgs.append(img)\n",
        "        targets.append(tgt)\n",
        "    imgs = torch.stack(imgs, 0)\n",
        "    return imgs, targets\n",
        "\n",
        "class YoloDataset(Dataset):\n",
        "    def __init__(self, root, is_train=True, mosaic_prob=0.5):\n",
        "        self.img_dir = os.path.join(root, \"images\")\n",
        "        self.label_dir = os.path.join(root, \"labels\")\n",
        "        self.images = sorted(os.listdir(self.img_dir))\n",
        "        self.is_train = is_train\n",
        "        self.mosaic_prob = mosaic_prob\n",
        "        self.img_size = IMG_SIZE\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def load_image_and_boxes(self, index):\n",
        "        # Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n",
        "        name = self.images[index]\n",
        "        img_path = os.path.join(self.img_dir, name)\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None: raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
        "        h, w = img.shape[:2]\n",
        "        \n",
        "        # ÎùºÎ≤® Î°úÎìú\n",
        "        label_path = os.path.join(self.label_dir, name.replace(\".jpg\",\".txt\").replace(\".png\",\".txt\"))\n",
        "        boxes = []\n",
        "        if os.path.exists(label_path):\n",
        "            with open(label_path, \"r\") as f:\n",
        "                for line in f.readlines():\n",
        "                    # class, x_c, y_c, w, h (normalized)\n",
        "                    cls, cx, cy, bw, bh = map(float, line.split())\n",
        "                    # normalized -> absolute xyxy\n",
        "                    x1 = (cx - bw/2) * w\n",
        "                    y1 = (cy - bh/2) * h\n",
        "                    x2 = (cx + bw/2) * w\n",
        "                    y2 = (cy + bh/2) * h\n",
        "                    boxes.append([cls, x1, y1, x2, y2])\n",
        "        return img, np.array(boxes) if len(boxes) > 0 else np.zeros((0, 5))\n",
        "\n",
        "    def load_mosaic(self, index):\n",
        "        # 4Ïû•Ïùò Ïù¥ÎØ∏ÏßÄÎ•º Ìï©Ï≥êÏÑú ÌïòÎÇòÎ°ú ÎßåÎì¶\n",
        "        indices = [index] + [random.randint(0, len(self.images) - 1) for _ in range(3)]\n",
        "        \n",
        "        yc, xc = [int(random.uniform(-x, 2 * self.img_size + x)) for x in [-self.img_size // 2]]\n",
        "        \n",
        "        mosaic_img = np.full((self.img_size * 2, self.img_size * 2, 3), 114, dtype=np.uint8)\n",
        "        mosaic_boxes = []\n",
        "\n",
        "        for i, idx in enumerate(indices):\n",
        "            img, boxes = self.load_image_and_boxes(idx)\n",
        "            h, w = img.shape[:2]\n",
        "\n",
        "            # Resize keeping aspect ratio\n",
        "            r = self.img_size / max(h, w)\n",
        "            if r != 1: \n",
        "                img = cv2.resize(img, (int(w * r), int(h * r)), interpolation=cv2.INTER_LINEAR)\n",
        "            h, w = img.shape[:2]\n",
        "\n",
        "            # Place image in mosaic\n",
        "            if i == 0:  # top left\n",
        "                x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc\n",
        "                x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h\n",
        "            elif i == 1:  # top right\n",
        "                x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, self.img_size * 2), yc\n",
        "                x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h\n",
        "            elif i == 2:  # bottom left\n",
        "                x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(self.img_size * 2, yc + h)\n",
        "                x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, w, min(y2a - y1a, h)\n",
        "            elif i == 3:  # bottom right\n",
        "                x1a, y1a, x2a, y2a = xc, yc, min(xc + w, self.img_size * 2), min(self.img_size * 2, yc + h)\n",
        "                x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)\n",
        "\n",
        "            mosaic_img[y1a:y2a, x1a:x2a] = img[y1b:y2b, x1b:x2b]\n",
        "            padw = x1a - x1b\n",
        "            padh = y1a - y1b\n",
        "\n",
        "            # Adjust boxes\n",
        "            if len(boxes) > 0:\n",
        "                boxes[:, 1] = boxes[:, 1] * r + padw\n",
        "                boxes[:, 2] = boxes[:, 2] * r + padh\n",
        "                boxes[:, 3] = boxes[:, 3] * r + padw\n",
        "                boxes[:, 4] = boxes[:, 4] * r + padh\n",
        "                mosaic_boxes.append(boxes)\n",
        "\n",
        "        if len(mosaic_boxes) > 0:\n",
        "            mosaic_boxes = np.concatenate(mosaic_boxes, 0)\n",
        "            # Clip boxes\n",
        "            np.clip(mosaic_boxes[:, 1:], 0, 2 * self.img_size, out=mosaic_boxes[:, 1:])\n",
        "        \n",
        "        # Center Crop (640x640)\n",
        "        img = mosaic_img\n",
        "        boxes = mosaic_boxes if len(mosaic_boxes) > 0 else np.zeros((0, 5))\n",
        "        \n",
        "        # Random Center Crop\n",
        "        h, w = img.shape[:2]\n",
        "        if h > self.img_size and w > self.img_size:\n",
        "             top = random.randint(0, h - self.img_size)\n",
        "             left = random.randint(0, w - self.img_size)\n",
        "             img = img[top:top+self.img_size, left:left+self.img_size]\n",
        "             \n",
        "             if len(boxes) > 0:\n",
        "                 boxes[:, 1] -= left\n",
        "                 boxes[:, 2] -= top\n",
        "                 boxes[:, 3] -= left\n",
        "                 boxes[:, 4] -= top\n",
        "                 np.clip(boxes[:, 1:], 0, self.img_size, out=boxes[:, 1:])\n",
        "        \n",
        "        return img, boxes\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Mosaic Ï†ÅÏö© Ïó¨Î∂Ä Í≤∞Ï†ï\n",
        "        if self.is_train and random.random() < self.mosaic_prob:\n",
        "            img, boxes_xyxy = self.load_mosaic(idx)\n",
        "        else:\n",
        "            img, boxes_xyxy = self.load_image_and_boxes(idx)\n",
        "            # Resize to IMG_SIZE (Letterbox ÏóÜÏù¥ Îã®Ïàú Î¶¨ÏÇ¨Ïù¥Ï¶à - Í∏∞Ï°¥ Ïú†ÏßÄ)\n",
        "            img = cv2.resize(img, (self.img_size, self.img_size))\n",
        "            h_orig, w_orig = img.shape[:2] # Actually it's 640, 640 now\n",
        "            # boxes need to be rescaled if not mosaic (load_image_and_boxes returns absolute original)\n",
        "            # But we loaded absolute. Let's simple resize boxes logic here for non-mosaic\n",
        "            # Wait, load_image_and_boxes returns absolute coord of ORIGINAL image.\n",
        "            # We need to re-read mostly. Simpler approach:\n",
        "            \n",
        "            # Re-read raw for non-mosaic to match logic perfectly\n",
        "            name = self.images[idx]\n",
        "            img_raw = cv2.imread(os.path.join(self.img_dir, name))\n",
        "            img_raw = cv2.cvtColor(img_raw, cv2.COLOR_BGR2RGB)\n",
        "            h0, w0 = img_raw.shape[:2]\n",
        "            img = cv2.resize(img_raw, (self.img_size, self.img_size))\n",
        "            \n",
        "            label_path = os.path.join(self.label_dir, name.replace(\".jpg\",\".txt\").replace(\".png\",\".txt\"))\n",
        "            boxes_xyxy = []\n",
        "            if os.path.exists(label_path):\n",
        "                with open(label_path, \"r\") as f:\n",
        "                    for line in f.readlines():\n",
        "                        c, cx, cy, bw, bh = map(float, line.split())\n",
        "                        # norm -> absolute 640\n",
        "                        x1 = (cx - bw/2) * self.img_size\n",
        "                        y1 = (cy - bh/2) * self.img_size\n",
        "                        x2 = (cx + bw/2) * self.img_size\n",
        "                        y2 = (cy + bh/2) * self.img_size\n",
        "                        boxes_xyxy.append([c, x1, y1, x2, y2])\n",
        "            boxes_xyxy = np.array(boxes_xyxy) if len(boxes_xyxy) > 0 else np.zeros((0, 5))\n",
        "\n",
        "        # Normalize & Tensor conversion\n",
        "        img = img.astype(np.float32) / 255.0\n",
        "        img = torch.from_numpy(img).permute(2, 0, 1) # (3, 640, 640)\n",
        "        \n",
        "        # Boxes: [cls, x1, y1, x2, y2] -> [cls, cx, cy, w, h] (Normalized) for Loss\n",
        "        targets = []\n",
        "        if len(boxes_xyxy) > 0:\n",
        "            # Remove small boxes\n",
        "            boxes_xyxy = boxes_xyxy[np.logical_and(boxes_xyxy[:, 3] > boxes_xyxy[:, 1], boxes_xyxy[:, 4] > boxes_xyxy[:, 2])]\n",
        "            \n",
        "            for box in boxes_xyxy:\n",
        "                cls = box[0]\n",
        "                x1, y1, x2, y2 = box[1:]\n",
        "                cx = (x1 + x2) / 2 / self.img_size\n",
        "                cy = (y1 + y2) / 2 / self.img_size\n",
        "                w = (x2 - x1) / self.img_size\n",
        "                h = (y2 - y1) / self.img_size\n",
        "                targets.append([cls, cx, cy, w, h])\n",
        "        \n",
        "        targets = torch.tensor(targets, dtype=torch.float32)\n",
        "        return img, targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98c51e46",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 8: Task Aligned Loss (TAL) - YOLOv8 Style\n",
        "# ============================================\n",
        "\n",
        "def select_highest_overlaps(mask_pos, overlaps, n_max_boxes):\n",
        "    # GTÏôÄ Í∞ÄÏû• ÎßéÏù¥ Í≤πÏπòÎäî ÏïµÏª§ ÌïòÎÇòÎäî Î¨¥Ï°∞Í±¥ Ìè¨Ìï®ÏãúÌÇ§Îäî Î°úÏßÅ (ÏïàÏ†ïÏÑ±)\n",
        "    fg_mask = mask_pos.sum(-2) > 0\n",
        "    if fg_mask.all():\n",
        "        return mask_pos\n",
        "    _, argmax_overlaps = overlaps.max(dim=0)\n",
        "    mask_pos[argmax_overlaps, torch.arange(mask_pos.shape[1], device=mask_pos.device)] = True\n",
        "    return mask_pos\n",
        "\n",
        "def select_candidates_in_gts(xy_centers, gt_bboxes, eps=1e-9):\n",
        "    # ÏïµÏª§ Ìè¨Ïù∏Ìä∏Í∞Ä GT Î∞ïÏä§ ÏïàÏóê Îì§Ïñ¥Ïò§ÎäîÏßÄ ÌôïÏù∏\n",
        "    n_anchors = xy_centers.shape[0]\n",
        "    bs, n_boxes, _ = gt_bboxes.shape\n",
        "    lt, rb = gt_bboxes.view(-1, 1, 4).chunk(2, 2) \n",
        "    bbox_deltas = torch.cat((xy_centers[None] - lt, rb - xy_centers[None]), dim=2).view(bs, n_boxes, n_anchors, -1)\n",
        "    return bbox_deltas.min(3)[0] > eps\n",
        "\n",
        "class TaskAlignedAssigner(nn.Module):\n",
        "    def __init__(self, topk=13, alpha=1.0, beta=6.0, eps=1e-9):\n",
        "        super().__init__()\n",
        "        self.topk = topk\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.eps = eps\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt):\n",
        "        # pd_scores: (B, A, C)\n",
        "        # pd_bboxes: (B, A, 4)\n",
        "        # anc_points: (A, 2)\n",
        "        # gt_bboxes: (B, M, 4)\n",
        "        \n",
        "        bs, n_max_boxes = gt_bboxes.shape[:2]\n",
        "        mask_pos, align_metric, overlaps = self.get_pos_mask(\n",
        "            pd_scores, pd_bboxes, gt_labels, gt_bboxes, anc_points, mask_gt\n",
        "        )\n",
        "        \n",
        "        target_gt_idx, fg_mask, mask_pos = self.select_topk_candidates(\n",
        "            align_metric * mask_pos, topk_mask=mask_gt.repeat([1, 1, self.topk]).bool()\n",
        "        )\n",
        "        \n",
        "        # Assign Targets\n",
        "        target_labels = gt_labels.long().flatten()[target_gt_idx.flatten()]\n",
        "        target_labels = target_labels.view(bs, -1)\n",
        "        target_bboxes = gt_bboxes.view(-1, 4)[target_gt_idx.flatten()]\n",
        "        target_bboxes = target_bboxes.view(bs, -1, 4)\n",
        "        \n",
        "        # Metric as score target\n",
        "        align_metric *= mask_pos\n",
        "        pos_align_metrics = align_metric.max(1)[0]\n",
        "        target_scores = pos_align_metrics[..., None].repeat(1, 1, 1) # dummy expand\n",
        "        \n",
        "        # One-hot encoding\n",
        "        t_scores = torch.zeros_like(pd_scores)\n",
        "        t_scores[fg_mask] = 0.0 # Clear\n",
        "        # Indexing is tricky, simple loop or scatter\n",
        "        # TAL usually sets target score = metric\n",
        "        \n",
        "        # Simplification for Colab:\n",
        "        # Just return masks and targets for loss calc\n",
        "        return target_labels, target_bboxes, target_scores, fg_mask, target_gt_idx\n",
        "\n",
        "    def get_pos_mask(self, pd_scores, pd_bboxes, gt_labels, gt_bboxes, anc_points, mask_gt):\n",
        "        bs, n_max_boxes = gt_bboxes.shape[:2]\n",
        "        mask_in_gts = select_candidates_in_gts(anc_points, gt_bboxes) # (B, M, A)\n",
        "        \n",
        "        # Compute IoU & Metric\n",
        "        # pd_bboxes: (B, A, 4), gt_bboxes: (B, M, 4)\n",
        "        # Expand for broadcasting\n",
        "        # overlaps: (B, M, A)\n",
        "        \n",
        "        # Simple IoU calculation for assignment\n",
        "        # ... (Full CIoU implementation omitted for brevity, using simplified bbox_iou)\n",
        "        # Assuming pd_bboxes and gt_bboxes are xyxy\n",
        "        \n",
        "        # Broadcasting IoU\n",
        "        # This can be heavy. Loop per batch if OOM.\n",
        "        overlaps = []\n",
        "        for b in range(bs):\n",
        "            iou = box_iou(gt_bboxes[b], pd_bboxes[b]) # (M, A)\n",
        "            overlaps.append(iou)\n",
        "        overlaps = torch.stack(overlaps)\n",
        "        \n",
        "        # Metric: s^alpha * u^beta\n",
        "        # pd_scores: (B, A, C) -> gather gt labels\n",
        "        # pd_scores for corresponding GT class\n",
        "        # gather: (B, M, A)\n",
        "        bbox_scores = []\n",
        "        for b in range(bs):\n",
        "            # gt_labels[b]: (M, 1)\n",
        "            labels = gt_labels[b].long().squeeze(-1)\n",
        "            scores = pd_scores[b, :, labels].T # (M, A)\n",
        "            bbox_scores.append(scores)\n",
        "        bbox_scores = torch.stack(bbox_scores)\n",
        "        \n",
        "        align_metric = bbox_scores.pow(self.alpha) * overlaps.pow(self.beta)\n",
        "        \n",
        "        mask_pos = mask_in_gts * mask_gt.view(bs, n_max_boxes, 1)\n",
        "        return mask_pos, align_metric, overlaps\n",
        "\n",
        "    def select_topk_candidates(self, metrics, topk_mask=None):\n",
        "        # metrics: (B, M, A)\n",
        "        num_anchors = metrics.shape[-1]\n",
        "        topk_metrics, topk_idxs = torch.topk(metrics, self.topk, dim=-1, largest=True)\n",
        "        \n",
        "        topk_mask = torch.zeros_like(metrics, dtype=torch.bool)\n",
        "        topk_mask.scatter_(-1, topk_idxs, True)\n",
        "        \n",
        "        # Fail safe: at least one anchor\n",
        "        # (Omitted)\n",
        "        \n",
        "        mask_pos = topk_mask\n",
        "        fg_mask = mask_pos.sum(-2) > 0 # (B, A)\n",
        "        \n",
        "        # If anchor assigned to multiple GTs, pick max metric\n",
        "        target_gt_idx = mask_pos.float().argmax(-2) # (B, A)\n",
        "        \n",
        "        return target_gt_idx, fg_mask, mask_pos\n",
        "\n",
        "\n",
        "# ----- Loss Wrapper -----\n",
        "class ComputeLoss(nn.Module):\n",
        "    def __init__(self, num_classes=3):\n",
        "        super().__init__()\n",
        "        self.assigner = TaskAlignedAssigner(topk=10)\n",
        "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def forward(self, preds, batch_targets, img_size):\n",
        "        # preds: [(cls, obj, box), ...] (3 scales)\n",
        "        # batch_targets: List[Tensor] len B, Tensor(M, 5)\n",
        "        \n",
        "        # 1. Concat Predictions\n",
        "        # We need to flatten all scales to (B, Total_Anchors, C/4)\n",
        "        cls_preds = []\n",
        "        box_preds = []\n",
        "        anchors = []\n",
        "        strides = []\n",
        "        \n",
        "        for i, (cls, obj, box) in enumerate(preds):\n",
        "            B, C, H, W = cls.shape\n",
        "            stride = img_size // H\n",
        "            \n",
        "            # Create Grid\n",
        "            grid_y, grid_x = torch.meshgrid(torch.arange(H), torch.arange(W), indexing='ij')\n",
        "            grid = torch.stack((grid_x, grid_y), 2).to(cls.device).float()\n",
        "            grid = (grid + 0.5) * stride # Center\n",
        "            anchors.append(grid.view(-1, 2))\n",
        "            strides.append(torch.full((H*W,), stride, device=cls.device))\n",
        "            \n",
        "            cls_preds.append(cls.permute(0, 2, 3, 1).reshape(B, -1, C))\n",
        "            # Box: xywh logits -> xyxy\n",
        "            # Our model outputs xywh normalized (sigmoid). \n",
        "            # We need to decode it to absolute xyxy for Assigner\n",
        "            b_p = box.permute(0, 2, 3, 1).reshape(B, -1, 4).sigmoid()\n",
        "            # decode: cx, cy, w, h (norm) -> abs\n",
        "            # x = (pred_x * stride + grid_x) ? No, our model predicts global norm coordinates directly.\n",
        "            # Let's assume model outputs global normalized sigmoid as before.\n",
        "            \n",
        "            cx = b_p[..., 0] * img_size\n",
        "            cy = b_p[..., 1] * img_size\n",
        "            w  = b_p[..., 2] * img_size\n",
        "            h  = b_p[..., 3] * img_size\n",
        "            x1 = cx - w/2\n",
        "            y1 = cy - h/2\n",
        "            x2 = cx + w/2\n",
        "            y2 = cy + h/2\n",
        "            box_preds.append(torch.stack([x1, y1, x2, y2], -1))\n",
        "\n",
        "        cls_preds = torch.cat(cls_preds, 1) # (B, A_tot, C)\n",
        "        box_preds = torch.cat(box_preds, 1) # (B, A_tot, 4)\n",
        "        anchors = torch.cat(anchors, 0)     # (A_tot, 2)\n",
        "        \n",
        "        # 2. Prepare Targets\n",
        "        # Pad targets to fixed size M for batch processing\n",
        "        max_boxes = max([len(t) for t in batch_targets])\n",
        "        batch_gt_labels = torch.zeros(B, max_boxes, 1).to(cls_preds.device)\n",
        "        batch_gt_bboxes = torch.zeros(B, max_boxes, 4).to(cls_preds.device)\n",
        "        batch_mask_gt = torch.zeros(B, max_boxes).to(cls_preds.device)\n",
        "        \n",
        "        for b, t in enumerate(batch_targets):\n",
        "            n = len(t)\n",
        "            if n > 0:\n",
        "                batch_gt_labels[b, :n, 0] = t[:, 0]\n",
        "                # t is [cls, cx, cy, w, h] norm\n",
        "                cx, cy, w, h = t[:, 1], t[:, 2], t[:, 3], t[:, 4]\n",
        "                x1 = (cx - w/2) * img_size\n",
        "                y1 = (cy - h/2) * img_size\n",
        "                x2 = (cx + w/2) * img_size\n",
        "                y2 = (cy + h/2) * img_size\n",
        "                batch_gt_bboxes[b, :n] = torch.stack([x1, y1, x2, y2], -1)\n",
        "                batch_mask_gt[b, :n] = 1.0\n",
        "\n",
        "        # 3. Assign\n",
        "        if max_boxes == 0:\n",
        "            return cls_preds.sum() * 0.0 # Zero loss\n",
        "            \n",
        "        targets = self.assigner(\n",
        "            cls_preds.sigmoid(), box_preds, anchors, \n",
        "            batch_gt_labels, batch_gt_bboxes, batch_mask_gt\n",
        "        )\n",
        "        target_labels, target_bboxes, target_scores, fg_mask, target_gt_idx = targets\n",
        "        \n",
        "        # 4. Calculate Loss\n",
        "        # Classification (BCE)\n",
        "        # target_scores: (B, M, A) -> max over M -> (B, A) is essentially \"soft label\"\n",
        "        # But Assigner output is tricky.\n",
        "        # Simplified:\n",
        "        # target_labels: (B, A) -> Class index\n",
        "        # fg_mask: (B, A) -> Positive mask\n",
        "        \n",
        "        # Build target one-hot scaled by alignment metric (Soft Label)\n",
        "        # For simplicity, Hard Label 1.0 for positives\n",
        "        \n",
        "        target_cls = torch.zeros_like(cls_preds)\n",
        "        # Scatter indices\n",
        "        # This part needs careful indexing.\n",
        "        # Using simple BCE on positives for now\n",
        "        \n",
        "        num_pos = fg_mask.sum().clamp(1.0)\n",
        "        \n",
        "        # Cls Loss\n",
        "        # For positives: Target = 1 (or IoU score). For negatives: Target = 0\n",
        "        target_cls[fg_mask] = 0 \n",
        "        # We need to set 1 at target_labels indices\n",
        "        for b in range(B):\n",
        "            if fg_mask[b].sum() > 0:\n",
        "                pos_idx = fg_mask[b].nonzero().squeeze(-1)\n",
        "                cls_idx = target_labels[b][pos_idx].long()\n",
        "                target_cls[b, pos_idx, cls_idx] = 1.0 # Can be replaced with IoU score\n",
        "        \n",
        "        loss_cls = self.bce(cls_preds, target_cls).sum() / num_pos\n",
        "        \n",
        "        # Box Loss (CIoU / GIoU)\n",
        "        loss_box = 0.0\n",
        "        if fg_mask.sum() > 0:\n",
        "            pos_pred_box = box_preds[fg_mask]\n",
        "            pos_tgt_box = target_bboxes[fg_mask]\n",
        "            loss_box = (1.0 - box_iou(pos_pred_box, pos_tgt_box).diag()).sum() / num_pos\n",
        "            \n",
        "        return loss_cls + 4.0 * loss_box\n",
        "\n",
        "# Replace the old function with this class\n",
        "criterion = ComputeLoss(num_classes=3)\n",
        "\n",
        "def yolo_loss_tal(preds, targets, img_size):\n",
        "    return criterion(preds, targets, img_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "276bf933",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 9: Decode Predictions + mAP Evaluation\n",
        "# ============================================\n",
        "\n",
        "def box_iou(box1, box2):\n",
        "    N = box1.size(0)\n",
        "    M = box2.size(0)\n",
        "    if N == 0 or M == 0: return torch.zeros(N, M)\n",
        "    tl = torch.max(box1[:, None, :2], box2[:, :2])\n",
        "    br = torch.min(box1[:, None, 2:], box2[:, 2:])\n",
        "    wh = (br - tl).clamp(min=0)\n",
        "    inter = wh[..., 0] * wh[..., 1]\n",
        "    area1 = (box1[:, 2]-box1[:, 0]) * (box1[:, 3]-box1[:, 1])\n",
        "    area2 = (box2[:, 2]-box2[:, 0]) * (box2[:, 3]-box2[:, 1])\n",
        "    return inter / (area1[:, None] + area2 - inter + 1e-6)\n",
        "\n",
        "def nms(boxes, scores, iou_thres=0.5):\n",
        "    if boxes.numel() == 0: return torch.zeros(0, dtype=torch.long, device=boxes.device)\n",
        "    idxs = scores.argsort(descending=True)\n",
        "    keep = []\n",
        "    while idxs.numel() > 0:\n",
        "        i = idxs[0]\n",
        "        keep.append(i.item())\n",
        "        if idxs.numel() == 1: break\n",
        "        ious = box_iou(boxes[i].unsqueeze(0), boxes[idxs[1:]]).squeeze(0)\n",
        "        idxs = idxs[1:][ious < iou_thres]\n",
        "    return torch.tensor(keep, dtype=torch.long, device=boxes.device)\n",
        "\n",
        "def decode_predictions(preds, img_size=512, conf_thres=0.25, nms_iou_thres=0.5):\n",
        "    all_outputs = []\n",
        "    B = preds[0][0].shape[0]\n",
        "    \n",
        "    for b in range(B):\n",
        "        dets_all = []\n",
        "        for (cls_pred, obj_pred, box_pred) in preds:\n",
        "            B_s, C, H, W = cls_pred.shape\n",
        "            \n",
        "            cls_logits = cls_pred[b].permute(1,2,0).reshape(H*W, C)\n",
        "            box_logits = box_pred[b].permute(1,2,0).reshape(H*W, 4)\n",
        "            \n",
        "            # [Î≥ÄÍ≤Ω] TALÏóêÏÑúÎäî ObjectnessÎ•º Î≥ÑÎèÑÎ°ú Ïì∞ÏßÄ ÏïäÏùå (ÌïôÏäµ Ïïà Îê®)\n",
        "            # Îî∞ÎùºÏÑú cls_scores ÏûêÏ≤¥Í∞Ä Ïã†Î¢∞ÎèÑÍ∞Ä Îê®\n",
        "            cls_scores = cls_logits.sigmoid()\n",
        "            box_norm = box_logits.sigmoid()\n",
        "            \n",
        "            cls_max_scores, cls_ids = cls_scores.max(dim=-1)\n",
        "            \n",
        "            # [Î≥ÄÍ≤Ω] scores = obj_scores * cls_max_scores  <-- ÏÇ≠Ï†ú\n",
        "            scores = cls_max_scores  # <-- Class scoreÎßå ÏÇ¨Ïö©\n",
        "            \n",
        "            mask = scores > conf_thres\n",
        "            if mask.sum() == 0: continue\n",
        "            \n",
        "            scores_ = scores[mask]\n",
        "            cls_ids_ = cls_ids[mask]\n",
        "            boxes = box_norm[mask]\n",
        "            \n",
        "            x_c, y_c, w, h = boxes[:, 0]*img_size, boxes[:, 1]*img_size, boxes[:, 2]*img_size, boxes[:, 3]*img_size\n",
        "            x1, y1 = (x_c - w/2).clamp(0, img_size), (y_c - h/2).clamp(0, img_size)\n",
        "            x2, y2 = (x_c + w/2).clamp(0, img_size), (y_c + h/2).clamp(0, img_size)\n",
        "            \n",
        "            boxes_xyxy = torch.stack([x1, y1, x2, y2], dim=1)\n",
        "            \n",
        "            keep = nms(boxes_xyxy, scores_, iou_thres=nms_iou_thres)\n",
        "            if keep.numel() == 0: continue\n",
        "            \n",
        "            dets = torch.cat([boxes_xyxy[keep], scores_[keep].unsqueeze(1), cls_ids_[keep].float().unsqueeze(1)], dim=1)\n",
        "            dets_all.append(dets)\n",
        "            \n",
        "        all_outputs.append(torch.cat(dets_all, dim=0) if len(dets_all) > 0 else [])\n",
        "    return all_outputs\n",
        "\n",
        "def compute_ap(recall, precision):\n",
        "    mrec = torch.cat([torch.tensor([0.0]), recall, torch.tensor([1.0])])\n",
        "    mpre = torch.cat([torch.tensor([0.0]), precision, torch.tensor([0.0])])\n",
        "    for i in range(mpre.size(0)-1, 0, -1):\n",
        "        mpre[i-1] = torch.max(mpre[i-1], mpre[i])\n",
        "    idx = (mrec[1:] != mrec[:-1]).nonzero().squeeze()\n",
        "    return ((mrec[idx+1] - mrec[idx]) * mpre[idx+1]).sum().item()\n",
        "\n",
        "def evaluate_map(model, dataloader, num_classes=3, img_size=512, iou_thr=0.5, conf_thres=0.25):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    all_dets = {c: [] for c in range(num_classes)}\n",
        "    all_gts  = {c: [] for c in range(num_classes)}\n",
        "    global_img_id = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_i, (imgs, targets) in enumerate(dataloader):\n",
        "            imgs = imgs.to(device)\n",
        "            targets = [t.to(device) for t in targets]\n",
        "            preds, _ = model(imgs)\n",
        "            dets_list = decode_predictions(preds, img_size=img_size, conf_thres=conf_thres)\n",
        "            \n",
        "            for b in range(len(imgs)):\n",
        "                dets = dets_list[b]\n",
        "                gt = targets[b]\n",
        "                current_img_id = global_img_id\n",
        "                global_img_id += 1\n",
        "                \n",
        "                if len(gt) > 0:\n",
        "                    gcls = gt[:, 0].long()\n",
        "                    gxy, gwh = gt[:, 1:3] * img_size, gt[:, 3:5] * img_size\n",
        "                    gx1, gy1 = gxy[:, 0] - gwh[:, 0]/2, gxy[:, 1] - gwh[:, 1]/2\n",
        "                    gx2, gy2 = gxy[:, 0] + gwh[:, 0]/2, gxy[:, 1] + gwh[:, 1]/2\n",
        "                    gboxes = torch.stack([gx1, gy1, gx2, gy2], dim=1)\n",
        "                    for c in range(num_classes):\n",
        "                        mask = (gcls == c)\n",
        "                        if mask.sum() > 0: all_gts[c].append((current_img_id, gboxes[mask].cpu()))\n",
        "                \n",
        "                if dets is not None and len(dets) > 0:\n",
        "                    boxes, scores, cls_ids = dets[:, :4], dets[:, 4], dets[:, 5].long()\n",
        "                    for c in range(num_classes):\n",
        "                        mask = (cls_ids == c)\n",
        "                        if mask.sum() > 0: all_dets[c].append((current_img_id, scores[mask].cpu(), boxes[mask].cpu()))\n",
        "\n",
        "    aps = []\n",
        "    for c in range(num_classes):\n",
        "        gts_c = all_gts[c]\n",
        "        if len(gts_c) == 0: continue\n",
        "        n_gt = sum(boxes.size(0) for _, boxes in gts_c)\n",
        "        gt_dict = {}\n",
        "        for img_id, boxes in gts_c:\n",
        "            gt_dict.setdefault(img_id, [])\n",
        "            gt_dict[img_id].append({\"boxes\": boxes, \"matched\": torch.zeros(boxes.size(0), dtype=torch.bool)})\n",
        "        \n",
        "        dets_c = all_dets[c]\n",
        "        if len(dets_c) == 0:\n",
        "            aps.append(0.0)\n",
        "            continue\n",
        "        \n",
        "        scores_all, boxes_all, img_ids_all = [], [], []\n",
        "        for img_id, scores, boxes in dets_c:\n",
        "            for i in range(boxes.size(0)):\n",
        "                scores_all.append(scores[i].item())\n",
        "                boxes_all.append(boxes[i])\n",
        "                img_ids_all.append(img_id)\n",
        "        \n",
        "        scores_all = torch.tensor(scores_all)\n",
        "        boxes_all = torch.stack(boxes_all, dim=0)\n",
        "        order = scores_all.argsort(descending=True)\n",
        "        scores_all, boxes_all = scores_all[order], boxes_all[order]\n",
        "        img_ids_all = [img_ids_all[i] for i in order]\n",
        "        \n",
        "        tps, fps = torch.zeros(len(scores_all)), torch.zeros(len(scores_all))\n",
        "        for i in range(len(scores_all)):\n",
        "            img_id = img_ids_all[i]\n",
        "            pred_box = boxes_all[i].unsqueeze(0)\n",
        "            if img_id not in gt_dict:\n",
        "                fps[i] = 1; continue\n",
        "            gt_entry = gt_dict[img_id][0]\n",
        "            ious = box_iou(pred_box, gt_entry[\"boxes\"]).squeeze(0)\n",
        "            if ious.numel() == 0: fps[i] = 1; continue\n",
        "            max_iou, max_idx = ious.max(0)\n",
        "            if max_iou >= iou_thr and not gt_entry[\"matched\"][max_idx]:\n",
        "                tps[i] = 1; gt_entry[\"matched\"][max_idx] = True\n",
        "            else: fps[i] = 1\n",
        "        \n",
        "        tp_cum, fp_cum = torch.cumsum(tps, dim=0), torch.cumsum(fps, dim=0)\n",
        "        recall = tp_cum / (n_gt + 1e-6)\n",
        "        precision = tp_cum / (tp_cum + fp_cum + 1e-6)\n",
        "        aps.append(compute_ap(recall, precision))\n",
        "    \n",
        "    mAP = sum(aps) / len(aps) if len(aps) > 0 else 0.0\n",
        "    return mAP, aps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "417b2f55",
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_PATH = dataset.location\n",
        "train_dataset = YoloDataset(os.path.join(DATA_PATH, \"train\"))\n",
        "val_dataset   = YoloDataset(os.path.join(DATA_PATH, \"valid\"))\n",
        "test_dataset  = YoloDataset(os.path.join(DATA_PATH, \"test\"))\n",
        "# [ÏµúÏ†ÅÌôî] num_workers=2, pin_memory=True Ï∂îÍ∞Ä\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=yolo_collate_fn, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=yolo_collate_fn, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=yolo_collate_fn, num_workers=2, pin_memory=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using Device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a2f274e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 10: ÌïôÏäµ Ï§ÄÎπÑ\n",
        "# ============================================\n",
        "cfg = dict(\n",
        "    in_ch=3,\n",
        "    stem_base=64,\n",
        "    embed_dim=192,   # ViT Tiny Í∏∞Ï§Ä\n",
        "    vit_depth=4,\n",
        "    vit_heads=8,     # TinyÎ©¥ Î≥¥ÌÜµ 3~6Ïù∏Îç∞ 8ÎèÑ OK\n",
        "    num_classes=3,\n",
        "    iters=2,\n",
        "    detach_feedback=False,\n",
        "    img_size=IMG_SIZE\n",
        ")\n",
        "\n",
        "model = HybridTwoWay(**cfg).to(device)\n",
        "\n",
        "# [ÏÑ†ÌÉù] torch.compile (PyTorch 2.0+, Colab T4/L4)\n",
        "try:\n",
        "    model = torch.compile(model)\n",
        "    print(\"‚úÖ Model compiled with torch.compile\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è torch.compile failed/skipped\")\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "scaler = torch.amp.GradScaler('cuda')\n",
        "EPOCHS = 5 # Ï°∞Ï†ï ÌïÑÏöî\n",
        "\n",
        "best_map = -1.0\n",
        "best_epoch = -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b18a0c7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# VRAM ÏÇ¨Ïö©Îüâ Ï≤¥ÌÅ¨ Î≥¥Ï°∞ Cell\n",
        "imgs, targets = next(iter(train_loader))\n",
        "imgs = imgs.to(device)\n",
        "targets = [t.to(device) for t in targets]\n",
        "\n",
        "optimizer.zero_grad()\n",
        "with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
        "    preds, aux = model(imgs)\n",
        "    loss = yolo_loss(preds, targets, img_size=IMG_SIZE)\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print(\"Max allocated:\", torch.cuda.max_memory_allocated() / 1024**3, \"GB\")\n",
        "print(\"Max reserved :\", torch.cuda.max_memory_reserved() / 1024**3, \"GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63ce3543",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 11: ÌïôÏäµ Î£®ÌîÑ (AMP + Flash Attention + PosEmbed + TAL Loss)\n",
        "# ============================================\n",
        "\n",
        "print(\"üöÄ Start Training...\")\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "    \n",
        "    for imgs, targets in loop:\n",
        "        imgs = imgs.to(device)\n",
        "        targets = [t.to(device) for t in targets]\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
        "            preds, aux = model(imgs)\n",
        "            \n",
        "            # [ÏàòÏ†ïÎê®] Ïó¨Í∏∞ÏÑú yolo_loss -> yolo_loss_tal Î°ú Î≥ÄÍ≤ΩÌï¥Ïïº ÏµúÏã† Î°úÏßÅÏù¥ Ï†ÅÏö©Îê©ÎãàÎã§!\n",
        "            loss = yolo_loss_tal(preds, targets, img_size=IMG_SIZE)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        \n",
        "        # [ÏïàÏ†ÑÏû•Ïπò] Í∑∏ÎûòÎîîÏñ∏Ìä∏ ÌÅ¥Î¶¨Ìïë (detach_feedback=FalseÏùº Îïå ÌïÑÏàò)\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        loop.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} | Train Average Loss: {avg_loss:.4f}\")\n",
        "    \n",
        "    # Validation\n",
        "    # Í≤ÄÏ¶ù ÏãúÏóêÎèÑ iou_thr Îì±ÏùÑ Ï°∞Í∏à Îçî ÏóÑÍ≤©ÌïòÍ≤å Î≥¥Í±∞ÎÇò Ïú†ÏßÄÌï¥ÎèÑ Îê©ÎãàÎã§.\n",
        "    val_map, val_aps = evaluate_map(model, val_loader, num_classes=cfg[\"num_classes\"], img_size=IMG_SIZE, iou_thr=0.5, conf_thres=0.001)\n",
        "    print(f\"Epoch {epoch+1} | Val mAP@0.5: {val_map:.4f}\")\n",
        "\n",
        "    if val_map > best_map:\n",
        "        best_map = val_map\n",
        "        best_epoch = epoch + 1\n",
        "        # compile ÏÇ¨Ïö© Ïãú ÏõêÎ≥∏ state_dict Ï†ÄÏû•ÏùÑ ÏúÑÌï¥ _orig_mod ÌôïÏù∏\n",
        "        state_dict = model._orig_mod.state_dict() if hasattr(model, '_orig_mod') else model.state_dict()\n",
        "        ckpt = {\"state_dict\": state_dict, \"cfg\": cfg, \"epoch\": best_epoch, \"val_map\": best_map}\n",
        "        torch.save(ckpt, \"hybrid_two_way_best.pt\")\n",
        "        print(f\"‚úÖ Best model saved! (Val mAP: {best_map:.4f})\")\n",
        "\n",
        "print(\"üèÅ Training Finished!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dded0d33",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 12: Ï†ÄÏû•Îêú Best Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞ Î∞è ÌÖåÏä§Ìä∏ ÌèâÍ∞Ä\n",
        "# ============================================\n",
        "\n",
        "# Ï†ÄÏû•Îêú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Î°úÎìú\n",
        "checkpoint = torch.load(\"hybrid_two_way_best.pt\", map_location=device)\n",
        "loaded_cfg = checkpoint[\"cfg\"]\n",
        "\n",
        "print(f\"üìÑ Loaded Config: {loaded_cfg}\")\n",
        "\n",
        "# Î™®Îç∏ Ï¥àÍ∏∞Ìôî (Ï†ÄÏû•Îêú ConfigÎåÄÎ°ú Î≥µÏõê)\n",
        "model = HybridTwoWay(**loaded_cfg).to(device)\n",
        "model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "model.eval()\n",
        "\n",
        "print(f\"‚úÖ Model Loaded from Epoch {checkpoint['epoch']} (Val mAP: {checkpoint['val_map']:.4f})\")\n",
        "\n",
        "# ÌÖåÏä§Ìä∏ÏÖã ÌèâÍ∞Ä\n",
        "# TAL Î∞©ÏãùÏùÄ scoreÍ∞Ä ÎÇÆÍ≤å ÍπîÎ¶¥ Ïàò ÏûàÏúºÎØÄÎ°ú conf_thresÎ•º ÎÇÆÏ∂∞ÏÑú(0.001) mAPÎ•º Ï∏°Ï†ïÌïòÎäî Í≤å Íµ≠Î£∞ÏûÖÎãàÎã§.\n",
        "test_map, class_aps = evaluate_map(model, test_loader, num_classes=loaded_cfg[\"num_classes\"], img_size=IMG_SIZE, conf_thres=0.001)\n",
        "\n",
        "print(f\"\\nüèÜ Final Test mAP@0.5: {test_map:.4f}\")\n",
        "for i, ap in enumerate(class_aps):\n",
        "    print(f\"   Class {i} AP@0.5: {ap:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Cell 13: Quick Sanity Check (ÏûÖÏ∂úÎ†• shape ÌôïÏù∏)\n",
        "# ============================================\n",
        "\n",
        "x = torch.randn(2, 3, 640, 640).to(device)\n",
        "preds, aux = model(x)\n",
        "\n",
        "for level, (c, o, b) in zip([\"P3\",\"P4\",\"P5\"], preds):\n",
        "    print(f\"[{level}] cls: {list(c.shape)}, obj: {list(o.shape)}, box: {list(b.shape)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8340317e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Îç∞Ïù¥ÌÑ∞ Î∂ÑÌè¨ ÌôïÏù∏\n",
        "# 0 : APC, 1 : Tank, 2 : Person\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "def count_class_dist(dataset, name):\n",
        "    cnt = Counter()\n",
        "    for i in range(len(dataset)):\n",
        "        _, tgt = dataset[i]\n",
        "        if tgt.numel() == 0:\n",
        "            continue\n",
        "        cls_ids = tgt[:, 0].long().tolist()\n",
        "        cnt.update(cls_ids)\n",
        "    print(f\"\\n[{name}] class distribution:\")\n",
        "    for k in sorted(cnt.keys()):\n",
        "        print(f\"  class {k}: {cnt[k]} boxes\")\n",
        "\n",
        "count_class_dist(train_dataset, \"train\")\n",
        "count_class_dist(val_dataset,   \"val\")\n",
        "count_class_dist(test_dataset,  \"test\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
